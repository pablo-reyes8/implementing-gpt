{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom tokenizers import decoders\nimport os\n\n\nDATASET_NAME   = \"wikitext\"\nDATASET_CONFIG = \"wikitext-103-raw-v1\"   \n\nVOCAB_SIZE = 16000         \nMIN_FREQ  = 2\nBLOCK_SIZE   = 256  # Ventana de contexto         \nVAL_FRACTION   = 0.1\nTOKENIZER_PATH = Path(\"wikitext103_tokenizer.json\")  \n\nCPU_COUNT   = os.cpu_count() or 2\nBATCH_SIZE  = 64\nNUM_WORKERS  = 2 if CPU_COUNT <= 2 else min(4, CPU_COUNT - 1)             \nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\ndef load_openwebtext10k():\n    \"\"\"\n    AHORA: carga wikitext-103-raw-v1 con splits train / validation.\n    \"\"\"\n\n    ds = load_dataset(DATASET_NAME, DATASET_CONFIG)\n    train_ds = ds[\"train\"]\n    val_ds   = ds[\"validation\"]\n    print(train_ds)\n    print(val_ds)\n    return train_ds, val_ds\n\n\ndef train_tokenizer(train_ds,\n                    vocab_size=VOCAB_SIZE,\n                    min_freq=MIN_FREQ,\n                    save_path=TOKENIZER_PATH):\n    \"\"\"\n    Entrena un tokenizer BPE estilo GPT (byte-level) sobre el split de entrenamiento.\n    \"\"\"\n    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n\n    tokenizer.normalizer = normalizers.Sequence([\n        normalizers.NFKC(),\n        normalizers.Lowercase(),])\n\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n    tokenizer.decoder = decoders.ByteLevel() \n\n    special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        min_frequency=min_freq,\n        special_tokens=special_tokens,)\n\n    def batch_iterator():\n        for ex in train_ds:\n            txt = ex[\"text\"]\n            if txt is not None and len(txt.strip()) > 0:\n                yield txt\n\n    print(\"Entrenando tokenizer BPE...\")\n    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n    print(\"Tamaño vocabulario:\", tokenizer.get_vocab_size())\n\n    save_path = Path(save_path)\n    tokenizer.save(str(save_path))\n    print(f\"Tokenizer guardado en {save_path.resolve()}\")\n\n    return tokenizer\n\n\ndef load_or_train_tokenizer(train_ds):\n    \"\"\"\n    Carga el tokenizer si ya existe en disco, si no lo entrena.\n    \"\"\"\n    if TOKENIZER_PATH.exists():\n        print(f\"Cargando tokenizer desde {TOKENIZER_PATH}...\")\n        tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n    else:\n        tokenizer = train_tokenizer(train_ds)\n    return tokenizer\n\n\nclass GPTTextDataset(Dataset):\n    \"\"\"\n    Construye un dataset para LM autoregresivo:\n      - Concatena todos los documentos en una secuencia larga de IDs.\n      - Añade <eos> al final de cada documento.\n      - Parte en chunks de longitud (BLOCK_SIZE + 1).\n      - Input = ids[:-1], Target = ids[1:].\n    \"\"\"\n    def __init__(self, hf_split, tokenizer, block_size=BLOCK_SIZE):\n        super().__init__()\n        self.block_size = block_size\n\n        eos_id = tokenizer.token_to_id(\"<eos>\")\n        if eos_id is None:\n            raise ValueError(\"El tokenizer no tiene token <eos>.\")\n\n        all_ids = []\n\n        print(\"Tokenizando y concatenando textos...\")\n        for ex in hf_split:\n            txt = ex[\"text\"]\n            if txt is None or len(txt.strip()) == 0:\n                continue\n            enc = tokenizer.encode(txt)\n            # enc.ids es una lista de ints\n            all_ids.extend(enc.ids + [eos_id])\n\n        self.data = torch.tensor(all_ids, dtype=torch.long)\n        print(f\"Total de tokens en este split: {len(self.data):,}\")\n\n        n_tokens = len(self.data)\n        chunk_len = block_size + 1\n        n_chunks = n_tokens // chunk_len\n\n        if n_chunks == 0:\n            raise ValueError(\"Muy pocos tokens para formar un solo chunk. \"\n                             \"Baja BLOCK_SIZE o usa más datos.\")\n\n        # Cortar a múltiplo exacto de chunk_len y reshape\n        self.data = self.data[: n_chunks * chunk_len]\n        self.data = self.data.view(n_chunks, chunk_len)\n\n        # inputs y targets precomputados\n        self.inputs  = self.data[:, :-1]  # [N, block_size]\n        self.targets = self.data[:, 1:]   # [N, block_size]\n\n        print(f\"Número de secuencias: {len(self.inputs):,}\")\n        print(f\"Forma inputs:  {self.inputs.shape}\")\n        print(f\"Forma targets: {self.targets.shape}\")\n\n    def __len__(self):\n        return self.inputs.size(0)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]\n\n\ndef create_dataloaders(block_size=BLOCK_SIZE,\n                       batch_size=BATCH_SIZE,\n                       num_workers=NUM_WORKERS):\n    train_hf, val_hf = load_openwebtext10k()\n    tokenizer = load_or_train_tokenizer(train_hf)\n\n    train_ds = GPTTextDataset(train_hf, tokenizer, block_size=block_size)\n    val_ds   = GPTTextDataset(val_hf,   tokenizer, block_size=block_size)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True)\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True)\n\n    return train_loader, val_loader, tokenizer\n\ntrain_loader, val_loader, tokenizer = create_dataloaders()\n\nx, y = next(iter(train_loader))\nprint(\"Batch x shape:\", x.shape) \nprint(\"Batch y shape:\", y.shape) \n\nexample_ids = x[0].tolist()\ntext = tokenizer.decode(example_ids)\nprint(\"Texto ejemplo (primer sample de x):\")\nprint(text)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:37:33.631614Z","iopub.execute_input":"2025-11-18T06:37:33.632230Z","iopub.status.idle":"2025-11-18T06:49:24.017409Z","shell.execute_reply.started":"2025-11-18T06:37:33.632205Z","shell.execute_reply":"2025-11-18T06:49:24.016599Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"736f54e77b594960813e3c1ce859f0e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/test-00000-of-00001.(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88c4eb325294a25a0de7da81104ebeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00000-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52aa11d3d0914439b143e68b834c1aeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/train-00001-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ec5956a4974a7f85fd33c7a777e7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-103-raw-v1/validation-00000-of-(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2de30a670cd49d789f6b4445278e2dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8bfd66cd3a64300872cc9fc0715aca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6261e15fad764500a407d2d6297d79b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bf31a250907475bafbcea04b4c59009"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 1801350\n})\nDataset({\n    features: ['text'],\n    num_rows: 3760\n})\nEntrenando tokenizer BPE...\n\n\n\nTamaño vocabulario: 16000\nTokenizer guardado en /kaggle/working/wikitext103_tokenizer.json\nTokenizando y concatenando textos...\nTotal de tokens en este split: 119,513,253\nNúmero de secuencias: 465,032\nForma inputs:  torch.Size([465032, 256])\nForma targets: torch.Size([465032, 256])\nTokenizando y concatenando textos...\nTotal de tokens en este split: 250,245\nNúmero de secuencias: 973\nForma inputs:  torch.Size([973, 256])\nForma targets: torch.Size([973, 256])\nBatch x shape: torch.Size([64, 256])\nBatch y shape: torch.Size([64, 256])\nTexto ejemplo (primer sample de x):\n nuns . \n the actor is the french association pour la béatification de l 'impératrice zita . \n the postulator for the cause is father alexander leonhardt . the judge of the tribunal is father bruno bonnet . the promoter of justice is the father françois scrive . \n = = titles , styles , honours and arms = = \n = = = titles and styles = = = \n 9 may 1892 – 21 october 1911 : her royal highness princess zita of bourbon @-@ parma \n 21 october 1911 – 21 november 1916 : her imperial and royal highness archduchess and princess zita of austria , princess of hungary and bohemia , princess of bourbon @-@ parma \n 21 november 1916 – 11 november 1918 : her imperial and royal apostolic majesty the empress of austria , apostolic queen of hungary \n 11 november 1918 – 14 march 1989 : \n her imperial and royal apostolic majesty empress zita of austria , apostolic queen of hungary ( used outside austria ) \n zita , duchess of bar ( inscribed in her passport ) \n zita habsburg @-@ lothringen ( used in austria ) \n = = =\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"from collections import Counter\nimport torch\n\ndef inspect_autoregressive_loader(train_loader, tokenizer, \n                                  num_batches=2,  # cuántos batches inspeccionar\n                                  max_examples=3, # cuántas secuencias imprimir por batch\n                                  max_tokens_print=40):  # cuántos tokens decodificar por ejemplo\n    \"\"\"\n    Inspecciona un DataLoader de lenguaje autoregresivo (GPT-style).\n\n    Supone que cada batch es:\n        x: [B, T]  (inputs)\n        y: [B, T]  (targets desplazados 1 a la derecha en el stream original)\n\n    Hace:\n    - Verificar qué tan cierto es que y[:, :-1] == x[:, 1:].\n    - Analizar la distribución de y[:, 0] (primer token que el modelo debe predecir).\n    - Imprimir ejemplos decodificados (input vs target) para entender el shift.\n    \"\"\"\n    total_shift_positions = 0\n    total_shift_matches   = 0\n\n    first_input_ids  = []\n    first_target_ids = []\n\n    for b_idx, (x, y) in enumerate(train_loader):\n        B, T = x.shape\n\n        # Comprobar el desplazamiento: y[:, :-1] debería ser igual a x[:, 1:]\n        shift_equal = (y[:, :-1] == x[:, 1:])  # [B, T-1] bool\n        total_shift_matches   += shift_equal.sum().item()\n        total_shift_positions += shift_equal.numel()\n\n        # Guardar los primeros tokens de input y target para estadísticas\n        first_input_ids.extend(x[:, 0].tolist())\n        first_target_ids.extend(y[:, 0].tolist())\n\n        # Imprimir algunos ejemplos para ver texto real\n        print(f\"\\n=== Batch {b_idx} ===\")\n        for i in range(min(max_examples, B)):\n            inp_ids = x[i].tolist()\n            tgt_ids = y[i].tolist()\n\n            inp_ids_short = inp_ids[:max_tokens_print]\n            tgt_ids_short = tgt_ids[:max_tokens_print]\n\n            inp_text = tokenizer.decode(inp_ids_short)\n            tgt_text = tokenizer.decode(tgt_ids_short)\n\n            print(f\"\\n--- Ejemplo {i} ---\")\n            print(f\"Input IDs   (primeros {len(inp_ids_short)}): {inp_ids_short}\")\n            print(f\"Target IDs  (primeros {len(tgt_ids_short)}): {tgt_ids_short}\")\n            print(\"Input texto (modelo VE):\")\n            print(repr(inp_text))\n            print(\"Target texto (modelo DEBE predecir):\")\n            print(repr(tgt_text))\n\n        if b_idx + 1 >= num_batches:\n            break\n\n    shift_ratio = total_shift_matches / total_shift_positions\n    print(\"\\n================== RESUMEN AUTORREGRESIVO ==================\")\n    print(f\"Total posiciones comparadas (y[:, :-1] vs x[:, 1:]): {total_shift_positions}\")\n    print(f\"Coincidencias: {total_shift_matches}\")\n    print(f\"Proporción de coincidencia (ideal ~1.0): {shift_ratio:.6f}\")\n\n    # Estadísticas sobre la PRIMERA posición\n    print(\"\\nDistribución de primeros tokens (input vs target):\")\n\n    first_input_counts  = Counter(first_input_ids)\n    first_target_counts = Counter(first_target_ids)\n\n    def top_tokens(counter, name, k=10):\n        print(f\"\\nTop {k} tokens más frecuentes en {name}:\")\n        for token_id, cnt in counter.most_common(k):\n            text = tokenizer.decode([token_id])\n            print(f\"  id={token_id:5d} | freq={cnt:6d} | texto={repr(text)}\")\n\n    top_tokens(first_input_counts,  \"x[:, 0]  (primer token que VE el modelo)\")\n    top_tokens(first_target_counts, \"y[:, 0]  (primer token que DEBE predecir)\")\n\n    print(\"\\n============================================================\")\n\n  \ninspect_autoregressive_loader(train_loader, tokenizer,\n                              num_batches=1,  \n                              max_examples=2, \n                              max_tokens_print=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:49:33.166590Z","iopub.execute_input":"2025-11-18T06:49:33.167082Z","iopub.status.idle":"2025-11-18T06:49:33.449781Z","shell.execute_reply.started":"2025-11-18T06:49:33.167036Z","shell.execute_reply":"2025-11-18T06:49:33.448866Z"}},"outputs":[{"name":"stdout","text":"\n=== Batch 0 ===\n\n--- Ejemplo 0 ---\nInput IDs   (primeros 50): [2442, 416, 283, 197, 259, 480, 199, 185, 507, 7922, 350, 1457, 240, 185, 15305, 336, 243, 182, 1047, 259, 434, 4987, 737, 1457, 313, 185, 5561, 5037, 189, 680, 185, 737, 209, 5399, 216, 189, 7254, 551, 864, 189, 215, 3684, 3671, 341, 56, 6608, 13262, 11077, 189, 6181]\nTarget IDs  (primeros 50): [416, 283, 197, 259, 480, 199, 185, 507, 7922, 350, 1457, 240, 185, 15305, 336, 243, 182, 1047, 259, 434, 4987, 737, 1457, 313, 185, 5561, 5037, 189, 680, 185, 737, 209, 5399, 216, 189, 7254, 551, 864, 189, 215, 3684, 3671, 341, 56, 6608, 13262, 11077, 189, 6181, 7618]\nInput texto (modelo VE):\n'fall after an f @-@ 5 . the other predominant match on the undercard was a six @-@ man tag team match from the raw brand , between the team of triple h , ric flair , and chris jericho facing shawn michaels , kevin'\nTarget texto (modelo DEBE predecir):\n' after an f @-@ 5 . the other predominant match on the undercard was a six @-@ man tag team match from the raw brand , between the team of triple h , ric flair , and chris jericho facing shawn michaels , kevin nash'\n\n--- Ejemplo 1 ---\nInput IDs   (primeros 50): [13771, 50, 10237, 217, 220, 331, 9281, 199, 463, 590, 2664, 182, 4474, 1394, 221, 5128, 62, 259, 3118, 11379, 3986, 209, 408, 9546, 3848, 204, 8900, 4546, 199, 204, 276, 495, 189, 185, 4032, 1587, 2038, 722, 428, 10025, 2811, 332, 204, 408, 2183, 452, 282, 50, 62, 334]\nTarget IDs  (primeros 50): [50, 10237, 217, 220, 331, 9281, 199, 463, 590, 2664, 182, 4474, 1394, 221, 5128, 62, 259, 3118, 11379, 3986, 209, 408, 9546, 3848, 204, 8900, 4546, 199, 204, 276, 495, 189, 185, 4032, 1587, 2038, 722, 428, 10025, 2811, 332, 204, 408, 2183, 452, 282, 50, 62, 334, 210]\nInput texto (modelo VE):\n' hexi corridor to lop nur . they repelled a joint xiongnu @-@ qiang invasion of this northwestern territory in 111 bc . in that year , the han court established four new frontier commanderies in this region : jiuqu'\nTarget texto (modelo DEBE predecir):\n'i corridor to lop nur . they repelled a joint xiongnu @-@ qiang invasion of this northwestern territory in 111 bc . in that year , the han court established four new frontier commanderies in this region : jiuquan'\n\n================== RESUMEN AUTORREGRESIVO ==================\nTotal posiciones comparadas (y[:, :-1] vs x[:, 1:]): 16320\nCoincidencias: 16320\nProporción de coincidencia (ideal ~1.0): 1.000000\n\nDistribución de primeros tokens (input vs target):\n\nTop 10 tokens más frecuentes en x[:, 0]  (primer token que VE el modelo):\n  id=  185 | freq=     5 | texto=' the'\n  id=  256 | freq=     2 | texto=' as'\n  id=    3 | freq=     2 | texto=''\n  id=  199 | freq=     2 | texto=' .'\n  id=  229 | freq=     2 | texto=' ='\n  id= 2442 | freq=     1 | texto='fall'\n  id=13771 | freq=     1 | texto=' hex'\n  id=  217 | freq=     1 | texto=' to'\n  id= 4940 | freq=     1 | texto=' unique'\n  id=  351 | freq=     1 | texto='ud'\n\nTop 10 tokens más frecuentes en y[:, 0]  (primer token que DEBE predecir):\n  id=  229 | freq=     3 | texto=' ='\n  id=  452 | freq=     2 | texto=' :'\n  id=  215 | freq=     2 | texto=' and'\n  id=  259 | freq=     2 | texto=' @-@'\n  id=  199 | freq=     2 | texto=' .'\n  id=  185 | freq=     2 | texto=' the'\n  id=  209 | freq=     2 | texto=' of'\n  id=  416 | freq=     1 | texto=' after'\n  id=   50 | freq=     1 | texto='i'\n  id=  479 | freq=     1 | texto=' have'\n\n============================================================\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass GPT2Embeddings(nn.Module):\n    \"\"\"\n    Embeddings estilo GPT-2:\n      - token embeddings aprendidos\n      - positional embeddings aprendidos\n      - dropout opcional\n\n    input_ids: LongTensor [B, T]\n    return:    FloatTensor [B, T, d_model]\n    \"\"\"\n    def __init__(self, vocab_size: int, d_model: int, block_size: int, dropout: float = 0.1):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(block_size, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.block_size = block_size\n        self.d_model = d_model\n\n    def forward(self, input_ids: torch.Tensor):\n        \"\"\"\n        input_ids: [B, T] con IDs de tokens.\n        \"\"\"\n        \n        B, T = input_ids.shape\n        if T > self.block_size:\n            raise ValueError(f\"Secuencia T={T} > block_size={self.block_size}\")\n\n        # posiciones [0, 1, ..., T-1]\n        device = input_ids.device\n        pos_ids = torch.arange(0, T, device=device).unsqueeze(0)  # [1, T]\n        pos_ids = pos_ids.expand(B, T)  \n\n        tok = self.tok_emb(input_ids)  # [B, T, d_model]\n        pos = self.pos_emb(pos_ids)   \n        x = tok + pos                  # [B, T, d_model]\n\n        x = self.dropout(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:26.646746Z","iopub.execute_input":"2025-11-18T05:29:26.647123Z","iopub.status.idle":"2025-11-18T05:29:26.654005Z","shell.execute_reply.started":"2025-11-18T05:29:26.647073Z","shell.execute_reply":"2025-11-18T05:29:26.653020Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nvocab_size = tokenizer.get_vocab_size()\nd_model = 256\nblock_size = 256\n\nemb = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\n\nx_ids, y_ids = next(iter(train_loader))  \nx_ids = x_ids.to(device)\n\nx_emb = emb(x_ids)  # [B, T, d_model]\nprint(x_emb.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:27:39.740569Z","iopub.execute_input":"2025-11-18T05:27:39.741399Z","iopub.status.idle":"2025-11-18T05:27:39.927771Z","shell.execute_reply.started":"2025-11-18T05:27:39.741289Z","shell.execute_reply":"2025-11-18T05:27:39.926784Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 256, 256])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef scaled_dot_product_attention(q, k, v, mask=None):\n    \"\"\"\n    q: (..., Lq, d)\n    k: (..., Lk, d)\n    v: (..., Lk, dv)\n    mask: broadcastable a (..., Lq, Lk)\n          - bool: True = BLOQUEAR (poner -inf)\n          - float: 1.0 = permitir, 0.0 = bloquear\n    Returns:\n        output: (..., Lq, dv)\n        attn:   (..., Lq, Lk)\n\n    \"\"\"\n    scores = torch.matmul(q, k.transpose(-2, -1))\n    dk = q.size(-1)\n    scores = scores / dk**0.5\n\n    if mask is not None:\n        # Normalizamos a un tensor float con -inf donde se bloquea\n        if mask.dtype == torch.bool:\n            scores = scores.masked_fill(mask, float(\"-inf\"))\n        else:\n            # asumimos máscara en {0,1}: 0 = bloquear\n            scores = scores.masked_fill(mask <= 0, float(\"-inf\"))\n\n    attn = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn, v)\n    return output, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model debe ser múltiplo de num_heads\"\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def _split_heads(self, x):\n        B, L, _ = x.shape\n        return x.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n\n    def _combine_heads(self, x):\n        B, H, L, D = x.shape\n        return x.transpose(1, 2).contiguous().view(B, L, H * D)\n\n    def forward(self, x_q, x_kv, mask=None):\n        #Primera proyeccion\n        q = self._split_heads(self.w_q(x_q))\n        k = self._split_heads(self.w_k(x_kv))\n        v = self._split_heads(self.w_v(x_kv))\n\n\n        if mask is not None:\n        # Aceptamos:\n        # (B, Lk), (B, Lq, Lk), (B, 1, Lq, Lk), (B, H, Lq, Lk)\n          if mask.dim() == 2:\n              mask = mask[:, None, None, :]\n\n          elif mask.dim() == 3:\n              mask = mask[:, None, :, :]\n\n          elif mask.dim() == 4:\n              pass # Ya funciona asi\n          else:\n              raise ValueError(f\"Máscara con dims no soportadas: {mask.shape}\")\n\n        if mask.dtype != torch.bool:\n            mask = (mask <= 0)\n\n        # Aplicamos Atencion y concatenamos\n        attn_out, _ = scaled_dot_product_attention(q, k, v, mask)\n        attn_out = self._combine_heads(attn_out)\n\n        # Proyeccion Final\n        attn_out = self.w_o(attn_out)\n        attn_out = self.dropout(attn_out)\n        return attn_out\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, block_size: int, dropout: float = 0.1):\n        super().__init__()\n        \n        self.mha = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n        self.block_size = block_size\n\n        mask = torch.triu(torch.ones(block_size, block_size, dtype=torch.bool), diagonal=1)\n        self.register_buffer(\"causal_mask\", mask, persistent=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x: [B, T, d_model]\n        \"\"\"\n        B, T, _ = x.shape\n        if T > self.block_size:\n            raise ValueError(f\"T={T} > block_size={self.block_size}\")\n\n        # [T, T] -> [1, T, T] para que MultiHeadAttention lo trate como 'batch size = 1'\n        mask = self.causal_mask[:T, :T].unsqueeze(0) \n\n        # Self-attention: q = k = v = x\n        out = self.mha(x, x, mask=mask)   # [B, T, d_model]\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:23.174179Z","iopub.execute_input":"2025-11-18T05:29:23.174435Z","iopub.status.idle":"2025-11-18T05:29:23.187603Z","shell.execute_reply.started":"2025-11-18T05:29:23.174418Z","shell.execute_reply":"2025-11-18T05:29:23.186816Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nvocab_size = tokenizer.get_vocab_size()\nd_model  = 256\nblock_size = 256\n\nemb  = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\nattn = CausalSelfAttention(d_model, num_heads=8, block_size=block_size, dropout=0.1).to(device)\n\nx_ids, y_ids = next(iter(train_loader))  # [B, T]\nx_ids = x_ids.to(device)\n\nx = emb(x_ids)  # pesos + input \nx = attn(x)     # [B, T, d_model]\nprint(x.shape) # [B, T, d_model] (self-attn causal)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:28:55.577292Z","iopub.execute_input":"2025-11-18T05:28:55.577568Z","iopub.status.idle":"2025-11-18T05:28:56.095549Z","shell.execute_reply.started":"2025-11-18T05:28:55.577549Z","shell.execute_reply":"2025-11-18T05:28:56.094812Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 256, 256])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class GPT2MLP(nn.Module):\n    \"\"\"\n    MLP posición a posición estilo GPT-2:\n      Linear(d_model → d_ff) + GELU + Dropout + Linear(d_ff → d_model)\n    \"\"\"\n    def __init__(self, d_model: int, d_ff = None, dropout: float = 0.1):\n        super().__init__()\n        if d_ff is None:\n            d_ff = 4 * d_model  # típico en GPT\n\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.act = nn.GELU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nclass GPT2Block(nn.Module):\n    \"\"\"\n    Bloque GPT-2:\n      x -> LN -> CausalSelfAttention -> +residual\n      -> LN -> MLP -> +residual\n    Pre-LN (estilo GPT-2 moderno).\n    \"\"\"\n    def __init__(self, d_model: int, num_heads: int, block_size: int,\n                 d_ff = None, dropout: float = 0.1, layernorm_eps: float = 1e-5):\n      \n        super().__init__()\n        self.ln_1 = nn.LayerNorm(d_model, eps=layernorm_eps)\n        self.ln_2 = nn.LayerNorm(d_model, eps=layernorm_eps)\n        self.attn = CausalSelfAttention(d_model=d_model, num_heads=num_heads,\n                                        block_size=block_size, dropout=dropout)\n        \n        self.mlp  = GPT2MLP(d_model=d_model, d_ff=d_ff, dropout=dropout)\n\n    def forward(self, x: torch.Tensor):\n      # Pre-LN + atención causal\n      normalized_1 = self.ln_1(x)\n      attn_output = self.attn(normalized_1)\n      x = x + attn_output # residual conection\n      \n      # Pre-LN + MLP\n      normalized_2 = self.ln_2(x)\n      mlp_output = self.mlp(normalized_2)\n      x = x + mlp_output # residual conection\n    \n      return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:21.038382Z","iopub.execute_input":"2025-11-18T05:29:21.039018Z","iopub.status.idle":"2025-11-18T05:29:21.047825Z","shell.execute_reply.started":"2025-11-18T05:29:21.038967Z","shell.execute_reply":"2025-11-18T05:29:21.046800Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nvocab_size = tokenizer.get_vocab_size()\nd_model = 256\nblock_size = 256\nnum_heads = 8\nd_ff = 4 * d_model  # 1024\n\nemb = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\nmlp = GPT2MLP(d_model, d_ff=d_ff, dropout=0.1).to(device)\nblock = GPT2Block(d_model, num_heads=num_heads, block_size=block_size, \n                  d_ff=d_ff, dropout=0.1).to(device)\n\n\nx_ids, y_ids = next(iter(train_loader))  # [B, T]\nx_ids = x_ids.to(device)\n\nx = emb(x_ids)  # [B, T, d_model]\nprint(f\"Después de embeddings: {x.shape}\")\n\nx_mlp = mlp(x)\nprint(f\"Después de MLP: {x_mlp.shape}\")  # [B, T, d_model]\n\nx_block = block(x)\nprint(f\"Después de GPT2Block: {x_block.shape}\")  # [B, T, d_model]\n\nprint(f\"¿Hay NaNs en la salida? {torch.isnan(x_block).any().item()}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:36.128188Z","iopub.execute_input":"2025-11-18T05:29:36.128881Z","iopub.status.idle":"2025-11-18T05:29:36.508530Z","shell.execute_reply.started":"2025-11-18T05:29:36.128850Z","shell.execute_reply":"2025-11-18T05:29:36.507479Z"}},"outputs":[{"name":"stdout","text":"Después de embeddings: torch.Size([64, 256, 256])\nDespués de MLP: torch.Size([64, 256, 256])\nDespués de GPT2Block: torch.Size([64, 256, 256])\n¿Hay NaNs en la salida? False\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def init_gptmini_weights(model):\n    with torch.no_grad():\n        for name, module in model.named_modules():\n            # Embeddings \n            if isinstance(module, nn.Embedding):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n            # Linears\n            elif isinstance(module, nn.Linear):\n                if name == \"lm_head\":\n                    # pesos pequeños\n                    nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                    if module.bias is not None:\n                        nn.init.zeros_(module.bias)\n                else:\n                    # Xavier \n                    nn.init.xavier_uniform_(module.weight)\n                    if module.bias is not None:\n                        nn.init.zeros_(module.bias)\n\n\nclass GPTMini(nn.Module):\n    \"\"\"\n    GPT-2 'mini' decoder-only:\n      - Embeddings (token + posición aprendida)\n      - n_layers de GPT2Block\n      - LayerNorm final\n      - LM head (atado a los embeddings de token)\n    \"\"\"\n    \n    def __init__(self,\n                 vocab_size: int,\n                 block_size: int,\n                 n_layer: int = 4,\n                 n_head: int = 4,\n                 d_model: int = 256,\n                 dropout: float = 0.1,\n                 layernorm_eps: float = 1e-5):\n        \n        super().__init__()\n\n        self.vocab_size = vocab_size\n        self.block_size = block_size\n        self.d_model    = d_model\n\n        self.emb = GPT2Embeddings(\n            vocab_size=vocab_size,\n            d_model=d_model,\n            block_size=block_size,\n            dropout=dropout)\n\n        self.blocks = nn.ModuleList([\n            GPT2Block(\n                d_model=d_model,\n                num_heads=n_head,\n                block_size=block_size,\n                d_ff=4*d_model,\n                dropout=dropout,\n                layernorm_eps=layernorm_eps,) for _ in range(n_layer)])\n\n        self.ln_f = nn.LayerNorm(d_model, eps=layernorm_eps)\n\n        # LM head: proyecta representaciones a logits de vocabulario\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n        # Weight tying: compartir pesos con embedding de tokens\n        init_gptmini_weights(self)\n        self.lm_head.weight = self.emb.tok_emb.weight\n\n    def forward(self, idx: torch.Tensor, targets = None):\n      \"\"\"\n      idx:     [B, T] con IDs de tokens de entrada\n      targets: [B, T] con IDs objetivo (shifted) o None\n\n      Returns:\n        logits: [B, T, vocab_size]\n        loss:   escalar (si targets no es None), sino None\n      \"\"\"\n      B, T = idx.shape\n      if T > self.block_size:\n          raise ValueError(f\"Secuencia demasiado larga: T={T}, block_size={self.block_size}\")\n\n      # Embeddings token + posición\n      x = self.emb(idx)  # [B, T, d_model]\n\n      # Pasar por los bloques GPT-2\n      for block in self.blocks:\n          x = block(x)    # [B, T, d_model]\n\n      # LayerNorm final\n      x = self.ln_f(x)   # [B, T, d_model]\n\n      # LM head -> logits\n      logits = self.lm_head(x)  # [B, T, vocab_size]\n\n      loss = None\n      if targets is not None:\n          # Cross-entropy autoregresiva\n          logits_flat  = logits.view(-1, self.vocab_size)\n          targets_flat = targets.view(-1)\n          # Opcional: ignorar padding tokens si usas ignore_index\n          loss = F.cross_entropy(logits_flat, targets_flat)\n\n      return logits, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:30:05.692202Z","iopub.execute_input":"2025-11-18T05:30:05.693129Z","iopub.status.idle":"2025-11-18T05:30:05.703891Z","shell.execute_reply.started":"2025-11-18T05:30:05.693083Z","shell.execute_reply":"2025-11-18T05:30:05.703047Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab_size()\nblock_size = 256  \n\nmodel = GPTMini(\n    vocab_size=vocab_size,\n    block_size=block_size,\n    n_layer=4,\n    n_head=4,\n    d_model=256,\n    dropout=0.1).to(device)\n\nx_ids, y_ids = next(iter(train_loader))  # [B, T]\nx_ids = x_ids.to(device)\ny_ids = y_ids.to(device)\n\nlogits, loss = model(x_ids, y_ids)\nprint(\"logits shape:\", logits.shape)  # [B, T, vocab_size]\nprint(\"loss:\", loss.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"def token_acc(logits: torch.Tensor, targets: torch.Tensor) -> float:\n    \"\"\"\n    logits:  [B, T, V]\n    targets: [B, T]\n    return: accuracy escalar en [0,1]\n    \"\"\"\n    preds = logits.argmax(dim=-1)       # [B, T]\n    correct = (preds == targets).sum().item()\n    total   = targets.numel()\n    return correct / total\n\n\nclass WarmupCosineScheduler(torch.optim.lr_scheduler._LRScheduler):\n    \"\"\"\n    Warmup lineal + decaimiento coseno:\n      - durante 'warmup_steps': lr sube lineal desde 0 hasta base_lr\n      - después: decae con coseno hasta ~0 en 'max_steps'\n    \"\"\"\n    def __init__(self, optimizer, warmup_steps: int, max_steps: int, last_epoch: int = -1):\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self._step_num = 0\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        self._step_num += 1\n        step = self._step_num\n\n        if step <= self.warmup_steps:\n            # Warmup lineal: 0 -> 1\n            scale = step / float(max(1, self.warmup_steps))\n        else:\n            # Cosine decay de 1 -> 0\n            progress = (step - self.warmup_steps) / float(\n                max(1, self.max_steps - self.warmup_steps))\n            \n            # cos(pi * 0) = 1, cos(pi * 1) = -1  ⇒  scale va de 1 -> 0\n            scale = 0.5 * (1.0 + math.cos(math.pi * progress))\n\n        return [base_lr * scale for base_lr in self.base_lrs]\n\n\ndef create_optimizer_and_scheduler(\n    model,\n    base_lr: float,\n    weight_decay: float,\n    warmup_steps: int,\n    max_steps: int):\n  \n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=base_lr,\n        betas=(0.9, 0.95),\n        weight_decay=weight_decay)\n\n    scheduler = WarmupCosineScheduler(\n        optimizer=optimizer,\n        warmup_steps=warmup_steps,\n        max_steps=max_steps)\n    \n    return optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:30:10.992754Z","iopub.execute_input":"2025-11-18T05:30:10.993549Z","iopub.status.idle":"2025-11-18T05:30:11.001677Z","shell.execute_reply.started":"2025-11-18T05:30:10.993523Z","shell.execute_reply":"2025-11-18T05:30:11.000787Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import inspect\nfrom contextlib import contextmanager, nullcontext\nimport torch\n\n_DTYPE_MAP = {\n    \"bf16\": torch.bfloat16,\n    \"bfloat16\": torch.bfloat16,\n    \"fp16\": torch.float16,\n    \"float16\": torch.float16,}\n\n\ndef _cuda_dtype_supported(dtype: torch.dtype) -> bool:\n    if not torch.cuda.is_available():\n        return False\n\n    return dtype in (torch.bfloat16, torch.float16)\n\n\ndef make_grad_scaler(device: str = \"cuda\", enabled: bool = True):\n    \"\"\"\n    Devuelve un GradScaler compatible con tu versión de PyTorch.\n    - Si AMP no está habilitado, devuelve None.\n    - Soporta torch.amp.GradScaler('cuda'|'cpu') o sin args.\n    \"\"\"\n    if not enabled:\n        return None\n\n    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n        try:\n            sig = inspect.signature(torch.amp.GradScaler)\n            if len(sig.parameters) >= 1:\n                return torch.amp.GradScaler(device if device in (\"cuda\", \"cpu\") else \"cuda\")\n            else:\n                return torch.amp.GradScaler()\n        except Exception:\n            pass\n\n    if hasattr(torch.cuda, \"amp\") and hasattr(torch.cuda.amp, \"GradScaler\"):\n        return torch.cuda.amp.GradScaler()\n\n    return None\n\n@contextmanager\ndef autocast_ctx(\n    device: str = \"cuda\",\n    enabled: bool = True,\n    dtype: str = \"bf16\",    \n    cache_enabled: bool = True):\n  \n    \"\"\"\n    Contexto robusto para autocast:\n      - CUDA: torch.amp.autocast(device_type=\"cuda\", dtype=...)\n      - CPU:  torch.amp.autocast(device_type=\"cpu\",  dtype=torch.bfloat16) si enabled\n      - fallback: nullcontext().\n\n    Notas:\n      * En BF16 NO uses GradScaler.\n      * En FP16 sí puedes usar GradScaler (torch.amp.GradScaler / torch.cuda.amp.GradScaler).\n    \"\"\"\n    if not enabled:\n        with nullcontext():\n            yield\n        return\n\n    if device == \"cuda\":\n        want = _DTYPE_MAP.get(dtype.lower(), torch.bfloat16)\n        use = want if _cuda_dtype_supported(want) else torch.float16\n        with torch.amp.autocast(device_type=\"cuda\", dtype=use, cache_enabled=cache_enabled):\n            yield\n        return\n\n    if device == \"cpu\":\n        try:\n            with torch.amp.autocast(device_type=\"cpu\", dtype=torch.bfloat16, cache_enabled=cache_enabled):\n                yield\n        except Exception:\n            # fallback seguro si el backend no soporta cpu autocast\n            with nullcontext():\n                yield\n        return\n\n    with nullcontext():\n        yield\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:30:12.370217Z","iopub.execute_input":"2025-11-18T05:30:12.370928Z","iopub.status.idle":"2025-11-18T05:30:12.380685Z","shell.execute_reply.started":"2025-11-18T05:30:12.370903Z","shell.execute_reply":"2025-11-18T05:30:12.380028Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import math\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.nn.functional as F\n\ndef train_gpt_lm(\n    model,\n    train_loader,\n    val_loader=None,\n    *,\n    epochs: int = 10,\n    base_lr: float = 3e-4,\n    weight_decay: float = 0.01,\n    warmup_steps: int = 2000,   # en steps, no epochs\n    label_smoothing: float = 0.0, \n    grad_clip: float | None = 1.0,\n    device: str = \"cuda\",\n    ckpt_path: str = \"gptmini_best.pt\",\n    log_every: int = 100,\n    preview_every: int | None = None,\n    id2tok_fn=None,             # callable: List[int] -> str\n    amp_enabled: bool = True,\n    amp_dtype: str = \"bf16\",    \n    val_checking: bool = False,     # por defecto NO se hace validación\n    save_ckpt_every: int | None = None,  # si no hay val, guardar cada N epochs\n):\n    \"\"\"\n    Entrena un modelo GPTMini (decoder-only LM) sobre (x, y) donde:\n      - x: [B, T]  ids de entrada\n      - y: [B, T]  ids objetivo (shifted)\n\n    - Usa AdamW + WarmupCosineScheduler\n    - CrossEntropy con label_smoothing opcional (sin PAD)\n    - Gradient clipping\n    - AMP (bf16/fp16) usando autocast_ctx y make_grad_scaler\n    - Si val_checking=True y val_loader no es None:\n        * corre validación y guarda mejor checkpoint por val_loss\n      Si val_checking=False:\n        * NO corre validación; puede guardar checkpoint cada 'save_ckpt_every' epochs\n    \"\"\"\n    device = torch.device(device)\n    torch.set_float32_matmul_precision(\"high\")\n    model.to(device)\n    model.train()\n\n    # Estimar total de steps (para el scheduler)\n    total_steps = epochs * len(train_loader)\n    \n    optimizer, scheduler = create_optimizer_and_scheduler(\n        model,base_lr=base_lr,weight_decay=weight_decay,\n        warmup_steps=warmup_steps,max_steps=total_steps,)\n\n    # Loss con y sin smoothing\n    ce_train = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n    ce_eval  = nn.CrossEntropyLoss(label_smoothing=0.0)\n\n    # AMP: GradScaler sólo si estamos en fp16; en bf16 normalmente no hace falta\n    use_scaler = amp_enabled and (amp_dtype.lower() in (\"fp16\", \"float16\"))\n    scaler = make_grad_scaler(device=\"cuda\" if device.type == \"cuda\" else \"cpu\",\n                              enabled=use_scaler)\n\n    best_val = float(\"inf\")\n    history = {\n        \"train_loss\": [], \"val_loss\": [],\n        \"train_ppl\":  [], \"val_ppl\":  [],\n        \"train_tok_acc\": [], \"val_tok_acc\": []}\n\n    global_step = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss_sum, epoch_tokens = 0.0, 0\n        epoch_acc_sum = 0.0\n        t0 = time.time()\n\n        for it, (x_ids, y_ids) in enumerate(train_loader, start=1):\n            global_step += 1\n            x_ids = x_ids.to(device, non_blocking=True)\n            y_ids = y_ids.to(device, non_blocking=True)\n            B, T = x_ids.shape\n            tokens = B * T\n\n            optimizer.zero_grad(set_to_none=True)\n\n            with autocast_ctx(device=device.type, enabled=amp_enabled, dtype=amp_dtype):\n                # Usamos la loss interna del modelo\n                logits, loss = model(x_ids, y_ids)   # [B, T, V], escalar\n                if loss is None:\n                    raise RuntimeError(\"GPTMini.forward debe devolver loss si targets != None\")\n\n            if scaler is not None:\n                # Asegurarnos de que loss es escalar\n                if loss.dim() > 0:\n                    loss = loss.mean()\n            \n                scaler.scale(loss).backward()\n                if grad_clip is not None:\n                    scaler.unscale_(optimizer)\n                    clip_grad_norm_(model.parameters(), grad_clip)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                # Asegurarnos de que loss es escalar\n                if loss.dim() > 0:\n                    loss = loss.mean()\n            \n                loss.backward()\n                if grad_clip is not None:\n                    clip_grad_norm_(model.parameters(), grad_clip)\n                optimizer.step()\n\n            if scheduler is not None:\n                scheduler.step()\n\n            with torch.no_grad():\n                acc = token_acc(logits, y_ids)\n\n            epoch_loss_sum += loss.item() * tokens\n            epoch_acc_sum  += acc * tokens\n            epoch_tokens   += tokens\n\n            if it % log_every == 0:\n                avg_loss = epoch_loss_sum / max(1, epoch_tokens)\n                avg_ppl  = math.exp(avg_loss)\n                avg_acc  = epoch_acc_sum / max(1, epoch_tokens)\n                tok_per_sec = epoch_tokens / (time.time() - t0 + 1e-9)\n                print(f\"[Epoch {epoch} | step {it:4d}/{len(train_loader)} | global_step={global_step}] \"\n                      f\"train_loss={avg_loss:.4f}  ppl={avg_ppl:.2f}  \"\n                      f\"tok_acc={avg_acc*100:.2f}%  tok/s={tok_per_sec:,.0f}\")\n\n\n            # Preview LM (teacher forcing, argmax)\n            if (preview_every is not None) and (id2tok_fn is not None) and (it % preview_every == 0):\n                with torch.no_grad():\n                    preds = logits.argmax(dim=-1)  # [B, T]\n                    b0 = 0\n\n                    in_ids  = x_ids[b0].tolist()\n                    tgt_ids = y_ids[b0].tolist()\n                    pred_ids= preds[b0].tolist()\n\n                    max_show = min(80, len(in_ids))\n                    in_ids   = in_ids[:max_show]\n                    tgt_ids  = tgt_ids[:max_show]\n                    pred_ids = pred_ids[:max_show]\n\n                    ctx = id2tok_fn(in_ids)\n                    ref = id2tok_fn(tgt_ids)\n                    hyp = id2tok_fn(pred_ids)\n\n                    print(\"— preview (LM, teacher-forced argmax) —\")\n                    print(\"CTX:\", repr(ctx))\n                    print(\"REF:\", repr(ref))\n                    print(\"HYP:\", repr(hyp))\n\n        \n        # Fin de epoch: promedios train\n        train_loss = epoch_loss_sum / max(1, epoch_tokens)\n        train_ppl  = math.exp(train_loss)\n        train_acc  = epoch_acc_sum / max(1, epoch_tokens)\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_ppl\"].append(train_ppl)\n        history[\"train_tok_acc\"].append(train_acc * 100.0)\n\n\n        # SIN VALIDACIÓN (val_checking == False o val_loader is None)\n        if (not val_checking) or (val_loader is None):\n            print(f\"Epoch {epoch} done | \"\n                  f\"train_loss={train_loss:.4f}  train_ppl={train_ppl:.2f}  \"\n                  f\"train_tok_acc={train_acc*100:.2f}%\")\n\n            # Guardar checkpoint cada 'save_ckpt_every' \n            if save_ckpt_every is not None and (epoch % save_ckpt_every == 0):\n                model_to_save = model.module if hasattr(model, \"module\") else model\n                torch.save({\n                    \"model_state\": model_to_save.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict(),\n                    \"epoch\": epoch}, ckpt_path)\n                \n                print(f\"Guardado checkpoint (cada {save_ckpt_every} epochs) -> {ckpt_path}\")\n\n            continue\n\n        # CON VALIDACIÓN (val_checking == True y val_loader no es None) \n        model.eval()\n        val_loss_sum, val_tokens = 0.0, 0\n        val_acc_sum = 0.0\n\n        with torch.no_grad():\n            for x_ids, y_ids in val_loader:\n                x_ids = x_ids.to(device, non_blocking=True)\n                y_ids = y_ids.to(device, non_blocking=True)\n                B, T = x_ids.shape\n                tokens = B * T\n\n                with autocast_ctx(device=device.type, enabled=amp_enabled, dtype=amp_dtype):\n                    logits, _ = model(x_ids, None)\n                    V = logits.size(-1)\n                    loss = ce_eval(\n                        logits.view(B * T, V),\n                        y_ids.view(B * T))\n\n                acc = token_acc(logits, y_ids)\n\n                val_loss_sum += loss.item() * tokens\n                val_acc_sum  += acc * tokens\n                val_tokens   += tokens\n\n        val_loss = val_loss_sum / max(1, val_tokens)\n        val_ppl  = math.exp(val_loss)\n        val_acc  = val_acc_sum / max(1, val_tokens)\n\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_ppl\"].append(val_ppl)\n        history[\"val_tok_acc\"].append(val_acc * 100.0)\n\n        print(f\"Epoch {epoch} done | \"\n              f\"train_loss={train_loss:.4f}  train_ppl={train_ppl:.2f}  train_tok_acc={train_acc*100:.2f}%  \"\n              f\"val_loss={val_loss:.4f}    val_ppl={val_ppl:.2f}    val_tok_acc={val_acc*100:.2f}%\")\n\n        # Guardar mejor checkpoint por val_loss\n        if val_loss < best_val:\n            best_val = val_loss\n            model_to_save = model.module if hasattr(model, \"module\") else model\n            torch.save({\n                    \"model_state\": model_to_save.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict(),\n                    \"epoch\": epoch,\n                    \"val_loss\": val_loss,\n                }, ckpt_path)\n            \n            print(f\"Guardado checkpoint (best val_loss={val_loss:.4f}) -> {ckpt_path}\")\n\n    return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:51:51.362684Z","iopub.execute_input":"2025-11-18T05:51:51.363364Z","iopub.status.idle":"2025-11-18T05:51:51.384533Z","shell.execute_reply.started":"2025-11-18T05:51:51.363337Z","shell.execute_reply":"2025-11-18T05:51:51.383825Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.ipc_collect()\n\nimport torch, gc\ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab_size()\nblock_size = 256\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = GPTMini(\n    vocab_size=vocab_size,\n    block_size=block_size,  \n    n_layer=10,             \n    n_head=8,            \n    d_model=512,dropout=0.1,).to(device)\n\nif torch.cuda.device_count() > 1:\n    print(f\"Usando {torch.cuda.device_count()} GPUs con DataParallel\")\n    model = torch.nn.DataParallel(model)\n    use_dataparallel = True\n\ndef id2tok_fn(ids):\n    return tokenizer.decode(ids)\n\n\nhistory = train_gpt_lm(\n    model,\n    train_loader,\n    val_loader=val_loader,\n    epochs=10,\n    base_lr=3e-4,\n    weight_decay=0.01,\n    warmup_steps=2000,\n    label_smoothing=0.1,\n    grad_clip=1.0,\n    device=device,\n    ckpt_path=\"gptmini_owt10k.pt\",\n    log_every=150,\n    preview_every=7000,\n    id2tok_fn=id2tok_fn,\n    amp_enabled=True,\n    amp_dtype=\"fp16\",   \n    val_checking = False , save_ckpt_every = 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:55:20.015073Z","iopub.execute_input":"2025-11-18T06:55:20.015334Z","iopub.status.idle":"2025-11-18T13:08:18.452811Z","shell.execute_reply.started":"2025-11-18T06:55:20.015319Z","shell.execute_reply":"2025-11-18T13:08:18.451914Z"}},"outputs":[{"name":"stdout","text":"Usando 2 GPUs con DataParallel\n[Epoch 1 | step  150/7267 | global_step=150] train_loss=8.9506  ppl=7712.62  tok_acc=4.44%  tok/s=51,953\n[Epoch 1 | step  300/7267 | global_step=300] train_loss=8.2313  ppl=3756.75  tok_acc=6.35%  tok/s=52,936\n[Epoch 1 | step  450/7267 | global_step=450] train_loss=7.7675  ppl=2362.63  tok_acc=8.10%  tok/s=52,863\n[Epoch 1 | step  600/7267 | global_step=600] train_loss=7.4614  ppl=1739.52  tok_acc=9.40%  tok/s=53,006\n[Epoch 1 | step  750/7267 | global_step=750] train_loss=7.2223  ppl=1369.65  tok_acc=10.55%  tok/s=53,011\n[Epoch 1 | step  900/7267 | global_step=900] train_loss=7.0297  ppl=1129.71  tok_acc=11.52%  tok/s=53,081\n[Epoch 1 | step 1050/7267 | global_step=1050] train_loss=6.8694  ppl=962.41  tok_acc=12.33%  tok/s=53,106\n[Epoch 1 | step 1200/7267 | global_step=1200] train_loss=6.7319  ppl=838.73  tok_acc=13.03%  tok/s=53,142\n[Epoch 1 | step 1350/7267 | global_step=1350] train_loss=6.6111  ppl=743.31  tok_acc=13.65%  tok/s=53,136\n[Epoch 1 | step 1500/7267 | global_step=1500] train_loss=6.5029  ppl=667.09  tok_acc=14.23%  tok/s=53,153\n[Epoch 1 | step 1650/7267 | global_step=1650] train_loss=6.4041  ppl=604.33  tok_acc=14.75%  tok/s=53,170\n[Epoch 1 | step 1800/7267 | global_step=1800] train_loss=6.3129  ppl=551.66  tok_acc=15.24%  tok/s=53,161\n[Epoch 1 | step 1950/7267 | global_step=1950] train_loss=6.2291  ppl=507.29  tok_acc=15.70%  tok/s=53,178\n[Epoch 1 | step 2100/7267 | global_step=2100] train_loss=6.1504  ppl=468.92  tok_acc=16.14%  tok/s=53,168\n[Epoch 1 | step 2250/7267 | global_step=2250] train_loss=6.0761  ppl=435.34  tok_acc=16.57%  tok/s=53,176\n[Epoch 1 | step 2400/7267 | global_step=2400] train_loss=6.0066  ppl=406.11  tok_acc=16.98%  tok/s=53,180\n[Epoch 1 | step 2550/7267 | global_step=2550] train_loss=5.9406  ppl=380.16  tok_acc=17.36%  tok/s=53,197\n[Epoch 1 | step 2700/7267 | global_step=2700] train_loss=5.8784  ppl=357.23  tok_acc=17.74%  tok/s=53,188\n[Epoch 1 | step 2850/7267 | global_step=2850] train_loss=5.8187  ppl=336.54  tok_acc=18.11%  tok/s=53,194\n[Epoch 1 | step 3000/7267 | global_step=3000] train_loss=5.7620  ppl=318.00  tok_acc=18.48%  tok/s=53,193\n[Epoch 1 | step 3150/7267 | global_step=3150] train_loss=5.7073  ppl=301.05  tok_acc=18.85%  tok/s=53,202\n[Epoch 1 | step 3300/7267 | global_step=3300] train_loss=5.6546  ppl=285.60  tok_acc=19.22%  tok/s=53,198\n[Epoch 1 | step 3450/7267 | global_step=3450] train_loss=5.6041  ppl=271.55  tok_acc=19.58%  tok/s=53,203\n[Epoch 1 | step 3600/7267 | global_step=3600] train_loss=5.5559  ppl=258.77  tok_acc=19.94%  tok/s=53,208\n[Epoch 1 | step 3750/7267 | global_step=3750] train_loss=5.5103  ppl=247.22  tok_acc=20.27%  tok/s=53,212\n[Epoch 1 | step 3900/7267 | global_step=3900] train_loss=5.4666  ppl=236.64  tok_acc=20.60%  tok/s=53,204\n[Epoch 1 | step 4050/7267 | global_step=4050] train_loss=5.4249  ppl=226.98  tok_acc=20.91%  tok/s=53,208\n[Epoch 1 | step 4200/7267 | global_step=4200] train_loss=5.3850  ppl=218.11  tok_acc=21.21%  tok/s=53,211\n[Epoch 1 | step 4350/7267 | global_step=4350] train_loss=5.3471  ppl=209.99  tok_acc=21.50%  tok/s=53,206\n[Epoch 1 | step 4500/7267 | global_step=4500] train_loss=5.3106  ppl=202.46  tok_acc=21.78%  tok/s=53,208\n[Epoch 1 | step 4650/7267 | global_step=4650] train_loss=5.2758  ppl=195.56  tok_acc=22.04%  tok/s=53,200\n[Epoch 1 | step 4800/7267 | global_step=4800] train_loss=5.2422  ppl=189.09  tok_acc=22.30%  tok/s=53,200\n[Epoch 1 | step 4950/7267 | global_step=4950] train_loss=5.2101  ppl=183.10  tok_acc=22.55%  tok/s=53,200\n[Epoch 1 | step 5100/7267 | global_step=5100] train_loss=5.1791  ppl=177.53  tok_acc=22.79%  tok/s=53,200\n[Epoch 1 | step 5250/7267 | global_step=5250] train_loss=5.1496  ppl=172.36  tok_acc=23.02%  tok/s=53,198\n[Epoch 1 | step 5400/7267 | global_step=5400] train_loss=5.1211  ppl=167.52  tok_acc=23.24%  tok/s=53,197\n[Epoch 1 | step 5550/7267 | global_step=5550] train_loss=5.0937  ppl=163.00  tok_acc=23.46%  tok/s=53,200\n[Epoch 1 | step 5700/7267 | global_step=5700] train_loss=5.0673  ppl=158.75  tok_acc=23.67%  tok/s=53,191\n[Epoch 1 | step 5850/7267 | global_step=5850] train_loss=5.0420  ppl=154.78  tok_acc=23.87%  tok/s=53,191\n[Epoch 1 | step 6000/7267 | global_step=6000] train_loss=5.0173  ppl=151.01  tok_acc=24.06%  tok/s=53,184\n[Epoch 1 | step 6150/7267 | global_step=6150] train_loss=4.9939  ppl=147.51  tok_acc=24.25%  tok/s=53,184\n[Epoch 1 | step 6300/7267 | global_step=6300] train_loss=4.9712  ppl=144.19  tok_acc=24.43%  tok/s=53,181\n[Epoch 1 | step 6450/7267 | global_step=6450] train_loss=4.9490  ppl=141.03  tok_acc=24.60%  tok/s=53,181\n[Epoch 1 | step 6600/7267 | global_step=6600] train_loss=4.9275  ppl=138.04  tok_acc=24.77%  tok/s=53,171\n[Epoch 1 | step 6750/7267 | global_step=6750] train_loss=4.9069  ppl=135.22  tok_acc=24.94%  tok/s=53,175\n[Epoch 1 | step 6900/7267 | global_step=6900] train_loss=4.8868  ppl=132.53  tok_acc=25.10%  tok/s=53,178\n— preview (LM, teacher-forced argmax) —\nCTX: \" lieu of cash taxes ) to mobilize the populace in panning for gold in the kingdom 's rivers . this expense , coupled with rainilaiarivony 's removal of $ 50 @,@ 000 in silver and gold coins from the tomb of ranavalona i to offset the cost of purchasing arms in the run @-@ up to the first franco @-@ hova war , effectively empt\"\nREF: \"u of cash taxes ) to mobilize the populace in panning for gold in the kingdom 's rivers . this expense , coupled with rainilaiarivony 's removal of $ 50 @,@ 000 in silver and gold coins from the tomb of ranavalona i to offset the cost of purchasing arms in the run @-@ up to the first franco @-@ hova war , effectively emptied\"\nHYP: ' to , the , , , theize the governmentace of thehandu the , the united ofs territories . \\n was was which with the andization ,ismism ,s lack of the 1 million 000 in cash , silver , , the united of theuona , , theend the expansion of the the . the city of up to the city half @-@ spanishul war . and allowingied'\n[Epoch 1 | step 7050/7267 | global_step=7050] train_loss=4.8674  ppl=129.98  tok_acc=25.26%  tok/s=53,174\n[Epoch 1 | step 7200/7267 | global_step=7200] train_loss=4.8485  ppl=127.55  tok_acc=25.41%  tok/s=53,170\nEpoch 1 done | train_loss=4.8403  train_ppl=126.51  train_tok_acc=25.47%\n[Epoch 2 | step  150/7267 | global_step=7417] train_loss=3.9157  ppl=50.18  tok_acc=32.95%  tok/s=53,038\n[Epoch 2 | step  300/7267 | global_step=7567] train_loss=3.9144  ppl=50.12  tok_acc=32.94%  tok/s=53,172\n[Epoch 2 | step  450/7267 | global_step=7717] train_loss=3.9119  ppl=49.99  tok_acc=32.98%  tok/s=53,192\n[Epoch 2 | step  600/7267 | global_step=7867] train_loss=3.9082  ppl=49.81  tok_acc=33.02%  tok/s=53,156\n[Epoch 2 | step  750/7267 | global_step=8017] train_loss=3.9053  ppl=49.67  tok_acc=33.04%  tok/s=53,178\n[Epoch 2 | step  900/7267 | global_step=8167] train_loss=3.9038  ppl=49.59  tok_acc=33.06%  tok/s=53,210\n[Epoch 2 | step 1050/7267 | global_step=8317] train_loss=3.8997  ppl=49.39  tok_acc=33.10%  tok/s=53,195\n[Epoch 2 | step 1200/7267 | global_step=8467] train_loss=3.8964  ppl=49.22  tok_acc=33.13%  tok/s=53,187\n[Epoch 2 | step 1350/7267 | global_step=8617] train_loss=3.8923  ppl=49.02  tok_acc=33.16%  tok/s=53,167\n[Epoch 2 | step 1500/7267 | global_step=8767] train_loss=3.8892  ppl=48.87  tok_acc=33.20%  tok/s=53,167\n[Epoch 2 | step 1650/7267 | global_step=8917] train_loss=3.8856  ppl=48.69  tok_acc=33.24%  tok/s=53,178\n[Epoch 2 | step 1800/7267 | global_step=9067] train_loss=3.8822  ppl=48.53  tok_acc=33.27%  tok/s=53,187\n[Epoch 2 | step 1950/7267 | global_step=9217] train_loss=3.8791  ppl=48.38  tok_acc=33.30%  tok/s=53,181\n[Epoch 2 | step 2100/7267 | global_step=9367] train_loss=3.8756  ppl=48.21  tok_acc=33.34%  tok/s=53,199\n[Epoch 2 | step 2250/7267 | global_step=9517] train_loss=3.8725  ppl=48.06  tok_acc=33.37%  tok/s=53,182\n[Epoch 2 | step 2400/7267 | global_step=9667] train_loss=3.8698  ppl=47.93  tok_acc=33.40%  tok/s=53,192\n[Epoch 2 | step 2550/7267 | global_step=9817] train_loss=3.8667  ppl=47.79  tok_acc=33.43%  tok/s=53,186\n[Epoch 2 | step 2700/7267 | global_step=9967] train_loss=3.8638  ppl=47.64  tok_acc=33.46%  tok/s=53,195\n[Epoch 2 | step 2850/7267 | global_step=10117] train_loss=3.8607  ppl=47.50  tok_acc=33.49%  tok/s=53,196\n[Epoch 2 | step 3000/7267 | global_step=10267] train_loss=3.8577  ppl=47.35  tok_acc=33.52%  tok/s=53,200\n[Epoch 2 | step 3150/7267 | global_step=10417] train_loss=3.8545  ppl=47.21  tok_acc=33.55%  tok/s=53,204\n[Epoch 2 | step 3300/7267 | global_step=10567] train_loss=3.8515  ppl=47.06  tok_acc=33.58%  tok/s=53,193\n[Epoch 2 | step 3450/7267 | global_step=10717] train_loss=3.8483  ppl=46.91  tok_acc=33.62%  tok/s=53,198\n[Epoch 2 | step 3600/7267 | global_step=10867] train_loss=3.8454  ppl=46.78  tok_acc=33.64%  tok/s=53,197\n[Epoch 2 | step 3750/7267 | global_step=11017] train_loss=3.8427  ppl=46.65  tok_acc=33.67%  tok/s=53,201\n[Epoch 2 | step 3900/7267 | global_step=11167] train_loss=3.8397  ppl=46.51  tok_acc=33.70%  tok/s=53,195\n[Epoch 2 | step 4050/7267 | global_step=11317] train_loss=3.8367  ppl=46.37  tok_acc=33.73%  tok/s=53,201\n[Epoch 2 | step 4200/7267 | global_step=11467] train_loss=3.8341  ppl=46.25  tok_acc=33.76%  tok/s=53,203\n[Epoch 2 | step 4350/7267 | global_step=11617] train_loss=3.8316  ppl=46.14  tok_acc=33.78%  tok/s=53,200\n[Epoch 2 | step 4500/7267 | global_step=11767] train_loss=3.8293  ppl=46.03  tok_acc=33.81%  tok/s=53,192\n[Epoch 2 | step 4650/7267 | global_step=11917] train_loss=3.8264  ppl=45.90  tok_acc=33.84%  tok/s=53,191\n[Epoch 2 | step 4800/7267 | global_step=12067] train_loss=3.8242  ppl=45.80  tok_acc=33.86%  tok/s=53,190\n[Epoch 2 | step 4950/7267 | global_step=12217] train_loss=3.8216  ppl=45.68  tok_acc=33.89%  tok/s=53,183\n[Epoch 2 | step 5100/7267 | global_step=12367] train_loss=3.8191  ppl=45.56  tok_acc=33.91%  tok/s=53,186\n[Epoch 2 | step 5250/7267 | global_step=12517] train_loss=3.8162  ppl=45.43  tok_acc=33.94%  tok/s=53,182\n[Epoch 2 | step 5400/7267 | global_step=12667] train_loss=3.8134  ppl=45.30  tok_acc=33.96%  tok/s=53,182\n[Epoch 2 | step 5550/7267 | global_step=12817] train_loss=3.8108  ppl=45.19  tok_acc=33.99%  tok/s=53,182\n[Epoch 2 | step 5700/7267 | global_step=12967] train_loss=3.8084  ppl=45.08  tok_acc=34.01%  tok/s=53,185\n[Epoch 2 | step 5850/7267 | global_step=13117] train_loss=3.8059  ppl=44.97  tok_acc=34.04%  tok/s=53,184\n[Epoch 2 | step 6000/7267 | global_step=13267] train_loss=3.8034  ppl=44.85  tok_acc=34.06%  tok/s=53,187\n[Epoch 2 | step 6150/7267 | global_step=13417] train_loss=3.8009  ppl=44.74  tok_acc=34.09%  tok/s=53,189\n[Epoch 2 | step 6300/7267 | global_step=13567] train_loss=3.7986  ppl=44.64  tok_acc=34.11%  tok/s=53,187\n[Epoch 2 | step 6450/7267 | global_step=13717] train_loss=3.7961  ppl=44.53  tok_acc=34.14%  tok/s=53,188\n[Epoch 2 | step 6600/7267 | global_step=13867] train_loss=3.7938  ppl=44.43  tok_acc=34.16%  tok/s=53,187\n[Epoch 2 | step 6750/7267 | global_step=14017] train_loss=3.7916  ppl=44.33  tok_acc=34.18%  tok/s=53,192\n[Epoch 2 | step 6900/7267 | global_step=14167] train_loss=3.7892  ppl=44.22  tok_acc=34.21%  tok/s=53,190\n— preview (LM, teacher-forced argmax) —\nCTX: 'ates and sounds with inherent secondary articulation have also been mostly rejected , with the idea that such features should be indicated with tie bars or diacritics : 〈 ƍ 〉 for [ zw ] is one . in addition , the rare voiceless implosives , 〈 ƥ ƭ ƈ ƙ ʠ 〉 , have'\nREF: ' and sounds with inherent secondary articulation have also been mostly rejected , with the idea that such features should be indicated with tie bars or diacritics : 〈 ƍ 〉 for [ zw ] is one . in addition , the rare voiceless implosives , 〈 ƥ ƭ ƈ ƙ ʠ 〉 , have been'\nHYP: ' , the like thely soundsulation . been been used used . but the use of the a are be used by a @-@ . aameritics . the� �͡ 〉 � � � ] ] , a of \\n the to the term andiceless isantsome are which� ư �ư �ư �ư �Ɗ 〉 for � been'\n[Epoch 2 | step 7050/7267 | global_step=14317] train_loss=3.7871  ppl=44.13  tok_acc=34.23%  tok/s=53,191\n[Epoch 2 | step 7200/7267 | global_step=14467] train_loss=3.7848  ppl=44.03  tok_acc=34.25%  tok/s=53,187\nEpoch 2 done | train_loss=3.7839  train_ppl=43.99  train_tok_acc=34.26%\n[Epoch 3 | step  150/7267 | global_step=14684] train_loss=3.6283  ppl=37.65  tok_acc=35.78%  tok/s=53,075\n[Epoch 3 | step  300/7267 | global_step=14834] train_loss=3.6295  ppl=37.69  tok_acc=35.75%  tok/s=52,940\n[Epoch 3 | step  450/7267 | global_step=14984] train_loss=3.6269  ppl=37.59  tok_acc=35.77%  tok/s=53,032\n[Epoch 3 | step  600/7267 | global_step=15134] train_loss=3.6273  ppl=37.61  tok_acc=35.76%  tok/s=53,030\n[Epoch 3 | step  750/7267 | global_step=15284] train_loss=3.6265  ppl=37.58  tok_acc=35.79%  tok/s=53,073\n[Epoch 3 | step  900/7267 | global_step=15434] train_loss=3.6256  ppl=37.55  tok_acc=35.80%  tok/s=53,102\n[Epoch 3 | step 1050/7267 | global_step=15584] train_loss=3.6256  ppl=37.55  tok_acc=35.80%  tok/s=53,124\n[Epoch 3 | step 1200/7267 | global_step=15734] train_loss=3.6253  ppl=37.54  tok_acc=35.81%  tok/s=53,130\n[Epoch 3 | step 1350/7267 | global_step=15884] train_loss=3.6251  ppl=37.53  tok_acc=35.81%  tok/s=53,133\n[Epoch 3 | step 1500/7267 | global_step=16034] train_loss=3.6253  ppl=37.54  tok_acc=35.81%  tok/s=53,142\n[Epoch 3 | step 1650/7267 | global_step=16184] train_loss=3.6237  ppl=37.48  tok_acc=35.83%  tok/s=53,137\n[Epoch 3 | step 1800/7267 | global_step=16334] train_loss=3.6231  ppl=37.45  tok_acc=35.84%  tok/s=53,134\n[Epoch 3 | step 1950/7267 | global_step=16484] train_loss=3.6231  ppl=37.45  tok_acc=35.84%  tok/s=53,145\n[Epoch 3 | step 2100/7267 | global_step=16634] train_loss=3.6225  ppl=37.43  tok_acc=35.85%  tok/s=53,148\n[Epoch 3 | step 2250/7267 | global_step=16784] train_loss=3.6220  ppl=37.41  tok_acc=35.85%  tok/s=53,151\n[Epoch 3 | step 2400/7267 | global_step=16934] train_loss=3.6216  ppl=37.40  tok_acc=35.86%  tok/s=53,151\n[Epoch 3 | step 2550/7267 | global_step=17084] train_loss=3.6213  ppl=37.38  tok_acc=35.86%  tok/s=53,141\n[Epoch 3 | step 2700/7267 | global_step=17234] train_loss=3.6200  ppl=37.34  tok_acc=35.88%  tok/s=53,145\n[Epoch 3 | step 2850/7267 | global_step=17384] train_loss=3.6189  ppl=37.30  tok_acc=35.89%  tok/s=53,142\n[Epoch 3 | step 3000/7267 | global_step=17534] train_loss=3.6184  ppl=37.28  tok_acc=35.90%  tok/s=53,144\n[Epoch 3 | step 3150/7267 | global_step=17684] train_loss=3.6175  ppl=37.25  tok_acc=35.91%  tok/s=53,140\n[Epoch 3 | step 3300/7267 | global_step=17834] train_loss=3.6168  ppl=37.22  tok_acc=35.92%  tok/s=53,133\n[Epoch 3 | step 3450/7267 | global_step=17984] train_loss=3.6155  ppl=37.17  tok_acc=35.93%  tok/s=53,138\n[Epoch 3 | step 3600/7267 | global_step=18134] train_loss=3.6145  ppl=37.13  tok_acc=35.94%  tok/s=53,136\n[Epoch 3 | step 3750/7267 | global_step=18284] train_loss=3.6138  ppl=37.11  tok_acc=35.95%  tok/s=53,134\n[Epoch 3 | step 3900/7267 | global_step=18434] train_loss=3.6129  ppl=37.07  tok_acc=35.96%  tok/s=53,131\n[Epoch 3 | step 4050/7267 | global_step=18584] train_loss=3.6121  ppl=37.04  tok_acc=35.97%  tok/s=53,129\n[Epoch 3 | step 4200/7267 | global_step=18734] train_loss=3.6112  ppl=37.01  tok_acc=35.98%  tok/s=53,123\n[Epoch 3 | step 4350/7267 | global_step=18884] train_loss=3.6107  ppl=36.99  tok_acc=35.99%  tok/s=53,127\n[Epoch 3 | step 4500/7267 | global_step=19034] train_loss=3.6099  ppl=36.96  tok_acc=35.99%  tok/s=53,123\n[Epoch 3 | step 4650/7267 | global_step=19184] train_loss=3.6091  ppl=36.93  tok_acc=36.00%  tok/s=53,125\n[Epoch 3 | step 4800/7267 | global_step=19334] train_loss=3.6087  ppl=36.92  tok_acc=36.01%  tok/s=53,127\n[Epoch 3 | step 4950/7267 | global_step=19484] train_loss=3.6078  ppl=36.88  tok_acc=36.02%  tok/s=53,129\n[Epoch 3 | step 5100/7267 | global_step=19634] train_loss=3.6066  ppl=36.84  tok_acc=36.03%  tok/s=53,126\n[Epoch 3 | step 5250/7267 | global_step=19784] train_loss=3.6057  ppl=36.81  tok_acc=36.04%  tok/s=53,129\n[Epoch 3 | step 5400/7267 | global_step=19934] train_loss=3.6047  ppl=36.77  tok_acc=36.05%  tok/s=53,133\n[Epoch 3 | step 5550/7267 | global_step=20084] train_loss=3.6039  ppl=36.74  tok_acc=36.06%  tok/s=53,133\n[Epoch 3 | step 5700/7267 | global_step=20234] train_loss=3.6029  ppl=36.71  tok_acc=36.07%  tok/s=53,132\n[Epoch 3 | step 5850/7267 | global_step=20384] train_loss=3.6023  ppl=36.68  tok_acc=36.08%  tok/s=53,129\n[Epoch 3 | step 6000/7267 | global_step=20534] train_loss=3.6015  ppl=36.65  tok_acc=36.09%  tok/s=53,130\n[Epoch 3 | step 6150/7267 | global_step=20684] train_loss=3.6007  ppl=36.62  tok_acc=36.10%  tok/s=53,130\n[Epoch 3 | step 6300/7267 | global_step=20834] train_loss=3.6000  ppl=36.60  tok_acc=36.11%  tok/s=53,133\n[Epoch 3 | step 6450/7267 | global_step=20984] train_loss=3.5992  ppl=36.57  tok_acc=36.12%  tok/s=53,134\n[Epoch 3 | step 6600/7267 | global_step=21134] train_loss=3.5983  ppl=36.54  tok_acc=36.13%  tok/s=53,137\n[Epoch 3 | step 6750/7267 | global_step=21284] train_loss=3.5974  ppl=36.50  tok_acc=36.14%  tok/s=53,139\n[Epoch 3 | step 6900/7267 | global_step=21434] train_loss=3.5964  ppl=36.47  tok_acc=36.15%  tok/s=53,144\n— preview (LM, teacher-forced argmax) —\nCTX: ' in 1923 when chase died . ( in 1924 , after completing his studies at occidental college in los angeles , jaeger began a 30 @-@ year teaching career at riverside junior college in riverside , california . ) jaeger wrote the initial eulogy for eytel upon his death and in 1948 , recalling his time with him , jaeger said : \\n as'\nREF: ' 1923 when chase died . ( in 1924 , after completing his studies at occidental college in los angeles , jaeger began a 30 @-@ year teaching career at riverside junior college in riverside , california . ) jaeger wrote the initial eulogy for eytel upon his death and in 1948 , recalling his time with him , jaeger said : \\n as an'\nHYP: ' the , the was of \\n the the , the a the studies at theultal college , new angeles , california.ger moved to career @-@ year course career at theide high high in newide , california . he \\naeger was a first script @-@ogy , theelel , his death in his 1925 he whening the experience at the , \"aeger said , \" i a'\n[Epoch 3 | step 7050/7267 | global_step=21584] train_loss=3.5955  ppl=36.43  tok_acc=36.16%  tok/s=53,144\n[Epoch 3 | step 7200/7267 | global_step=21734] train_loss=3.5947  ppl=36.40  tok_acc=36.16%  tok/s=53,148\nEpoch 3 done | train_loss=3.5943  train_ppl=36.39  train_tok_acc=36.17%\nGuardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n[Epoch 4 | step  150/7267 | global_step=21951] train_loss=3.5011  ppl=33.15  tok_acc=37.06%  tok/s=53,068\n[Epoch 4 | step  300/7267 | global_step=22101] train_loss=3.4976  ppl=33.04  tok_acc=37.13%  tok/s=53,213\n[Epoch 4 | step  450/7267 | global_step=22251] train_loss=3.4995  ppl=33.10  tok_acc=37.13%  tok/s=53,151\n[Epoch 4 | step  600/7267 | global_step=22401] train_loss=3.4999  ppl=33.11  tok_acc=37.11%  tok/s=53,203\n[Epoch 4 | step  750/7267 | global_step=22551] train_loss=3.5014  ppl=33.16  tok_acc=37.10%  tok/s=53,186\n[Epoch 4 | step  900/7267 | global_step=22701] train_loss=3.5025  ppl=33.20  tok_acc=37.09%  tok/s=53,214\n[Epoch 4 | step 1050/7267 | global_step=22851] train_loss=3.5034  ppl=33.23  tok_acc=37.07%  tok/s=53,186\n[Epoch 4 | step 1200/7267 | global_step=23001] train_loss=3.5038  ppl=33.24  tok_acc=37.07%  tok/s=53,212\n[Epoch 4 | step 1350/7267 | global_step=23151] train_loss=3.5039  ppl=33.24  tok_acc=37.07%  tok/s=53,223\n[Epoch 4 | step 1500/7267 | global_step=23301] train_loss=3.5046  ppl=33.27  tok_acc=37.07%  tok/s=53,242\n[Epoch 4 | step 1650/7267 | global_step=23451] train_loss=3.5042  ppl=33.25  tok_acc=37.08%  tok/s=53,257\n[Epoch 4 | step 1800/7267 | global_step=23601] train_loss=3.5035  ppl=33.23  tok_acc=37.09%  tok/s=53,255\n[Epoch 4 | step 1950/7267 | global_step=23751] train_loss=3.5041  ppl=33.25  tok_acc=37.08%  tok/s=53,249\n[Epoch 4 | step 2100/7267 | global_step=23901] train_loss=3.5035  ppl=33.23  tok_acc=37.09%  tok/s=53,243\n[Epoch 4 | step 2250/7267 | global_step=24051] train_loss=3.5031  ppl=33.22  tok_acc=37.10%  tok/s=53,254\n[Epoch 4 | step 2400/7267 | global_step=24201] train_loss=3.5032  ppl=33.22  tok_acc=37.10%  tok/s=53,255\n[Epoch 4 | step 2550/7267 | global_step=24351] train_loss=3.5031  ppl=33.22  tok_acc=37.10%  tok/s=53,262\n[Epoch 4 | step 2700/7267 | global_step=24501] train_loss=3.5026  ppl=33.20  tok_acc=37.11%  tok/s=53,267\n[Epoch 4 | step 2850/7267 | global_step=24651] train_loss=3.5020  ppl=33.18  tok_acc=37.12%  tok/s=53,277\n[Epoch 4 | step 3000/7267 | global_step=24801] train_loss=3.5016  ppl=33.17  tok_acc=37.12%  tok/s=53,260\n[Epoch 4 | step 3150/7267 | global_step=24951] train_loss=3.5012  ppl=33.16  tok_acc=37.13%  tok/s=53,262\n[Epoch 4 | step 3300/7267 | global_step=25101] train_loss=3.5012  ppl=33.15  tok_acc=37.13%  tok/s=53,262\n[Epoch 4 | step 3450/7267 | global_step=25251] train_loss=3.5008  ppl=33.14  tok_acc=37.13%  tok/s=53,246\n[Epoch 4 | step 3600/7267 | global_step=25401] train_loss=3.5004  ppl=33.13  tok_acc=37.14%  tok/s=53,245\n[Epoch 4 | step 3750/7267 | global_step=25551] train_loss=3.5002  ppl=33.12  tok_acc=37.14%  tok/s=53,235\n[Epoch 4 | step 3900/7267 | global_step=25701] train_loss=3.5003  ppl=33.12  tok_acc=37.14%  tok/s=53,241\n[Epoch 4 | step 4050/7267 | global_step=25851] train_loss=3.5001  ppl=33.12  tok_acc=37.14%  tok/s=53,244\n[Epoch 4 | step 4200/7267 | global_step=26001] train_loss=3.4997  ppl=33.10  tok_acc=37.15%  tok/s=53,248\n[Epoch 4 | step 4350/7267 | global_step=26151] train_loss=3.4991  ppl=33.08  tok_acc=37.16%  tok/s=53,245\n[Epoch 4 | step 4500/7267 | global_step=26301] train_loss=3.4989  ppl=33.08  tok_acc=37.16%  tok/s=53,249\n[Epoch 4 | step 4650/7267 | global_step=26451] train_loss=3.4986  ppl=33.07  tok_acc=37.16%  tok/s=53,252\n[Epoch 4 | step 4800/7267 | global_step=26601] train_loss=3.4981  ppl=33.05  tok_acc=37.17%  tok/s=53,249\n[Epoch 4 | step 4950/7267 | global_step=26751] train_loss=3.4976  ppl=33.03  tok_acc=37.18%  tok/s=53,254\n[Epoch 4 | step 5100/7267 | global_step=26901] train_loss=3.4970  ppl=33.01  tok_acc=37.19%  tok/s=53,251\n[Epoch 4 | step 5250/7267 | global_step=27051] train_loss=3.4967  ppl=33.01  tok_acc=37.19%  tok/s=53,255\n[Epoch 4 | step 5400/7267 | global_step=27201] train_loss=3.4965  ppl=33.00  tok_acc=37.19%  tok/s=53,256\n[Epoch 4 | step 5550/7267 | global_step=27351] train_loss=3.4961  ppl=32.99  tok_acc=37.20%  tok/s=53,259\n[Epoch 4 | step 5700/7267 | global_step=27501] train_loss=3.4959  ppl=32.98  tok_acc=37.20%  tok/s=53,254\n[Epoch 4 | step 5850/7267 | global_step=27651] train_loss=3.4953  ppl=32.96  tok_acc=37.21%  tok/s=53,256\n[Epoch 4 | step 6000/7267 | global_step=27801] train_loss=3.4949  ppl=32.95  tok_acc=37.22%  tok/s=53,253\n[Epoch 4 | step 6150/7267 | global_step=27951] train_loss=3.4946  ppl=32.94  tok_acc=37.22%  tok/s=53,254\n[Epoch 4 | step 6300/7267 | global_step=28101] train_loss=3.4944  ppl=32.93  tok_acc=37.22%  tok/s=53,250\n[Epoch 4 | step 6450/7267 | global_step=28251] train_loss=3.4939  ppl=32.91  tok_acc=37.23%  tok/s=53,253\n[Epoch 4 | step 6600/7267 | global_step=28401] train_loss=3.4935  ppl=32.90  tok_acc=37.23%  tok/s=53,253\n[Epoch 4 | step 6750/7267 | global_step=28551] train_loss=3.4932  ppl=32.89  tok_acc=37.24%  tok/s=53,254\n[Epoch 4 | step 6900/7267 | global_step=28701] train_loss=3.4927  ppl=32.87  tok_acc=37.25%  tok/s=53,250\n— preview (LM, teacher-forced argmax) —\nCTX: ' . the title track , a \" densely produced blast of layered vocals [ and ] strummed acoustic guitars \" , features a \" circling bass riff \" similar to that of \" picture book \" by the kinks . \" waiting \" , which has been categorized as a \" retro @-@ pop lament \" , is based on the riff from petula clark \\'s 1964 song \" downtown'\nREF: ' the title track , a \" densely produced blast of layered vocals [ and ] strummed acoustic guitars \" , features a \" circling bass riff \" similar to that of \" picture book \" by the kinks . \" waiting \" , which has been categorized as a \" retro @-@ pop lament \" , is based on the riff from petula clark \\'s 1964 song \" downtown \"'\nHYP: ' the first of was \" song songer @-@ track \" theered \" \" and ] autmer vocals guitar \" , was a guitar heavyqueing @-@ guitar \" . to the of the the of \" . the bandks . the the for was a features a describedized as a \" rock @-@ rock song \" , is a on the song of the sounds , \\'s song song \" the \"'\n[Epoch 4 | step 7050/7267 | global_step=28851] train_loss=3.4922  ppl=32.86  tok_acc=37.25%  tok/s=53,252\n[Epoch 4 | step 7200/7267 | global_step=29001] train_loss=3.4917  ppl=32.84  tok_acc=37.26%  tok/s=53,251\nEpoch 4 done | train_loss=3.4914  train_ppl=32.83  train_tok_acc=37.26%\n[Epoch 5 | step  150/7267 | global_step=29218] train_loss=3.4181  ppl=30.51  tok_acc=38.00%  tok/s=52,733\n[Epoch 5 | step  300/7267 | global_step=29368] train_loss=3.4178  ppl=30.50  tok_acc=38.01%  tok/s=53,040\n[Epoch 5 | step  450/7267 | global_step=29518] train_loss=3.4188  ppl=30.53  tok_acc=37.98%  tok/s=53,063\n[Epoch 5 | step  600/7267 | global_step=29668] train_loss=3.4174  ppl=30.49  tok_acc=38.01%  tok/s=53,114\n[Epoch 5 | step  750/7267 | global_step=29818] train_loss=3.4169  ppl=30.48  tok_acc=38.02%  tok/s=53,164\n[Epoch 5 | step  900/7267 | global_step=29968] train_loss=3.4171  ppl=30.48  tok_acc=38.02%  tok/s=53,190\n[Epoch 5 | step 1050/7267 | global_step=30118] train_loss=3.4184  ppl=30.52  tok_acc=38.01%  tok/s=53,188\n[Epoch 5 | step 1200/7267 | global_step=30268] train_loss=3.4198  ppl=30.56  tok_acc=37.99%  tok/s=53,200\n[Epoch 5 | step 1350/7267 | global_step=30418] train_loss=3.4202  ppl=30.57  tok_acc=37.99%  tok/s=53,199\n[Epoch 5 | step 1500/7267 | global_step=30568] train_loss=3.4209  ppl=30.60  tok_acc=37.98%  tok/s=53,215\n[Epoch 5 | step 1650/7267 | global_step=30718] train_loss=3.4213  ppl=30.61  tok_acc=37.98%  tok/s=53,208\n[Epoch 5 | step 1800/7267 | global_step=30868] train_loss=3.4219  ppl=30.63  tok_acc=37.97%  tok/s=53,216\n[Epoch 5 | step 1950/7267 | global_step=31018] train_loss=3.4223  ppl=30.64  tok_acc=37.97%  tok/s=53,225\n[Epoch 5 | step 2100/7267 | global_step=31168] train_loss=3.4226  ppl=30.65  tok_acc=37.97%  tok/s=53,229\n[Epoch 5 | step 2250/7267 | global_step=31318] train_loss=3.4230  ppl=30.66  tok_acc=37.97%  tok/s=53,229\n[Epoch 5 | step 2400/7267 | global_step=31468] train_loss=3.4236  ppl=30.68  tok_acc=37.96%  tok/s=53,241\n[Epoch 5 | step 2550/7267 | global_step=31618] train_loss=3.4234  ppl=30.67  tok_acc=37.97%  tok/s=53,244\n[Epoch 5 | step 2700/7267 | global_step=31768] train_loss=3.4234  ppl=30.67  tok_acc=37.97%  tok/s=53,250\n[Epoch 5 | step 2850/7267 | global_step=31918] train_loss=3.4229  ppl=30.66  tok_acc=37.98%  tok/s=53,252\n[Epoch 5 | step 3000/7267 | global_step=32068] train_loss=3.4231  ppl=30.67  tok_acc=37.98%  tok/s=53,255\n[Epoch 5 | step 3150/7267 | global_step=32218] train_loss=3.4233  ppl=30.67  tok_acc=37.98%  tok/s=53,261\n[Epoch 5 | step 3300/7267 | global_step=32368] train_loss=3.4231  ppl=30.67  tok_acc=37.98%  tok/s=53,261\n[Epoch 5 | step 3450/7267 | global_step=32518] train_loss=3.4233  ppl=30.67  tok_acc=37.98%  tok/s=53,266\n[Epoch 5 | step 3600/7267 | global_step=32668] train_loss=3.4230  ppl=30.66  tok_acc=37.99%  tok/s=53,262\n[Epoch 5 | step 3750/7267 | global_step=32818] train_loss=3.4225  ppl=30.64  tok_acc=38.00%  tok/s=53,266\n[Epoch 5 | step 3900/7267 | global_step=32968] train_loss=3.4224  ppl=30.64  tok_acc=38.00%  tok/s=53,266\n[Epoch 5 | step 4050/7267 | global_step=33118] train_loss=3.4222  ppl=30.64  tok_acc=38.00%  tok/s=53,262\n[Epoch 5 | step 4200/7267 | global_step=33268] train_loss=3.4222  ppl=30.64  tok_acc=38.00%  tok/s=53,267\n[Epoch 5 | step 4350/7267 | global_step=33418] train_loss=3.4221  ppl=30.63  tok_acc=38.00%  tok/s=53,257\n[Epoch 5 | step 4500/7267 | global_step=33568] train_loss=3.4221  ppl=30.63  tok_acc=38.00%  tok/s=53,254\n[Epoch 5 | step 4650/7267 | global_step=33718] train_loss=3.4220  ppl=30.63  tok_acc=38.00%  tok/s=53,248\n[Epoch 5 | step 4800/7267 | global_step=33868] train_loss=3.4218  ppl=30.62  tok_acc=38.01%  tok/s=53,249\n[Epoch 5 | step 4950/7267 | global_step=34018] train_loss=3.4215  ppl=30.62  tok_acc=38.01%  tok/s=53,238\n[Epoch 5 | step 5100/7267 | global_step=34168] train_loss=3.4213  ppl=30.61  tok_acc=38.02%  tok/s=53,237\n[Epoch 5 | step 5250/7267 | global_step=34318] train_loss=3.4214  ppl=30.61  tok_acc=38.02%  tok/s=53,237\n[Epoch 5 | step 5400/7267 | global_step=34468] train_loss=3.4213  ppl=30.61  tok_acc=38.02%  tok/s=53,233\n[Epoch 5 | step 5550/7267 | global_step=34618] train_loss=3.4214  ppl=30.61  tok_acc=38.02%  tok/s=53,227\n[Epoch 5 | step 5700/7267 | global_step=34768] train_loss=3.4213  ppl=30.61  tok_acc=38.02%  tok/s=53,226\n[Epoch 5 | step 5850/7267 | global_step=34918] train_loss=3.4211  ppl=30.60  tok_acc=38.02%  tok/s=53,228\n[Epoch 5 | step 6000/7267 | global_step=35068] train_loss=3.4209  ppl=30.60  tok_acc=38.03%  tok/s=53,228\n[Epoch 5 | step 6150/7267 | global_step=35218] train_loss=3.4207  ppl=30.59  tok_acc=38.03%  tok/s=53,227\n[Epoch 5 | step 6300/7267 | global_step=35368] train_loss=3.4206  ppl=30.59  tok_acc=38.03%  tok/s=53,223\n[Epoch 5 | step 6450/7267 | global_step=35518] train_loss=3.4203  ppl=30.58  tok_acc=38.04%  tok/s=53,226\n[Epoch 5 | step 6600/7267 | global_step=35668] train_loss=3.4200  ppl=30.57  tok_acc=38.04%  tok/s=53,221\n[Epoch 5 | step 6750/7267 | global_step=35818] train_loss=3.4195  ppl=30.55  tok_acc=38.05%  tok/s=53,222\n[Epoch 5 | step 6900/7267 | global_step=35968] train_loss=3.4192  ppl=30.54  tok_acc=38.05%  tok/s=53,219\n— preview (LM, teacher-forced argmax) —\nCTX: \" war against the ottoman empire , which french diplomacy despite great efforts failed to achieve . furthermore , sobieski was opposed by the papacy , by polish gentry who saw the ottomans as the greater threat , and by polish magnates bribed by berlin and vienna . inner @-@ polish catholic opposition to an intervention on the protestant hungarian rebels ' side added to the resentments . thus ,\"\nREF: \" against the ottoman empire , which french diplomacy despite great efforts failed to achieve . furthermore , sobieski was opposed by the papacy , by polish gentry who saw the ottomans as the greater threat , and by polish magnates bribed by berlin and vienna . inner @-@ polish catholic opposition to an intervention on the protestant hungarian rebels ' side added to the resentments . thus , while\"\nHYP: \" , the united empire . the was andats was the pressure to to achieve . \\n , the @-@ib was ' a to the frenchacy , who the diplomry , had the french as a only power of and by the diplomates whoribing them the . the . \\n @-@ city politicsism to the ottoman in the part side throne , demands was to the situationentment of \\n , the\"\n[Epoch 5 | step 7050/7267 | global_step=36118] train_loss=3.4188  ppl=30.53  tok_acc=38.06%  tok/s=53,221\n[Epoch 5 | step 7200/7267 | global_step=36268] train_loss=3.4186  ppl=30.53  tok_acc=38.06%  tok/s=53,222\nEpoch 5 done | train_loss=3.4185  train_ppl=30.52  train_tok_acc=38.06%\n[Epoch 6 | step  150/7267 | global_step=36485] train_loss=3.3485  ppl=28.46  tok_acc=38.75%  tok/s=53,076\n[Epoch 6 | step  300/7267 | global_step=36635] train_loss=3.3454  ppl=28.37  tok_acc=38.80%  tok/s=52,992\n[Epoch 6 | step  450/7267 | global_step=36785] train_loss=3.3501  ppl=28.50  tok_acc=38.75%  tok/s=53,086\n[Epoch 6 | step  600/7267 | global_step=36935] train_loss=3.3522  ppl=28.57  tok_acc=38.73%  tok/s=53,116\n[Epoch 6 | step  750/7267 | global_step=37085] train_loss=3.3540  ppl=28.62  tok_acc=38.72%  tok/s=53,093\n[Epoch 6 | step  900/7267 | global_step=37235] train_loss=3.3554  ppl=28.66  tok_acc=38.70%  tok/s=53,073\n[Epoch 6 | step 1050/7267 | global_step=37385] train_loss=3.3558  ppl=28.67  tok_acc=38.70%  tok/s=53,078\n[Epoch 6 | step 1200/7267 | global_step=37535] train_loss=3.3570  ppl=28.70  tok_acc=38.69%  tok/s=53,090\n[Epoch 6 | step 1350/7267 | global_step=37685] train_loss=3.3591  ppl=28.76  tok_acc=38.66%  tok/s=53,087\n[Epoch 6 | step 1500/7267 | global_step=37835] train_loss=3.3597  ppl=28.78  tok_acc=38.66%  tok/s=53,093\n[Epoch 6 | step 1650/7267 | global_step=37985] train_loss=3.3603  ppl=28.80  tok_acc=38.66%  tok/s=53,073\n[Epoch 6 | step 1800/7267 | global_step=38135] train_loss=3.3607  ppl=28.81  tok_acc=38.67%  tok/s=53,089\n[Epoch 6 | step 1950/7267 | global_step=38285] train_loss=3.3606  ppl=28.81  tok_acc=38.67%  tok/s=53,091\n[Epoch 6 | step 2100/7267 | global_step=38435] train_loss=3.3614  ppl=28.83  tok_acc=38.66%  tok/s=53,088\n[Epoch 6 | step 2250/7267 | global_step=38585] train_loss=3.3614  ppl=28.83  tok_acc=38.66%  tok/s=53,082\n[Epoch 6 | step 2400/7267 | global_step=38735] train_loss=3.3619  ppl=28.84  tok_acc=38.66%  tok/s=53,095\n[Epoch 6 | step 2550/7267 | global_step=38885] train_loss=3.3623  ppl=28.86  tok_acc=38.65%  tok/s=53,106\n[Epoch 6 | step 2700/7267 | global_step=39035] train_loss=3.3622  ppl=28.85  tok_acc=38.66%  tok/s=53,109\n[Epoch 6 | step 2850/7267 | global_step=39185] train_loss=3.3624  ppl=28.86  tok_acc=38.66%  tok/s=53,105\n[Epoch 6 | step 3000/7267 | global_step=39335] train_loss=3.3620  ppl=28.85  tok_acc=38.66%  tok/s=53,114\n[Epoch 6 | step 3150/7267 | global_step=39485] train_loss=3.3622  ppl=28.85  tok_acc=38.66%  tok/s=53,116\n[Epoch 6 | step 3300/7267 | global_step=39635] train_loss=3.3621  ppl=28.85  tok_acc=38.67%  tok/s=53,115\n[Epoch 6 | step 3450/7267 | global_step=39785] train_loss=3.3625  ppl=28.86  tok_acc=38.66%  tok/s=53,123\n[Epoch 6 | step 3600/7267 | global_step=39935] train_loss=3.3630  ppl=28.87  tok_acc=38.66%  tok/s=53,115\n[Epoch 6 | step 3750/7267 | global_step=40085] train_loss=3.3632  ppl=28.88  tok_acc=38.66%  tok/s=53,120\n[Epoch 6 | step 3900/7267 | global_step=40235] train_loss=3.3629  ppl=28.87  tok_acc=38.66%  tok/s=53,125\n[Epoch 6 | step 4050/7267 | global_step=40385] train_loss=3.3631  ppl=28.88  tok_acc=38.66%  tok/s=53,133\n[Epoch 6 | step 4200/7267 | global_step=40535] train_loss=3.3630  ppl=28.87  tok_acc=38.67%  tok/s=53,131\n[Epoch 6 | step 4350/7267 | global_step=40685] train_loss=3.3627  ppl=28.87  tok_acc=38.67%  tok/s=53,138\n[Epoch 6 | step 4500/7267 | global_step=40835] train_loss=3.3625  ppl=28.86  tok_acc=38.68%  tok/s=53,143\n[Epoch 6 | step 4650/7267 | global_step=40985] train_loss=3.3625  ppl=28.86  tok_acc=38.67%  tok/s=53,144\n[Epoch 6 | step 4800/7267 | global_step=41135] train_loss=3.3628  ppl=28.87  tok_acc=38.67%  tok/s=53,152\n[Epoch 6 | step 4950/7267 | global_step=41285] train_loss=3.3628  ppl=28.87  tok_acc=38.67%  tok/s=53,152\n[Epoch 6 | step 5100/7267 | global_step=41435] train_loss=3.3627  ppl=28.87  tok_acc=38.68%  tok/s=53,158\n[Epoch 6 | step 5250/7267 | global_step=41585] train_loss=3.3626  ppl=28.86  tok_acc=38.68%  tok/s=53,163\n[Epoch 6 | step 5400/7267 | global_step=41735] train_loss=3.3624  ppl=28.86  tok_acc=38.68%  tok/s=53,166\n[Epoch 6 | step 5550/7267 | global_step=41885] train_loss=3.3624  ppl=28.86  tok_acc=38.68%  tok/s=53,163\n[Epoch 6 | step 5700/7267 | global_step=42035] train_loss=3.3622  ppl=28.85  tok_acc=38.69%  tok/s=53,168\n[Epoch 6 | step 5850/7267 | global_step=42185] train_loss=3.3622  ppl=28.85  tok_acc=38.69%  tok/s=53,165\n[Epoch 6 | step 6000/7267 | global_step=42335] train_loss=3.3619  ppl=28.84  tok_acc=38.70%  tok/s=53,168\n[Epoch 6 | step 6150/7267 | global_step=42485] train_loss=3.3617  ppl=28.84  tok_acc=38.70%  tok/s=53,165\n[Epoch 6 | step 6300/7267 | global_step=42635] train_loss=3.3616  ppl=28.83  tok_acc=38.70%  tok/s=53,168\n[Epoch 6 | step 6450/7267 | global_step=42785] train_loss=3.3613  ppl=28.83  tok_acc=38.70%  tok/s=53,173\n[Epoch 6 | step 6600/7267 | global_step=42935] train_loss=3.3610  ppl=28.82  tok_acc=38.71%  tok/s=53,175\n[Epoch 6 | step 6750/7267 | global_step=43085] train_loss=3.3607  ppl=28.81  tok_acc=38.71%  tok/s=53,176\n[Epoch 6 | step 6900/7267 | global_step=43235] train_loss=3.3605  ppl=28.80  tok_acc=38.72%  tok/s=53,179\n— preview (LM, teacher-forced argmax) —\nCTX: ' perspective on the value of life ... it ’ s hard not to admire her . \" she stated that she had \" a bit of a freak @-@ out moment \" when she first saw the cat @-@ suit . when asked about fighting in the costume , johansson responded \" a big part of me is like \\' can i move in this ? can i run in it ? can i like throw'\nREF: ' on the value of life ... it ’ s hard not to admire her . \" she stated that she had \" a bit of a freak @-@ out moment \" when she first saw the cat @-@ suit . when asked about fighting in the costume , johansson responded \" a big part of me is like \\' can i move in this ? can i run in it ? can i like throw myself'\nHYP: ' of the subject of the , the is s a to to sayire the . \" \\n also that the was \" a lot of aweak show like of \" , she was met her filmast and , \\n asked about the , the film , sheansson said , i lot , of the , a a i i have \\' the world \\' i move in this ? \\' i go it your'\n[Epoch 6 | step 7050/7267 | global_step=43385] train_loss=3.3604  ppl=28.80  tok_acc=38.72%  tok/s=53,181\n[Epoch 6 | step 7200/7267 | global_step=43535] train_loss=3.3604  ppl=28.80  tok_acc=38.72%  tok/s=53,179\nEpoch 6 done | train_loss=3.3602  train_ppl=28.79  train_tok_acc=38.72%\nGuardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n[Epoch 7 | step  150/7267 | global_step=43752] train_loss=3.3072  ppl=27.31  tok_acc=39.29%  tok/s=52,953\n[Epoch 7 | step  300/7267 | global_step=43902] train_loss=3.3095  ppl=27.37  tok_acc=39.27%  tok/s=53,068\n[Epoch 7 | step  450/7267 | global_step=44052] train_loss=3.3076  ppl=27.32  tok_acc=39.30%  tok/s=53,051\n[Epoch 7 | step  600/7267 | global_step=44202] train_loss=3.3109  ppl=27.41  tok_acc=39.25%  tok/s=53,114\n[Epoch 7 | step  750/7267 | global_step=44352] train_loss=3.3122  ppl=27.45  tok_acc=39.24%  tok/s=53,108\n[Epoch 7 | step  900/7267 | global_step=44502] train_loss=3.3102  ppl=27.39  tok_acc=39.26%  tok/s=53,111\n[Epoch 7 | step 1050/7267 | global_step=44652] train_loss=3.3114  ppl=27.42  tok_acc=39.25%  tok/s=53,119\n[Epoch 7 | step 1200/7267 | global_step=44802] train_loss=3.3112  ppl=27.42  tok_acc=39.25%  tok/s=53,135\n[Epoch 7 | step 1350/7267 | global_step=44952] train_loss=3.3115  ppl=27.43  tok_acc=39.25%  tok/s=53,156\n[Epoch 7 | step 1500/7267 | global_step=45102] train_loss=3.3120  ppl=27.44  tok_acc=39.24%  tok/s=53,174\n[Epoch 7 | step 1650/7267 | global_step=45252] train_loss=3.3109  ppl=27.41  tok_acc=39.25%  tok/s=53,192\n[Epoch 7 | step 1800/7267 | global_step=45402] train_loss=3.3111  ppl=27.42  tok_acc=39.25%  tok/s=53,176\n[Epoch 7 | step 1950/7267 | global_step=45552] train_loss=3.3112  ppl=27.42  tok_acc=39.25%  tok/s=53,187\n[Epoch 7 | step 2100/7267 | global_step=45702] train_loss=3.3119  ppl=27.44  tok_acc=39.24%  tok/s=53,181\n[Epoch 7 | step 2250/7267 | global_step=45852] train_loss=3.3118  ppl=27.43  tok_acc=39.25%  tok/s=53,185\n[Epoch 7 | step 2400/7267 | global_step=46002] train_loss=3.3118  ppl=27.43  tok_acc=39.25%  tok/s=53,178\n[Epoch 7 | step 2550/7267 | global_step=46152] train_loss=3.3124  ppl=27.45  tok_acc=39.24%  tok/s=53,186\n[Epoch 7 | step 2700/7267 | global_step=46302] train_loss=3.3122  ppl=27.45  tok_acc=39.24%  tok/s=53,189\n[Epoch 7 | step 2850/7267 | global_step=46452] train_loss=3.3125  ppl=27.45  tok_acc=39.24%  tok/s=53,194\n[Epoch 7 | step 3000/7267 | global_step=46602] train_loss=3.3128  ppl=27.46  tok_acc=39.24%  tok/s=53,195\n[Epoch 7 | step 3150/7267 | global_step=46752] train_loss=3.3129  ppl=27.46  tok_acc=39.24%  tok/s=53,199\n[Epoch 7 | step 3300/7267 | global_step=46902] train_loss=3.3126  ppl=27.46  tok_acc=39.24%  tok/s=53,202\n[Epoch 7 | step 3450/7267 | global_step=47052] train_loss=3.3133  ppl=27.47  tok_acc=39.24%  tok/s=53,198\n[Epoch 7 | step 3600/7267 | global_step=47202] train_loss=3.3132  ppl=27.47  tok_acc=39.24%  tok/s=53,205\n[Epoch 7 | step 3750/7267 | global_step=47352] train_loss=3.3134  ppl=27.48  tok_acc=39.24%  tok/s=53,201\n[Epoch 7 | step 3900/7267 | global_step=47502] train_loss=3.3133  ppl=27.48  tok_acc=39.24%  tok/s=53,211\n[Epoch 7 | step 4050/7267 | global_step=47652] train_loss=3.3132  ppl=27.47  tok_acc=39.25%  tok/s=53,221\n[Epoch 7 | step 4200/7267 | global_step=47802] train_loss=3.3135  ppl=27.48  tok_acc=39.24%  tok/s=53,219\n[Epoch 7 | step 4350/7267 | global_step=47952] train_loss=3.3137  ppl=27.49  tok_acc=39.24%  tok/s=53,211\n[Epoch 7 | step 4500/7267 | global_step=48102] train_loss=3.3136  ppl=27.48  tok_acc=39.25%  tok/s=53,211\n[Epoch 7 | step 4650/7267 | global_step=48252] train_loss=3.3138  ppl=27.49  tok_acc=39.24%  tok/s=53,212\n[Epoch 7 | step 4800/7267 | global_step=48402] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,206\n[Epoch 7 | step 4950/7267 | global_step=48552] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,207\n[Epoch 7 | step 5100/7267 | global_step=48702] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,202\n[Epoch 7 | step 5250/7267 | global_step=48852] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,206\n[Epoch 7 | step 5400/7267 | global_step=49002] train_loss=3.3133  ppl=27.48  tok_acc=39.26%  tok/s=53,207\n[Epoch 7 | step 5550/7267 | global_step=49152] train_loss=3.3133  ppl=27.48  tok_acc=39.26%  tok/s=53,214\n[Epoch 7 | step 5700/7267 | global_step=49302] train_loss=3.3131  ppl=27.47  tok_acc=39.26%  tok/s=53,210\n[Epoch 7 | step 5850/7267 | global_step=49452] train_loss=3.3130  ppl=27.47  tok_acc=39.26%  tok/s=53,213\n[Epoch 7 | step 6000/7267 | global_step=49602] train_loss=3.3127  ppl=27.46  tok_acc=39.26%  tok/s=53,210\n[Epoch 7 | step 6150/7267 | global_step=49752] train_loss=3.3127  ppl=27.46  tok_acc=39.26%  tok/s=53,214\n[Epoch 7 | step 6300/7267 | global_step=49902] train_loss=3.3127  ppl=27.46  tok_acc=39.27%  tok/s=53,212\n[Epoch 7 | step 6450/7267 | global_step=50052] train_loss=3.3127  ppl=27.46  tok_acc=39.27%  tok/s=53,217\n[Epoch 7 | step 6600/7267 | global_step=50202] train_loss=3.3126  ppl=27.46  tok_acc=39.27%  tok/s=53,219\n[Epoch 7 | step 6750/7267 | global_step=50352] train_loss=3.3126  ppl=27.46  tok_acc=39.27%  tok/s=53,223\n[Epoch 7 | step 6900/7267 | global_step=50502] train_loss=3.3125  ppl=27.45  tok_acc=39.27%  tok/s=53,220\n— preview (LM, teacher-forced argmax) —\nCTX: ' minnesota governor and olympic medalist \\n les auge , hockey player \\n alana blahoski , olympic gold medalist in hockey \\n matthew d. bostrom , former saint paul police assistant chief and current ramsey county sheriff . \\n herb brooks , hockey coach of the \" miracle on ice \" gold medal winning u.s. olympic hockey team \\n warren e.'\nREF: ' governor and olympic medalist \\n les auge , hockey player \\n alana blahoski , olympic gold medalist in hockey \\n matthew d. bostrom , former saint paul police assistant chief and current ramsey county sheriff . \\n herb brooks , hockey coach of the \" miracle on ice \" gold medal winning u.s. olympic hockey team \\n warren e. burger'\nHYP: ' . of the goldist . =ter.ux a \\n \\n =ain ,anche , , , olympic medal medalist \\n the \\n = g. smithwick , olympic olympic @-@ \\' chief \\n \\n former olympicsey university sheriff \\n \\n = rit , former player \\n the university newacle \" ice \" \\n medalist team.s. soccer gold team \\n = b. smith'\n[Epoch 7 | step 7050/7267 | global_step=50652] train_loss=3.3122  ppl=27.45  tok_acc=39.28%  tok/s=53,223\n[Epoch 7 | step 7200/7267 | global_step=50802] train_loss=3.3120  ppl=27.44  tok_acc=39.28%  tok/s=53,223\nEpoch 7 done | train_loss=3.3120  train_ppl=27.44  train_tok_acc=39.28%\n[Epoch 8 | step  150/7267 | global_step=51019] train_loss=3.2681  ppl=26.26  tok_acc=39.77%  tok/s=52,856\n[Epoch 8 | step  300/7267 | global_step=51169] train_loss=3.2704  ppl=26.32  tok_acc=39.75%  tok/s=53,113\n[Epoch 8 | step  450/7267 | global_step=51319] train_loss=3.2675  ppl=26.24  tok_acc=39.75%  tok/s=53,120\n[Epoch 8 | step  600/7267 | global_step=51469] train_loss=3.2676  ppl=26.25  tok_acc=39.76%  tok/s=53,144\n[Epoch 8 | step  750/7267 | global_step=51619] train_loss=3.2689  ppl=26.28  tok_acc=39.75%  tok/s=53,195\n[Epoch 8 | step  900/7267 | global_step=51769] train_loss=3.2702  ppl=26.32  tok_acc=39.73%  tok/s=53,205\n[Epoch 8 | step 1050/7267 | global_step=51919] train_loss=3.2707  ppl=26.33  tok_acc=39.72%  tok/s=53,192\n[Epoch 8 | step 1200/7267 | global_step=52069] train_loss=3.2713  ppl=26.34  tok_acc=39.72%  tok/s=53,197\n[Epoch 8 | step 1350/7267 | global_step=52219] train_loss=3.2710  ppl=26.34  tok_acc=39.73%  tok/s=53,196\n[Epoch 8 | step 1500/7267 | global_step=52369] train_loss=3.2716  ppl=26.35  tok_acc=39.72%  tok/s=53,206\n[Epoch 8 | step 1650/7267 | global_step=52519] train_loss=3.2719  ppl=26.36  tok_acc=39.72%  tok/s=53,198\n[Epoch 8 | step 1800/7267 | global_step=52669] train_loss=3.2718  ppl=26.36  tok_acc=39.73%  tok/s=53,201\n[Epoch 8 | step 1950/7267 | global_step=52819] train_loss=3.2723  ppl=26.37  tok_acc=39.72%  tok/s=53,216\n[Epoch 8 | step 2100/7267 | global_step=52969] train_loss=3.2727  ppl=26.38  tok_acc=39.72%  tok/s=53,218\n[Epoch 8 | step 2250/7267 | global_step=53119] train_loss=3.2730  ppl=26.39  tok_acc=39.72%  tok/s=53,215\n[Epoch 8 | step 2400/7267 | global_step=53269] train_loss=3.2731  ppl=26.39  tok_acc=39.72%  tok/s=53,219\n[Epoch 8 | step 2550/7267 | global_step=53419] train_loss=3.2735  ppl=26.40  tok_acc=39.71%  tok/s=53,215\n[Epoch 8 | step 2700/7267 | global_step=53569] train_loss=3.2736  ppl=26.41  tok_acc=39.71%  tok/s=53,201\n[Epoch 8 | step 2850/7267 | global_step=53719] train_loss=3.2737  ppl=26.41  tok_acc=39.71%  tok/s=53,195\n[Epoch 8 | step 3000/7267 | global_step=53869] train_loss=3.2737  ppl=26.41  tok_acc=39.71%  tok/s=53,186\n[Epoch 8 | step 3150/7267 | global_step=54019] train_loss=3.2740  ppl=26.42  tok_acc=39.70%  tok/s=53,184\n[Epoch 8 | step 3300/7267 | global_step=54169] train_loss=3.2740  ppl=26.42  tok_acc=39.71%  tok/s=53,186\n[Epoch 8 | step 3450/7267 | global_step=54319] train_loss=3.2743  ppl=26.43  tok_acc=39.70%  tok/s=53,186\n[Epoch 8 | step 3600/7267 | global_step=54469] train_loss=3.2741  ppl=26.42  tok_acc=39.70%  tok/s=53,185\n[Epoch 8 | step 3750/7267 | global_step=54619] train_loss=3.2741  ppl=26.42  tok_acc=39.71%  tok/s=53,181\n[Epoch 8 | step 3900/7267 | global_step=54769] train_loss=3.2742  ppl=26.42  tok_acc=39.71%  tok/s=53,188\n[Epoch 8 | step 4050/7267 | global_step=54919] train_loss=3.2738  ppl=26.41  tok_acc=39.71%  tok/s=53,183\n[Epoch 8 | step 4200/7267 | global_step=55069] train_loss=3.2737  ppl=26.41  tok_acc=39.71%  tok/s=53,192\n[Epoch 8 | step 4350/7267 | global_step=55219] train_loss=3.2738  ppl=26.41  tok_acc=39.71%  tok/s=53,192\n[Epoch 8 | step 4500/7267 | global_step=55369] train_loss=3.2738  ppl=26.41  tok_acc=39.71%  tok/s=53,204\n[Epoch 8 | step 4650/7267 | global_step=55519] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,207\n[Epoch 8 | step 4800/7267 | global_step=55669] train_loss=3.2738  ppl=26.41  tok_acc=39.72%  tok/s=53,217\n[Epoch 8 | step 4950/7267 | global_step=55819] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,221\n[Epoch 8 | step 5100/7267 | global_step=55969] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,228\n[Epoch 8 | step 5250/7267 | global_step=56119] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,237\n[Epoch 8 | step 5400/7267 | global_step=56269] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,238\n[Epoch 8 | step 5550/7267 | global_step=56419] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,238\n[Epoch 8 | step 5700/7267 | global_step=56569] train_loss=3.2739  ppl=26.42  tok_acc=39.71%  tok/s=53,246\n[Epoch 8 | step 5850/7267 | global_step=56719] train_loss=3.2741  ppl=26.42  tok_acc=39.71%  tok/s=53,255\n[Epoch 8 | step 6000/7267 | global_step=56869] train_loss=3.2740  ppl=26.42  tok_acc=39.71%  tok/s=53,259\n[Epoch 8 | step 6150/7267 | global_step=57019] train_loss=3.2739  ppl=26.41  tok_acc=39.72%  tok/s=53,263\n[Epoch 8 | step 6300/7267 | global_step=57169] train_loss=3.2739  ppl=26.41  tok_acc=39.72%  tok/s=53,264\n[Epoch 8 | step 6450/7267 | global_step=57319] train_loss=3.2741  ppl=26.42  tok_acc=39.72%  tok/s=53,265\n[Epoch 8 | step 6600/7267 | global_step=57469] train_loss=3.2740  ppl=26.42  tok_acc=39.72%  tok/s=53,259\n[Epoch 8 | step 6750/7267 | global_step=57619] train_loss=3.2739  ppl=26.41  tok_acc=39.72%  tok/s=53,262\n[Epoch 8 | step 6900/7267 | global_step=57769] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,258\n— preview (LM, teacher-forced argmax) —\nCTX: ' as the corporation counsel in a great metropolis ; the merchant at the cross @-@ roads store is as much a business man as the merchant of new york ; the farmer who goes forth in the morning and toils all day , who begins in spring and toils all summer , and who by the application of brain and muscle to the natural resources of the country creates wealth , is as much a business man'\nREF: ' the corporation counsel in a great metropolis ; the merchant at the cross @-@ roads store is as much a business man as the merchant of new york ; the farmer who goes forth in the morning and toils all day , who begins in spring and toils all summer , and who by the application of brain and muscle to the natural resources of the country creates wealth , is as much a business man as'\nHYP: \" the first of , the case dealis . the company ' the timeroad river office in the follows as part . as a corporation at the york . and company ' is to to the city ; the the in the ; and is to the and ends eat , day , and the is the evening of thew brain , the city world of the city . a and wealth the much a business man as\"\n[Epoch 8 | step 7050/7267 | global_step=57919] train_loss=3.2735  ppl=26.40  tok_acc=39.73%  tok/s=53,261\n[Epoch 8 | step 7200/7267 | global_step=58069] train_loss=3.2734  ppl=26.40  tok_acc=39.73%  tok/s=53,262\nEpoch 8 done | train_loss=3.2734  train_ppl=26.40  train_tok_acc=39.73%\n[Epoch 9 | step  150/7267 | global_step=58286] train_loss=3.2475  ppl=25.73  tok_acc=40.00%  tok/s=53,137\n[Epoch 9 | step  300/7267 | global_step=58436] train_loss=3.2470  ppl=25.71  tok_acc=40.03%  tok/s=53,088\n[Epoch 9 | step  450/7267 | global_step=58586] train_loss=3.2460  ppl=25.69  tok_acc=40.03%  tok/s=53,142\n[Epoch 9 | step  600/7267 | global_step=58736] train_loss=3.2448  ppl=25.66  tok_acc=40.04%  tok/s=53,178\n[Epoch 9 | step  750/7267 | global_step=58886] train_loss=3.2447  ppl=25.65  tok_acc=40.03%  tok/s=53,191\n[Epoch 9 | step  900/7267 | global_step=59036] train_loss=3.2439  ppl=25.63  tok_acc=40.05%  tok/s=53,194\n[Epoch 9 | step 1050/7267 | global_step=59186] train_loss=3.2450  ppl=25.66  tok_acc=40.04%  tok/s=53,218\n[Epoch 9 | step 1200/7267 | global_step=59336] train_loss=3.2446  ppl=25.65  tok_acc=40.04%  tok/s=53,245\n[Epoch 9 | step 1350/7267 | global_step=59486] train_loss=3.2451  ppl=25.66  tok_acc=40.04%  tok/s=53,248\n[Epoch 9 | step 1500/7267 | global_step=59636] train_loss=3.2443  ppl=25.64  tok_acc=40.05%  tok/s=53,241\n[Epoch 9 | step 1650/7267 | global_step=59786] train_loss=3.2445  ppl=25.65  tok_acc=40.05%  tok/s=53,221\n[Epoch 9 | step 1800/7267 | global_step=59936] train_loss=3.2445  ppl=25.65  tok_acc=40.05%  tok/s=53,222\n[Epoch 9 | step 1950/7267 | global_step=60086] train_loss=3.2446  ppl=25.65  tok_acc=40.05%  tok/s=53,204\n[Epoch 9 | step 2100/7267 | global_step=60236] train_loss=3.2447  ppl=25.65  tok_acc=40.05%  tok/s=53,207\n[Epoch 9 | step 2250/7267 | global_step=60386] train_loss=3.2448  ppl=25.66  tok_acc=40.04%  tok/s=53,192\n[Epoch 9 | step 2400/7267 | global_step=60536] train_loss=3.2454  ppl=25.67  tok_acc=40.04%  tok/s=53,197\n[Epoch 9 | step 2550/7267 | global_step=60686] train_loss=3.2447  ppl=25.66  tok_acc=40.05%  tok/s=53,203\n[Epoch 9 | step 2700/7267 | global_step=60836] train_loss=3.2449  ppl=25.66  tok_acc=40.05%  tok/s=53,208\n[Epoch 9 | step 2850/7267 | global_step=60986] train_loss=3.2448  ppl=25.66  tok_acc=40.05%  tok/s=53,198\n[Epoch 9 | step 3000/7267 | global_step=61136] train_loss=3.2448  ppl=25.66  tok_acc=40.05%  tok/s=53,204\n[Epoch 9 | step 3150/7267 | global_step=61286] train_loss=3.2454  ppl=25.67  tok_acc=40.04%  tok/s=53,214\n[Epoch 9 | step 3300/7267 | global_step=61436] train_loss=3.2455  ppl=25.67  tok_acc=40.04%  tok/s=53,202\n[Epoch 9 | step 3450/7267 | global_step=61586] train_loss=3.2453  ppl=25.67  tok_acc=40.05%  tok/s=53,209\n[Epoch 9 | step 3600/7267 | global_step=61736] train_loss=3.2450  ppl=25.66  tok_acc=40.05%  tok/s=53,203\n[Epoch 9 | step 3750/7267 | global_step=61886] train_loss=3.2450  ppl=25.66  tok_acc=40.05%  tok/s=53,206\n[Epoch 9 | step 3900/7267 | global_step=62036] train_loss=3.2451  ppl=25.66  tok_acc=40.05%  tok/s=53,210\n[Epoch 9 | step 4050/7267 | global_step=62186] train_loss=3.2453  ppl=25.67  tok_acc=40.05%  tok/s=53,213\n[Epoch 9 | step 4200/7267 | global_step=62336] train_loss=3.2455  ppl=25.67  tok_acc=40.05%  tok/s=53,206\n[Epoch 9 | step 4350/7267 | global_step=62486] train_loss=3.2453  ppl=25.67  tok_acc=40.05%  tok/s=53,207\n[Epoch 9 | step 4500/7267 | global_step=62636] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,209\n[Epoch 9 | step 4650/7267 | global_step=62786] train_loss=3.2456  ppl=25.68  tok_acc=40.05%  tok/s=53,206\n[Epoch 9 | step 4800/7267 | global_step=62936] train_loss=3.2454  ppl=25.67  tok_acc=40.05%  tok/s=53,208\n[Epoch 9 | step 4950/7267 | global_step=63086] train_loss=3.2454  ppl=25.67  tok_acc=40.06%  tok/s=53,208\n[Epoch 9 | step 5100/7267 | global_step=63236] train_loss=3.2453  ppl=25.67  tok_acc=40.06%  tok/s=53,211\n[Epoch 9 | step 5250/7267 | global_step=63386] train_loss=3.2454  ppl=25.67  tok_acc=40.05%  tok/s=53,216\n[Epoch 9 | step 5400/7267 | global_step=63536] train_loss=3.2455  ppl=25.68  tok_acc=40.05%  tok/s=53,216\n[Epoch 9 | step 5550/7267 | global_step=63686] train_loss=3.2457  ppl=25.68  tok_acc=40.05%  tok/s=53,213\n[Epoch 9 | step 5700/7267 | global_step=63836] train_loss=3.2457  ppl=25.68  tok_acc=40.05%  tok/s=53,215\n[Epoch 9 | step 5850/7267 | global_step=63986] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,213\n[Epoch 9 | step 6000/7267 | global_step=64136] train_loss=3.2459  ppl=25.68  tok_acc=40.05%  tok/s=53,218\n[Epoch 9 | step 6150/7267 | global_step=64286] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,215\n[Epoch 9 | step 6300/7267 | global_step=64436] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,218\n[Epoch 9 | step 6450/7267 | global_step=64586] train_loss=3.2456  ppl=25.68  tok_acc=40.05%  tok/s=53,221\n[Epoch 9 | step 6600/7267 | global_step=64736] train_loss=3.2454  ppl=25.67  tok_acc=40.06%  tok/s=53,226\n[Epoch 9 | step 6750/7267 | global_step=64886] train_loss=3.2456  ppl=25.68  tok_acc=40.06%  tok/s=53,225\n[Epoch 9 | step 6900/7267 | global_step=65036] train_loss=3.2455  ppl=25.68  tok_acc=40.06%  tok/s=53,230\n— preview (LM, teacher-forced argmax) —\nCTX: \" in washington , dc , thus beginning a 15 @-@ year affiliation as laucke 's active supporter in the u.s. pell 's former campaign manager , raymond nelson , handled logistics for many of laucke 's u.s. performances . in 1973 , laucke starred in a documentary produced by radio @-@ québec called la guitare , and he performed\"\nREF: \" washington , dc , thus beginning a 15 @-@ year affiliation as laucke 's active supporter in the u.s. pell 's former campaign manager , raymond nelson , handled logistics for many of laucke 's u.s. performances . in 1973 , laucke starred in a documentary produced by radio @-@ québec called la guitare , and he performed at\"\nHYP: ' the , d , and making the new @-@ year periodiliation with thevkends new political . the state.s. senateension \\'s campaign position . , john b , was theging and the of theaucke \\'s campaign.s. campaign . \\n the , laucke was in the short film by the personality televisionizb , \" va , which in was in'\n[Epoch 9 | step 7050/7267 | global_step=65186] train_loss=3.2456  ppl=25.68  tok_acc=40.06%  tok/s=53,231\n[Epoch 9 | step 7200/7267 | global_step=65336] train_loss=3.2455  ppl=25.67  tok_acc=40.06%  tok/s=53,231\nEpoch 9 done | train_loss=3.2456  train_ppl=25.68  train_tok_acc=40.06%\nGuardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n[Epoch 10 | step  150/7267 | global_step=65553] train_loss=3.2264  ppl=25.19  tok_acc=40.24%  tok/s=53,163\n[Epoch 10 | step  300/7267 | global_step=65703] train_loss=3.2280  ppl=25.23  tok_acc=40.26%  tok/s=53,160\n[Epoch 10 | step  450/7267 | global_step=65853] train_loss=3.2272  ppl=25.21  tok_acc=40.26%  tok/s=53,161\n[Epoch 10 | step  600/7267 | global_step=66003] train_loss=3.2281  ppl=25.23  tok_acc=40.24%  tok/s=53,210\n[Epoch 10 | step  750/7267 | global_step=66153] train_loss=3.2281  ppl=25.23  tok_acc=40.25%  tok/s=53,185\n[Epoch 10 | step  900/7267 | global_step=66303] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,200\n[Epoch 10 | step 1050/7267 | global_step=66453] train_loss=3.2292  ppl=25.26  tok_acc=40.24%  tok/s=53,168\n[Epoch 10 | step 1200/7267 | global_step=66603] train_loss=3.2288  ppl=25.25  tok_acc=40.24%  tok/s=53,206\n[Epoch 10 | step 1350/7267 | global_step=66753] train_loss=3.2283  ppl=25.24  tok_acc=40.25%  tok/s=53,216\n[Epoch 10 | step 1500/7267 | global_step=66903] train_loss=3.2290  ppl=25.25  tok_acc=40.24%  tok/s=53,230\n[Epoch 10 | step 1650/7267 | global_step=67053] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,216\n[Epoch 10 | step 1800/7267 | global_step=67203] train_loss=3.2292  ppl=25.26  tok_acc=40.24%  tok/s=53,240\n[Epoch 10 | step 1950/7267 | global_step=67353] train_loss=3.2285  ppl=25.24  tok_acc=40.25%  tok/s=53,236\n[Epoch 10 | step 2100/7267 | global_step=67503] train_loss=3.2290  ppl=25.25  tok_acc=40.25%  tok/s=53,236\n[Epoch 10 | step 2250/7267 | global_step=67653] train_loss=3.2287  ppl=25.25  tok_acc=40.25%  tok/s=53,240\n[Epoch 10 | step 2400/7267 | global_step=67803] train_loss=3.2291  ppl=25.26  tok_acc=40.24%  tok/s=53,237\n[Epoch 10 | step 2550/7267 | global_step=67953] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,237\n[Epoch 10 | step 2700/7267 | global_step=68103] train_loss=3.2291  ppl=25.26  tok_acc=40.24%  tok/s=53,246\n[Epoch 10 | step 2850/7267 | global_step=68253] train_loss=3.2291  ppl=25.26  tok_acc=40.25%  tok/s=53,240\n[Epoch 10 | step 3000/7267 | global_step=68403] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,240\n[Epoch 10 | step 3150/7267 | global_step=68553] train_loss=3.2288  ppl=25.25  tok_acc=40.25%  tok/s=53,245\n[Epoch 10 | step 3300/7267 | global_step=68703] train_loss=3.2289  ppl=25.25  tok_acc=40.25%  tok/s=53,247\n[Epoch 10 | step 3450/7267 | global_step=68853] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,243\n[Epoch 10 | step 3600/7267 | global_step=69003] train_loss=3.2298  ppl=25.27  tok_acc=40.24%  tok/s=53,246\n[Epoch 10 | step 3750/7267 | global_step=69153] train_loss=3.2297  ppl=25.27  tok_acc=40.24%  tok/s=53,246\n[Epoch 10 | step 3900/7267 | global_step=69303] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,244\n[Epoch 10 | step 4050/7267 | global_step=69453] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,249\n[Epoch 10 | step 4200/7267 | global_step=69603] train_loss=3.2294  ppl=25.26  tok_acc=40.24%  tok/s=53,248\n[Epoch 10 | step 4350/7267 | global_step=69753] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,249\n[Epoch 10 | step 4500/7267 | global_step=69903] train_loss=3.2292  ppl=25.26  tok_acc=40.25%  tok/s=53,251\n[Epoch 10 | step 4650/7267 | global_step=70053] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,257\n[Epoch 10 | step 4800/7267 | global_step=70203] train_loss=3.2292  ppl=25.26  tok_acc=40.24%  tok/s=53,251\n[Epoch 10 | step 4950/7267 | global_step=70353] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,245\n[Epoch 10 | step 5100/7267 | global_step=70503] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,244\n[Epoch 10 | step 5250/7267 | global_step=70653] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,240\n[Epoch 10 | step 5400/7267 | global_step=70803] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,246\n[Epoch 10 | step 5550/7267 | global_step=70953] train_loss=3.2294  ppl=25.26  tok_acc=40.24%  tok/s=53,247\n[Epoch 10 | step 5700/7267 | global_step=71103] train_loss=3.2297  ppl=25.27  tok_acc=40.24%  tok/s=53,243\n[Epoch 10 | step 5850/7267 | global_step=71253] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,245\n[Epoch 10 | step 6000/7267 | global_step=71403] train_loss=3.2298  ppl=25.28  tok_acc=40.23%  tok/s=53,242\n[Epoch 10 | step 6150/7267 | global_step=71553] train_loss=3.2298  ppl=25.27  tok_acc=40.23%  tok/s=53,242\n[Epoch 10 | step 6300/7267 | global_step=71703] train_loss=3.2298  ppl=25.27  tok_acc=40.24%  tok/s=53,240\n[Epoch 10 | step 6450/7267 | global_step=71853] train_loss=3.2300  ppl=25.28  tok_acc=40.23%  tok/s=53,244\n[Epoch 10 | step 6600/7267 | global_step=72003] train_loss=3.2300  ppl=25.28  tok_acc=40.23%  tok/s=53,241\n[Epoch 10 | step 6750/7267 | global_step=72153] train_loss=3.2301  ppl=25.28  tok_acc=40.23%  tok/s=53,245\n[Epoch 10 | step 6900/7267 | global_step=72303] train_loss=3.2299  ppl=25.28  tok_acc=40.23%  tok/s=53,240\n— preview (LM, teacher-forced argmax) —\nCTX: ' field , florida . its mission focus is unconventional warfare : counter @-@ terrorism , combat search and rescue , personnel recovery , psychological operations , aviation assistance to developing nations , \" deep battlefield \" resupply , interdiction and close air support . the wing \\'s core missions include aerospace surface interface , agile combat support , combat aviation advisory operations , information operations , personnel recovery /'\nREF: ' , florida . its mission focus is unconventional warfare : counter @-@ terrorism , combat search and rescue , personnel recovery , psychological operations , aviation assistance to developing nations , \" deep battlefield \" resupply , interdiction and close air support . the wing \\'s core missions include aerospace surface interface , agile combat support , combat aviation advisory operations , information operations , personnel recovery / recovery'\nHYP: ' , and , \\n population was was onnentional , , the @-@ terrorism , and , and rescue , and search , and warfare , and , , the countries , and the @-@ \" ,upply , and @-@isc , other air support , \\n mission \\'s mission mission include theobace , @-@ , airm air , , and support , , , and support , and recovery , recovery'\n[Epoch 10 | step 7050/7267 | global_step=72453] train_loss=3.2300  ppl=25.28  tok_acc=40.23%  tok/s=53,243\n[Epoch 10 | step 7200/7267 | global_step=72603] train_loss=3.2299  ppl=25.28  tok_acc=40.23%  tok/s=53,243\nEpoch 10 done | train_loss=3.2300  train_ppl=25.28  train_tok_acc=40.23%\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"history = train_gpt_lm(\n    model,\n    train_loader,\n    val_loader=val_loader,\n    epochs=4,\n    base_lr=3e-4,\n    weight_decay=0.01,\n    warmup_steps=2000,\n    label_smoothing=0.1,\n    grad_clip=1.0,\n    device=device,\n    ckpt_path=\"gptmini_owt10k.pt\",\n    log_every=1000,\n    preview_every=7000,\n    id2tok_fn=id2tok_fn,\n    amp_enabled=True,\n    amp_dtype=\"fp16\",   \n    val_checking = False , save_ckpt_every = 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T13:08:18.454386Z","iopub.execute_input":"2025-11-18T13:08:18.454683Z","iopub.status.idle":"2025-11-18T14:53:42.223476Z","shell.execute_reply.started":"2025-11-18T13:08:18.454664Z","shell.execute_reply":"2025-11-18T14:53:42.222320Z"}},"outputs":[{"name":"stdout","text":"[Epoch 1 | step 1000/7267 | global_step=1000] train_loss=3.2498  ppl=25.79  tok_acc=39.95%  tok/s=53,346\n[Epoch 1 | step 2000/7267 | global_step=2000] train_loss=3.2949  ppl=26.97  tok_acc=39.41%  tok/s=53,379\n[Epoch 1 | step 3000/7267 | global_step=3000] train_loss=3.3319  ppl=27.99  tok_acc=38.97%  tok/s=53,389\n[Epoch 1 | step 4000/7267 | global_step=4000] train_loss=3.3542  ppl=28.62  tok_acc=38.73%  tok/s=53,375\n[Epoch 1 | step 5000/7267 | global_step=5000] train_loss=3.3672  ppl=29.00  tok_acc=38.59%  tok/s=53,371\n[Epoch 1 | step 6000/7267 | global_step=6000] train_loss=3.3758  ppl=29.25  tok_acc=38.50%  tok/s=53,366\n[Epoch 1 | step 7000/7267 | global_step=7000] train_loss=3.3815  ppl=29.42  tok_acc=38.44%  tok/s=53,365\n— preview (LM, teacher-forced argmax) —\nCTX: \" and detection of vibrations underwater . compared to the harbor seal , the california sea lion 's vibrissae are smoother and less specialized and thus perform less when following hydrodynamic trails , although they still perform well . \\n = = ecology = = \\n = = = range and habitat = = = \\n the california sea lion ranges along the western coast and islands\"\nREF: \" detection of vibrations underwater . compared to the harbor seal , the california sea lion 's vibrissae are smoother and less specialized and thus perform less when following hydrodynamic trails , although they still perform well . \\n = = ecology = = \\n = = = range and habitat = = = \\n the california sea lion ranges along the western coast and islands of\"\nHYP: ' the of ther . . the to the other , , the bay state has wass highrationationalive are notoother than have prone than have more a than compared thechynamic or . and the are have more in \\n = = = = = \\n = = = life = habitat = = = \\n the california sea lion is from the coast coast of is ,'\nEpoch 1 done | train_loss=3.3826  train_ppl=29.45  train_tok_acc=38.43%\n[Epoch 2 | step 1000/7267 | global_step=8267] train_loss=3.3629  ppl=28.87  tok_acc=38.60%  tok/s=53,275\n[Epoch 2 | step 2000/7267 | global_step=9267] train_loss=3.3666  ppl=28.98  tok_acc=38.59%  tok/s=53,294\n[Epoch 2 | step 3000/7267 | global_step=10267] train_loss=3.3675  ppl=29.01  tok_acc=38.59%  tok/s=53,309\n[Epoch 2 | step 4000/7267 | global_step=11267] train_loss=3.3664  ppl=28.97  tok_acc=38.61%  tok/s=53,304\n[Epoch 2 | step 5000/7267 | global_step=12267] train_loss=3.3646  ppl=28.92  tok_acc=38.64%  tok/s=53,296\n[Epoch 2 | step 6000/7267 | global_step=13267] train_loss=3.3624  ppl=28.86  tok_acc=38.67%  tok/s=53,297\n[Epoch 2 | step 7000/7267 | global_step=14267] train_loss=3.3594  ppl=28.77  tok_acc=38.72%  tok/s=53,307\n— preview (LM, teacher-forced argmax) —\nCTX: \" as of 2012 . the procedural part of oracle 's pl / sql supports boolean however variables ; these can also be assigned null and the value is considered the same as unknown . \\n = = controversy = = \\n = = = common mistakes = = = \\n misunderstanding of how null works is the cause of a great number of errors in sq\"\nREF: \" of 2012 . the procedural part of oracle 's pl / sql supports boolean however variables ; these can also be assigned null and the value is considered the same as unknown . \\n = = controversy = = \\n = = = common mistakes = = = \\n misunderstanding of how null works is the cause of a great number of errors in sql\"\nHYP: \" the the . \\n songural of of theion wass storyural el is theiling ' ' ,ables are the include be be used to @-@s c pl of not to same as the . \\n = = = = = \\n = = = = names = = = \\n theinterstanding is the theull is are used subject of the dispute number of errors , thel\"\nEpoch 2 done | train_loss=3.3585  train_ppl=28.74  train_tok_acc=38.73%\nGuardado checkpoint (cada 2 epochs) -> gptmini_owt10k.pt\n[Epoch 3 | step 1000/7267 | global_step=15534] train_loss=3.2833  ppl=26.66  tok_acc=39.55%  tok/s=53,302\n[Epoch 3 | step 2000/7267 | global_step=16534] train_loss=3.2822  ppl=26.64  tok_acc=39.58%  tok/s=53,295\n[Epoch 3 | step 3000/7267 | global_step=17534] train_loss=3.2809  ppl=26.60  tok_acc=39.60%  tok/s=53,313\n[Epoch 3 | step 4000/7267 | global_step=18534] train_loss=3.2788  ppl=26.54  tok_acc=39.64%  tok/s=53,331\n[Epoch 3 | step 5000/7267 | global_step=19534] train_loss=3.2765  ppl=26.48  tok_acc=39.67%  tok/s=53,332\n[Epoch 3 | step 6000/7267 | global_step=20534] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,321\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3902434519.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = train_gpt_lm(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/638714474.py\u001b[0m in \u001b[0;36mtrain_gpt_lm\u001b[0;34m(model, train_loader, val_loader, epochs, base_lr, weight_decay, warmup_steps, label_smoothing, grad_clip, device, ckpt_path, log_every, preview_every, id2tok_fn, amp_enabled, amp_dtype, val_checking, save_ckpt_every)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":56},{"cell_type":"code","source":"history = train_gpt_lm(\n    model,\n    train_loader,\n    val_loader=val_loader,\n    epochs=15,\n    base_lr=3e-4,\n    weight_decay=0.01,\n    warmup_steps=2000,\n    label_smoothing=0.1,\n    grad_clip=1.0,\n    device=device,\n    ckpt_path=\"gptmini_owt10k.pt\",\n    log_every=150,\n    preview_every=1000,\n    id2tok_fn=id2tok_fn,\n    amp_enabled=True,\n    amp_dtype=\"fp16\",   \n    val_checking = False , save_ckpt_every = 8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:26:54.122532Z","iopub.execute_input":"2025-11-18T06:26:54.122863Z","iopub.status.idle":"2025-11-18T06:33:03.125817Z","shell.execute_reply.started":"2025-11-18T06:26:54.122833Z","shell.execute_reply":"2025-11-18T06:33:03.124720Z"}},"outputs":[{"name":"stdout","text":"[Epoch 1 | step  150/680 | global_step=150] train_loss=4.2893  ppl=72.91  tok_acc=27.91%  tok/s=62,028\n[Epoch 1 | step  300/680 | global_step=300] train_loss=4.2883  ppl=72.84  tok_acc=27.93%  tok/s=63,176\n[Epoch 1 | step  450/680 | global_step=450] train_loss=4.2959  ppl=73.40  tok_acc=27.85%  tok/s=63,260\n[Epoch 1 | step  600/680 | global_step=600] train_loss=4.3012  ppl=73.79  tok_acc=27.78%  tok/s=63,290\nEpoch 1 done | train_loss=4.3020  train_ppl=73.85  train_tok_acc=27.78%\n[Epoch 2 | step  150/680 | global_step=830] train_loss=4.2913  ppl=73.06  tok_acc=27.82%  tok/s=63,453\n[Epoch 2 | step  300/680 | global_step=980] train_loss=4.2999  ppl=73.69  tok_acc=27.75%  tok/s=63,623\n[Epoch 2 | step  450/680 | global_step=1130] train_loss=4.3094  ppl=74.40  tok_acc=27.65%  tok/s=63,628\n[Epoch 2 | step  600/680 | global_step=1280] train_loss=4.3149  ppl=74.80  tok_acc=27.60%  tok/s=63,583\nEpoch 2 done | train_loss=4.3175  train_ppl=75.00  train_tok_acc=27.58%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3036610972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = train_gpt_lm(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/638714474.py\u001b[0m in \u001b[0;36mtrain_gpt_lm\u001b[0;34m(model, train_loader, val_loader, epochs, base_lr, weight_decay, warmup_steps, label_smoothing, grad_clip, device, ckpt_path, log_every, preview_every, id2tok_fn, amp_enabled, amp_dtype, val_checking, save_ckpt_every)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":46},{"cell_type":"code","source":"def generate(model, tokenizer, prompt, max_new_tokens=30, temperature=1.0, top_k=50, device=\"cuda\"):\n    model.eval()\n\n    ids = tokenizer.encode(prompt, add_special_tokens=False).ids\n    x = torch.tensor([ids], dtype=torch.long, device=device)\n\n    block_size = model.module.block_size if hasattr(model, \"module\") else model.block_size\n\n    for _ in range(max_new_tokens):\n        x_cond = x[:, -block_size:]\n\n        # forward\n        logits, _ = model(x_cond, None)\n        logits = logits[:, -1, :] / temperature\n\n        # top-k truncation\n        if top_k is not None:\n            values, _ = torch.topk(logits, top_k)\n            logits[logits < values[:, [-1]]] = -float(\"inf\")\n\n        probs = torch.softmax(logits, dim=-1)\n        next_id = torch.multinomial(probs, num_samples=1)\n\n        x = torch.cat([x, next_id], dim=1)\n\n    return tokenizer.decode(x[0].tolist())\n\n\nprompt = \"are you happy?\"\nprint(generate(model, tokenizer, prompt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T14:54:19.989940Z","iopub.execute_input":"2025-11-18T14:54:19.990687Z","iopub.status.idle":"2025-11-18T14:54:20.497287Z","shell.execute_reply.started":"2025-11-18T14:54:19.990658Z","shell.execute_reply":"2025-11-18T14:54:20.496474Z"}},"outputs":[{"name":"stdout","text":" are you happy?able with your story 's storyline , if anything 's going on the run as ' what they did ' – and you 're getting too much\n","output_type":"stream"}],"execution_count":58}]}