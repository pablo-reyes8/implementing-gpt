{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom tokenizers import decoders\nimport os\n\nDATASET_NAME   = \"Ankursingh/openwebtext_10K\"\nDATASET_CONFIG = \"plain_text\"   # config por defecto del dataset\nVOCAB_SIZE = 16000         \nMIN_FREQ  = 2\nBLOCK_SIZE   = 256  # Ventana de contexto         \nVAL_FRACTION   = 0.1\nTOKENIZER_PATH = Path(\"owt10k_tokenizer.json\")\n\nCPU_COUNT   = os.cpu_count() or 2\nBATCH_SIZE  = 64\nNUM_WORKERS  = 2 if CPU_COUNT <= 2 else min(4, CPU_COUNT - 1)             \nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef load_openwebtext10k():\n    \"\"\"\n    Carga Ankursingh/openwebtext_10K que ya viene con splits train y val.\n    \"\"\"\n    ds = load_dataset(DATASET_NAME)   \n    train_ds = ds[\"train\"]\n    val_ds   = ds[\"val\"]\n    print(train_ds)\n    print(val_ds)\n    return train_ds, val_ds\n\n\n\ndef train_tokenizer(train_ds,\n                    vocab_size=VOCAB_SIZE,\n                    min_freq=MIN_FREQ,\n                    save_path=TOKENIZER_PATH):\n    \"\"\"\n    Entrena un tokenizer BPE estilo GPT (byte-level) sobre el split de entrenamiento.\n    \"\"\"\n    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n\n    tokenizer.normalizer = normalizers.Sequence([\n        normalizers.NFKC(),\n        normalizers.Lowercase(),])\n\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n    tokenizer.decoder = decoders.ByteLevel() \n\n    special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        min_frequency=min_freq,\n        special_tokens=special_tokens,)\n\n    def batch_iterator():\n        for ex in train_ds:\n            txt = ex[\"text\"]\n            if txt is not None and len(txt.strip()) > 0:\n                yield txt\n\n    print(\"Entrenando tokenizer BPE...\")\n    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n    print(\"Tamaño vocabulario:\", tokenizer.get_vocab_size())\n\n    save_path = Path(save_path)\n    tokenizer.save(str(save_path))\n    print(f\"Tokenizer guardado en {save_path.resolve()}\")\n\n    return tokenizer\ndef load_or_train_tokenizer(train_ds):\n    \"\"\"\n    Carga el tokenizer si ya existe en disco, si no lo entrena.\n    \"\"\"\n    if TOKENIZER_PATH.exists():\n        print(f\"Cargando tokenizer desde {TOKENIZER_PATH}...\")\n        tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n    else:\n        tokenizer = train_tokenizer(train_ds)\n    return tokenizer\n\n\nclass GPTTextDataset(Dataset):\n    \"\"\"\n    Construye un dataset para LM autoregresivo:\n      - Concatena todos los documentos en una secuencia larga de IDs.\n      - Añade <eos> al final de cada documento.\n      - Parte en chunks de longitud (BLOCK_SIZE + 1).\n      - Input = ids[:-1], Target = ids[1:].\n    \"\"\"\n    def __init__(self, hf_split, tokenizer, block_size=BLOCK_SIZE):\n        super().__init__()\n        self.block_size = block_size\n\n        eos_id = tokenizer.token_to_id(\"<eos>\")\n        if eos_id is None:\n            raise ValueError(\"El tokenizer no tiene token <eos>.\")\n\n        all_ids = []\n\n        print(\"Tokenizando y concatenando textos...\")\n        for ex in hf_split:\n            txt = ex[\"text\"]\n            if txt is None or len(txt.strip()) == 0:\n                continue\n            enc = tokenizer.encode(txt)\n            # enc.ids es una lista de ints\n            all_ids.extend(enc.ids + [eos_id])\n\n        self.data = torch.tensor(all_ids, dtype=torch.long)\n        print(f\"Total de tokens en este split: {len(self.data):,}\")\n\n        n_tokens = len(self.data)\n        chunk_len = block_size + 1\n        n_chunks = n_tokens // chunk_len\n\n        if n_chunks == 0:\n            raise ValueError(\"Muy pocos tokens para formar un solo chunk. \"\n                             \"Baja BLOCK_SIZE o usa más datos.\")\n\n        # Cortar a múltiplo exacto de chunk_len y reshape\n        self.data = self.data[: n_chunks * chunk_len]\n        self.data = self.data.view(n_chunks, chunk_len)\n\n        # inputs y targets precomputados\n        self.inputs  = self.data[:, :-1]  # [N, block_size]\n        self.targets = self.data[:, 1:]   # [N, block_size]\n\n        print(f\"Número de secuencias: {len(self.inputs):,}\")\n        print(f\"Forma inputs:  {self.inputs.shape}\")\n        print(f\"Forma targets: {self.targets.shape}\")\n\n    def __len__(self):\n        return self.inputs.size(0)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]\n\n\ndef create_dataloaders(block_size=BLOCK_SIZE,\n                       batch_size=BATCH_SIZE,\n                       num_workers=NUM_WORKERS):\n    train_hf, val_hf = load_openwebtext10k()\n    tokenizer = load_or_train_tokenizer(train_hf)\n\n    train_ds = GPTTextDataset(train_hf, tokenizer, block_size=block_size)\n    val_ds   = GPTTextDataset(val_hf,   tokenizer, block_size=block_size)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True)\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True)\n\n    return train_loader, val_loader, tokenizer\n\n\n\ntrain_loader, val_loader, tokenizer = create_dataloaders()\n\nx, y = next(iter(train_loader))\nprint(\"Batch x shape:\", x.shape) \nprint(\"Batch y shape:\", y.shape) \n\n\nexample_ids = x[0].tolist()\ntext = tokenizer.decode(example_ids)\nprint(\"Texto ejemplo (primer sample de x):\")\nprint(text)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:26:41.860457Z","iopub.execute_input":"2025-11-18T05:26:41.860791Z","iopub.status.idle":"2025-11-18T05:27:29.471567Z","shell.execute_reply.started":"2025-11-18T05:26:41.860762Z","shell.execute_reply":"2025-11-18T05:27:29.470536Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 10000\n})\nDataset({\n    features: ['text'],\n    num_rows: 4007\n})\nCargando tokenizer desde owt10k_tokenizer.json...\nTokenizando y concatenando textos...\nTotal de tokens en este split: 11,175,296\nNúmero de secuencias: 43,483\nForma inputs:  torch.Size([43483, 256])\nForma targets: torch.Size([43483, 256])\nTokenizando y concatenando textos...\nTotal de tokens en este split: 4,808,361\nNúmero de secuencias: 18,709\nForma inputs:  torch.Size([18709, 256])\nForma targets: torch.Size([18709, 256])\nBatch x shape: torch.Size([64, 256])\nBatch y shape: torch.Size([64, 256])\nTexto ejemplo (primer sample de x):\npremier.ticketek.com.au\n\n■ make beautiful music with elton john and his band\n\nmedia_camera elton john performs in adelaide on january 28\n\nthe legendary sir elton john is playing all the hits from his brilliant career spanning five decades including songs from his classic album goodbye yellow brick road which recently celebrated its 40th anniversary.\n\ndetails: adelaide entertainment centre, tuesday, january 28, 7pm. visit www.theaec.net\n\n■ catch your idol\n\nsee amercian idol finalist turned international star adam lambert in this intimate show as part of his the original high tour supported by 20-year-old new york artist melanie martinez.\n\ndetails: adelaide entertainment centre theatre, tuesday, december 15, 8pm. www.theaec.net\n\n■ sassy swizzle\n\nget ready for a brand new show jam-packed with acrobatics, cabaret and feisty entertainment. get ready for club swizzle! if you’re looking for a fun and fantastic night out, then this show is not one to be missed. suitable for ages 12 and over.\n\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from collections import Counter\nimport torch\n\ndef inspect_autoregressive_loader(train_loader, tokenizer, \n                                  num_batches=2,  # cuántos batches inspeccionar\n                                  max_examples=3, # cuántas secuencias imprimir por batch\n                                  max_tokens_print=40):  # cuántos tokens decodificar por ejemplo\n    \"\"\"\n    Inspecciona un DataLoader de lenguaje autoregresivo (GPT-style).\n\n    Supone que cada batch es:\n        x: [B, T]  (inputs)\n        y: [B, T]  (targets desplazados 1 a la derecha en el stream original)\n\n    Hace:\n    - Verificar qué tan cierto es que y[:, :-1] == x[:, 1:].\n    - Analizar la distribución de y[:, 0] (primer token que el modelo debe predecir).\n    - Imprimir ejemplos decodificados (input vs target) para entender el shift.\n    \"\"\"\n    total_shift_positions = 0\n    total_shift_matches   = 0\n\n    first_input_ids  = []\n    first_target_ids = []\n\n    for b_idx, (x, y) in enumerate(train_loader):\n        B, T = x.shape\n\n        # Comprobar el desplazamiento: y[:, :-1] debería ser igual a x[:, 1:]\n        shift_equal = (y[:, :-1] == x[:, 1:])  # [B, T-1] bool\n        total_shift_matches   += shift_equal.sum().item()\n        total_shift_positions += shift_equal.numel()\n\n        # Guardar los primeros tokens de input y target para estadísticas\n        first_input_ids.extend(x[:, 0].tolist())\n        first_target_ids.extend(y[:, 0].tolist())\n\n        # Imprimir algunos ejemplos para ver texto real\n        print(f\"\\n=== Batch {b_idx} ===\")\n        for i in range(min(max_examples, B)):\n            inp_ids = x[i].tolist()\n            tgt_ids = y[i].tolist()\n\n            inp_ids_short = inp_ids[:max_tokens_print]\n            tgt_ids_short = tgt_ids[:max_tokens_print]\n\n            inp_text = tokenizer.decode(inp_ids_short)\n            tgt_text = tokenizer.decode(tgt_ids_short)\n\n            print(f\"\\n--- Ejemplo {i} ---\")\n            print(f\"Input IDs   (primeros {len(inp_ids_short)}): {inp_ids_short}\")\n            print(f\"Target IDs  (primeros {len(tgt_ids_short)}): {tgt_ids_short}\")\n            print(\"Input texto (modelo VE):\")\n            print(repr(inp_text))\n            print(\"Target texto (modelo DEBE predecir):\")\n            print(repr(tgt_text))\n\n        if b_idx + 1 >= num_batches:\n            break\n\n    shift_ratio = total_shift_matches / total_shift_positions\n    print(\"\\n================== RESUMEN AUTORREGRESIVO ==================\")\n    print(f\"Total posiciones comparadas (y[:, :-1] vs x[:, 1:]): {total_shift_positions}\")\n    print(f\"Coincidencias: {total_shift_matches}\")\n    print(f\"Proporción de coincidencia (ideal ~1.0): {shift_ratio:.6f}\")\n\n    # Estadísticas sobre la PRIMERA posición\n    print(\"\\nDistribución de primeros tokens (input vs target):\")\n\n    first_input_counts  = Counter(first_input_ids)\n    first_target_counts = Counter(first_target_ids)\n\n    def top_tokens(counter, name, k=10):\n        print(f\"\\nTop {k} tokens más frecuentes en {name}:\")\n        for token_id, cnt in counter.most_common(k):\n            text = tokenizer.decode([token_id])\n            print(f\"  id={token_id:5d} | freq={cnt:6d} | texto={repr(text)}\")\n\n    top_tokens(first_input_counts,  \"x[:, 0]  (primer token que VE el modelo)\")\n    top_tokens(first_target_counts, \"y[:, 0]  (primer token que DEBE predecir)\")\n\n    print(\"\\n============================================================\")\n\n  \ninspect_autoregressive_loader(train_loader, tokenizer,\n                              num_batches=1,  \n                              max_examples=2, \n                              max_tokens_print=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:27:35.197176Z","iopub.execute_input":"2025-11-18T05:27:35.197853Z","iopub.status.idle":"2025-11-18T05:27:35.338039Z","shell.execute_reply.started":"2025-11-18T05:27:35.197818Z","shell.execute_reply":"2025-11-18T05:27:35.336872Z"}},"outputs":[{"name":"stdout","text":"\n=== Batch 0 ===\n\n--- Ejemplo 0 ---\nInput IDs   (primeros 50): [138, 33, 2872, 401, 138, 138, 46, 294, 182, 505, 5295, 1505, 1704, 4402, 1209, 138, 138, 46, 294, 182, 505, 5295, 235, 1505, 1704, 335, 3488, 10375, 207, 2496, 3041, 4402, 17, 256, 2728, 943, 4617, 209, 9540, 3041, 4402, 3870, 234, 14057, 7731, 831, 4036, 209, 1456, 2571]\nTarget IDs  (primeros 50): [33, 2872, 401, 138, 138, 46, 294, 182, 505, 5295, 1505, 1704, 4402, 1209, 138, 138, 46, 294, 182, 505, 5295, 235, 1505, 1704, 335, 3488, 10375, 207, 2496, 3041, 4402, 17, 256, 2728, 943, 4617, 209, 9540, 3041, 4402, 3870, 234, 14057, 7731, 831, 4036, 209, 1456, 2571, 204]\nInput texto (modelo VE):\n\"\\n> learn more\\n\\neuler hermes north america insurance company\\n\\neuler hermes is north america's largest provider of trade credit insurance. we offer both domestic and export credit insurance policies that insure clients against commercial and political risk\"\nTarget texto (modelo DEBE predecir):\n\"> learn more\\n\\neuler hermes north america insurance company\\n\\neuler hermes is north america's largest provider of trade credit insurance. we offer both domestic and export credit insurance policies that insure clients against commercial and political risk in\"\n\n--- Ejemplo 1 ---\nInput IDs   (primeros 50): [138, 138, 449, 1705, 653, 6656, 241, 640, 9891, 17, 401, 998, 201, 178, 2809, 17, 138, 138, 9059, 15, 382, 477, 424, 1944, 11566, 6656, 34, 716, 15, 1460, 178, 2406, 201, 1464, 3231, 317, 11154, 2457, 12, 2671, 13151, 434, 869, 17, 204, 178, 12651, 16, 805, 2020]\nTarget IDs  (primeros 50): [138, 449, 1705, 653, 6656, 241, 640, 9891, 17, 401, 998, 201, 178, 2809, 17, 138, 138, 9059, 15, 382, 477, 424, 1944, 11566, 6656, 34, 716, 15, 1460, 178, 2406, 201, 1464, 3231, 317, 11154, 2457, 12, 2671, 13151, 434, 869, 17, 204, 178, 12651, 16, 805, 2020, 254]\nInput texto (modelo VE):\n'\\n\\nindividual controls for every filter. more power to the user.\\n\\ngreat, so what about individual mask controls? well, having the chance to actually select (multiple) smart filters would help. in the mock-up below'\nTarget texto (modelo DEBE predecir):\n'\\nindividual controls for every filter. more power to the user.\\n\\ngreat, so what about individual mask controls? well, having the chance to actually select (multiple) smart filters would help. in the mock-up below i'\n\n================== RESUMEN AUTORREGRESIVO ==================\nTotal posiciones comparadas (y[:, :-1] vs x[:, 1:]): 16320\nCoincidencias: 16320\nProporción de coincidencia (ideal ~1.0): 1.000000\n\nDistribución de primeros tokens (input vs target):\n\nTop 10 tokens más frecuentes en x[:, 0]  (primer token que VE el modelo):\n  id=  138 | freq=     3 | texto='\\n'\n  id=  175 | freq=     3 | texto=' a'\n  id=  207 | freq=     2 | texto=' of'\n  id=  376 | freq=     2 | texto=' will'\n  id=  178 | freq=     2 | texto=' the'\n  id=  201 | freq=     2 | texto=' to'\n  id= 7307 | freq=     1 | texto=' resource'\n  id=  247 | freq=     1 | texto=' it'\n  id=10104 | freq=     1 | texto=' perry'\n  id=  195 | freq=     1 | texto='or'\n\nTop 10 tokens más frecuentes en y[:, 0]  (primer token que DEBE predecir):\n  id=  178 | freq=     4 | texto=' the'\n  id=   17 | freq=     3 | texto='.'\n  id=  138 | freq=     2 | texto='\\n'\n  id=  393 | freq=     2 | texto=' \"'\n  id=  175 | freq=     2 | texto=' a'\n  id=   15 | freq=     2 | texto=','\n  id=   33 | freq=     1 | texto='>'\n  id=  884 | freq=     1 | texto=' 6'\n  id= 1100 | freq=     1 | texto='att'\n  id= 1167 | freq=     1 | texto=' rest'\n\n============================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass GPT2Embeddings(nn.Module):\n    \"\"\"\n    Embeddings estilo GPT-2:\n      - token embeddings aprendidos\n      - positional embeddings aprendidos\n      - dropout opcional\n\n    input_ids: LongTensor [B, T]\n    return:    FloatTensor [B, T, d_model]\n    \"\"\"\n    def __init__(self, vocab_size: int, d_model: int, block_size: int, dropout: float = 0.1):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(block_size, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.block_size = block_size\n        self.d_model = d_model\n\n    def forward(self, input_ids: torch.Tensor):\n        \"\"\"\n        input_ids: [B, T] con IDs de tokens.\n        \"\"\"\n        \n        B, T = input_ids.shape\n        if T > self.block_size:\n            raise ValueError(f\"Secuencia T={T} > block_size={self.block_size}\")\n\n        # posiciones [0, 1, ..., T-1]\n        device = input_ids.device\n        pos_ids = torch.arange(0, T, device=device).unsqueeze(0)  # [1, T]\n        pos_ids = pos_ids.expand(B, T)  \n\n        tok = self.tok_emb(input_ids)  # [B, T, d_model]\n        pos = self.pos_emb(pos_ids)   \n        x = tok + pos                  # [B, T, d_model]\n\n        x = self.dropout(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:26.646746Z","iopub.execute_input":"2025-11-18T05:29:26.647123Z","iopub.status.idle":"2025-11-18T05:29:26.654005Z","shell.execute_reply.started":"2025-11-18T05:29:26.647073Z","shell.execute_reply":"2025-11-18T05:29:26.653020Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nvocab_size = tokenizer.get_vocab_size()\nd_model = 256\nblock_size = 256\n\nemb = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\n\nx_ids, y_ids = next(iter(train_loader))  \nx_ids = x_ids.to(device)\n\nx_emb = emb(x_ids)  # [B, T, d_model]\nprint(x_emb.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:27:39.740569Z","iopub.execute_input":"2025-11-18T05:27:39.741399Z","iopub.status.idle":"2025-11-18T05:27:39.927771Z","shell.execute_reply.started":"2025-11-18T05:27:39.741289Z","shell.execute_reply":"2025-11-18T05:27:39.926784Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 256, 256])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef scaled_dot_product_attention(q, k, v, mask=None):\n    \"\"\"\n    q: (..., Lq, d)\n    k: (..., Lk, d)\n    v: (..., Lk, dv)\n    mask: broadcastable a (..., Lq, Lk)\n          - bool: True = BLOQUEAR (poner -inf)\n          - float: 1.0 = permitir, 0.0 = bloquear\n    Returns:\n        output: (..., Lq, dv)\n        attn:   (..., Lq, Lk)\n\n    \"\"\"\n    scores = torch.matmul(q, k.transpose(-2, -1))\n    dk = q.size(-1)\n    scores = scores / dk**0.5\n\n    if mask is not None:\n        # Normalizamos a un tensor float con -inf donde se bloquea\n        if mask.dtype == torch.bool:\n            scores = scores.masked_fill(mask, float(\"-inf\"))\n        else:\n            # asumimos máscara en {0,1}: 0 = bloquear\n            scores = scores.masked_fill(mask <= 0, float(\"-inf\"))\n\n    attn = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn, v)\n    return output, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model debe ser múltiplo de num_heads\"\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def _split_heads(self, x):\n        B, L, _ = x.shape\n        return x.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n\n    def _combine_heads(self, x):\n        B, H, L, D = x.shape\n        return x.transpose(1, 2).contiguous().view(B, L, H * D)\n\n    def forward(self, x_q, x_kv, mask=None):\n        #Primera proyeccion\n        q = self._split_heads(self.w_q(x_q))\n        k = self._split_heads(self.w_k(x_kv))\n        v = self._split_heads(self.w_v(x_kv))\n\n\n        if mask is not None:\n        # Aceptamos:\n        # (B, Lk), (B, Lq, Lk), (B, 1, Lq, Lk), (B, H, Lq, Lk)\n          if mask.dim() == 2:\n              mask = mask[:, None, None, :]\n\n          elif mask.dim() == 3:\n              mask = mask[:, None, :, :]\n\n          elif mask.dim() == 4:\n              pass # Ya funciona asi\n          else:\n              raise ValueError(f\"Máscara con dims no soportadas: {mask.shape}\")\n\n        if mask.dtype != torch.bool:\n            mask = (mask <= 0)\n\n        # Aplicamos Atencion y concatenamos\n        attn_out, _ = scaled_dot_product_attention(q, k, v, mask)\n        attn_out = self._combine_heads(attn_out)\n\n        # Proyeccion Final\n        attn_out = self.w_o(attn_out)\n        attn_out = self.dropout(attn_out)\n        return attn_out\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, block_size: int, dropout: float = 0.1):\n        super().__init__()\n        \n        self.mha = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n        self.block_size = block_size\n\n        mask = torch.triu(torch.ones(block_size, block_size, dtype=torch.bool), diagonal=1)\n        self.register_buffer(\"causal_mask\", mask, persistent=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x: [B, T, d_model]\n        \"\"\"\n        B, T, _ = x.shape\n        if T > self.block_size:\n            raise ValueError(f\"T={T} > block_size={self.block_size}\")\n\n        # [T, T] -> [1, T, T] para que MultiHeadAttention lo trate como 'batch size = 1'\n        mask = self.causal_mask[:T, :T].unsqueeze(0) \n\n        # Self-attention: q = k = v = x\n        out = self.mha(x, x, mask=mask)   # [B, T, d_model]\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:23.174179Z","iopub.execute_input":"2025-11-18T05:29:23.174435Z","iopub.status.idle":"2025-11-18T05:29:23.187603Z","shell.execute_reply.started":"2025-11-18T05:29:23.174418Z","shell.execute_reply":"2025-11-18T05:29:23.186816Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nvocab_size = tokenizer.get_vocab_size()\nd_model  = 256\nblock_size = 256\n\nemb  = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\nattn = CausalSelfAttention(d_model, num_heads=8, block_size=block_size, dropout=0.1).to(device)\n\nx_ids, y_ids = next(iter(train_loader))  # [B, T]\nx_ids = x_ids.to(device)\n\nx = emb(x_ids)  # pesos + input \nx = attn(x)     # [B, T, d_model]\nprint(x.shape) # [B, T, d_model] (self-attn causal)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:28:55.577292Z","iopub.execute_input":"2025-11-18T05:28:55.577568Z","iopub.status.idle":"2025-11-18T05:28:56.095549Z","shell.execute_reply.started":"2025-11-18T05:28:55.577549Z","shell.execute_reply":"2025-11-18T05:28:56.094812Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 256, 256])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class GPT2MLP(nn.Module):\n    \"\"\"\n    MLP posición a posición estilo GPT-2:\n      Linear(d_model → d_ff) + GELU + Dropout + Linear(d_ff → d_model)\n    \"\"\"\n    def __init__(self, d_model: int, d_ff = None, dropout: float = 0.1):\n        super().__init__()\n        if d_ff is None:\n            d_ff = 4 * d_model  # típico en GPT\n\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.act = nn.GELU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nclass GPT2Block(nn.Module):\n    \"\"\"\n    Bloque GPT-2:\n      x -> LN -> CausalSelfAttention -> +residual\n      -> LN -> MLP -> +residual\n    Pre-LN (estilo GPT-2 moderno).\n    \"\"\"\n    def __init__(self, d_model: int, num_heads: int, block_size: int,\n                 d_ff = None, dropout: float = 0.1, layernorm_eps: float = 1e-5):\n      \n        super().__init__()\n        self.ln_1 = nn.LayerNorm(d_model, eps=layernorm_eps)\n        self.ln_2 = nn.LayerNorm(d_model, eps=layernorm_eps)\n        self.attn = CausalSelfAttention(d_model=d_model, num_heads=num_heads,\n                                        block_size=block_size, dropout=dropout)\n        self.mlp  = GPT2MLP(d_model=d_model, d_ff=d_ff, dropout=dropout)\n\n    def forward(self, x: torch.Tensor):\n      # Pre-LN + atención causal\n      normalized_1 = self.ln_1(x)\n      attn_output = self.attn(normalized_1)\n      x = x + attn_output # residual conection\n      \n      # Pre-LN + MLP\n      normalized_2 = self.ln_2(x)\n      mlp_output = self.mlp(normalized_2)\n      x = x + mlp_output # residual conection\n    \n      return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:21.038382Z","iopub.execute_input":"2025-11-18T05:29:21.039018Z","iopub.status.idle":"2025-11-18T05:29:21.047825Z","shell.execute_reply.started":"2025-11-18T05:29:21.038967Z","shell.execute_reply":"2025-11-18T05:29:21.046800Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nvocab_size = tokenizer.get_vocab_size()\nd_model = 256\nblock_size = 256\nnum_heads = 8\nd_ff = 4 * d_model  # 1024\n\nemb = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\nmlp = GPT2MLP(d_model, d_ff=d_ff, dropout=0.1).to(device)\nblock = GPT2Block(d_model, num_heads=num_heads, block_size=block_size, \n                  d_ff=d_ff, dropout=0.1).to(device)\n\n\nx_ids, y_ids = next(iter(train_loader))  # [B, T]\nx_ids = x_ids.to(device)\n\nx = emb(x_ids)  # [B, T, d_model]\nprint(f\"Después de embeddings: {x.shape}\")\n\nx_mlp = mlp(x)\nprint(f\"Después de MLP: {x_mlp.shape}\")  # [B, T, d_model]\n\nx_block = block(x)\nprint(f\"Después de GPT2Block: {x_block.shape}\")  # [B, T, d_model]\n\nprint(f\"¿Hay NaNs en la salida? {torch.isnan(x_block).any().item()}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:29:36.128188Z","iopub.execute_input":"2025-11-18T05:29:36.128881Z","iopub.status.idle":"2025-11-18T05:29:36.508530Z","shell.execute_reply.started":"2025-11-18T05:29:36.128850Z","shell.execute_reply":"2025-11-18T05:29:36.507479Z"}},"outputs":[{"name":"stdout","text":"Después de embeddings: torch.Size([64, 256, 256])\nDespués de MLP: torch.Size([64, 256, 256])\nDespués de GPT2Block: torch.Size([64, 256, 256])\n¿Hay NaNs en la salida? False\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def init_gptmini_weights(model):\n    with torch.no_grad():\n        for name, module in model.named_modules():\n            # Embeddings \n            if isinstance(module, nn.Embedding):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n            # Linears\n            elif isinstance(module, nn.Linear):\n                if name == \"lm_head\":\n                    # pesos pequeños\n                    nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                    if module.bias is not None:\n                        nn.init.zeros_(module.bias)\n                else:\n                    # Xavier \n                    nn.init.xavier_uniform_(module.weight)\n                    if module.bias is not None:\n                        nn.init.zeros_(module.bias)\n\n\nclass GPTMini(nn.Module):\n    \"\"\"\n    GPT-2 'mini' decoder-only:\n      - Embeddings (token + posición aprendida)\n      - n_layers de GPT2Block\n      - LayerNorm final\n      - LM head (atado a los embeddings de token)\n    \"\"\"\n    def __init__(self,\n                 vocab_size: int,\n                 block_size: int,\n                 n_layer: int = 4,\n                 n_head: int = 4,\n                 d_model: int = 256,\n                 dropout: float = 0.1,\n                 layernorm_eps: float = 1e-5):\n        \n        super().__init__()\n\n        self.vocab_size = vocab_size\n        self.block_size = block_size\n        self.d_model    = d_model\n\n        self.emb = GPT2Embeddings(\n            vocab_size=vocab_size,\n            d_model=d_model,\n            block_size=block_size,\n            dropout=dropout)\n\n        self.blocks = nn.ModuleList([\n            GPT2Block(\n                d_model=d_model,\n                num_heads=n_head,\n                block_size=block_size,\n                d_ff=4*d_model,\n                dropout=dropout,\n                layernorm_eps=layernorm_eps,) for _ in range(n_layer)])\n\n        self.ln_f = nn.LayerNorm(d_model, eps=layernorm_eps)\n\n        # LM head: proyecta representaciones a logits de vocabulario\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n        # Weight tying: compartir pesos con embedding de tokens\n        init_gptmini_weights(self)\n        self.lm_head.weight = self.emb.tok_emb.weight\n\n    def forward(self, idx: torch.Tensor, targets = None):\n      \"\"\"\n      idx:     [B, T] con IDs de tokens de entrada\n      targets: [B, T] con IDs objetivo (shifted) o None\n\n      Returns:\n        logits: [B, T, vocab_size]\n        loss:   escalar (si targets no es None), sino None\n      \"\"\"\n      B, T = idx.shape\n      if T > self.block_size:\n          raise ValueError(f\"Secuencia demasiado larga: T={T}, block_size={self.block_size}\")\n\n      # Embeddings token + posición\n      x = self.emb(idx)  # [B, T, d_model]\n\n      # Pasar por los bloques GPT-2\n      for block in self.blocks:\n          x = block(x)    # [B, T, d_model]\n\n      # LayerNorm final\n      x = self.ln_f(x)   # [B, T, d_model]\n\n      # LM head -> logits\n      logits = self.lm_head(x)  # [B, T, vocab_size]\n\n      loss = None\n      if targets is not None:\n          # Cross-entropy autoregresiva\n          logits_flat  = logits.view(-1, self.vocab_size)\n          targets_flat = targets.view(-1)\n          # Opcional: ignorar padding tokens si usas ignore_index\n          loss = F.cross_entropy(logits_flat, targets_flat)\n\n      return logits, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:30:05.692202Z","iopub.execute_input":"2025-11-18T05:30:05.693129Z","iopub.status.idle":"2025-11-18T05:30:05.703891Z","shell.execute_reply.started":"2025-11-18T05:30:05.693083Z","shell.execute_reply":"2025-11-18T05:30:05.703047Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab_size()\nblock_size = 256  \n\nmodel = GPTMini(\n    vocab_size=vocab_size,\n    block_size=block_size,\n    n_layer=4,\n    n_head=4,\n    d_model=256,\n    dropout=0.1).to(device)\n\nx_ids, y_ids = next(iter(train_loader))  # [B, T]\nx_ids = x_ids.to(device)\ny_ids = y_ids.to(device)\n\nlogits, loss = model(x_ids, y_ids)\nprint(\"logits shape:\", logits.shape)  # [B, T, vocab_size]\nprint(\"loss:\", loss.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"def token_acc(logits: torch.Tensor, targets: torch.Tensor) -> float:\n    \"\"\"\n    logits:  [B, T, V]\n    targets: [B, T]\n    return: accuracy escalar en [0,1]\n    \"\"\"\n    preds = logits.argmax(dim=-1)       # [B, T]\n    correct = (preds == targets).sum().item()\n    total   = targets.numel()\n    return correct / total\n\n\nclass WarmupCosineScheduler(torch.optim.lr_scheduler._LRScheduler):\n    \"\"\"\n    Warmup lineal + decaimiento coseno:\n      - durante 'warmup_steps': lr sube lineal desde 0 hasta base_lr\n      - después: decae con coseno hasta ~0 en 'max_steps'\n    \"\"\"\n    def __init__(self, optimizer, warmup_steps: int, max_steps: int, last_epoch: int = -1):\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self._step_num = 0\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        self._step_num += 1\n        step = self._step_num\n\n        if step <= self.warmup_steps:\n            # Warmup lineal: 0 -> 1\n            scale = step / float(max(1, self.warmup_steps))\n        else:\n            # Cosine decay de 1 -> 0\n            progress = (step - self.warmup_steps) / float(\n                max(1, self.max_steps - self.warmup_steps))\n            \n            # cos(pi * 0) = 1, cos(pi * 1) = -1  ⇒  scale va de 1 -> 0\n            scale = 0.5 * (1.0 + math.cos(math.pi * progress))\n\n        return [base_lr * scale for base_lr in self.base_lrs]\n\n\ndef create_optimizer_and_scheduler(\n    model,\n    base_lr: float,\n    weight_decay: float,\n    warmup_steps: int,\n    max_steps: int):\n  \n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=base_lr,\n        betas=(0.9, 0.95),\n        weight_decay=weight_decay)\n\n    scheduler = WarmupCosineScheduler(\n        optimizer=optimizer,\n        warmup_steps=warmup_steps,\n        max_steps=max_steps)\n    \n    return optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:30:10.992754Z","iopub.execute_input":"2025-11-18T05:30:10.993549Z","iopub.status.idle":"2025-11-18T05:30:11.001677Z","shell.execute_reply.started":"2025-11-18T05:30:10.993523Z","shell.execute_reply":"2025-11-18T05:30:11.000787Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import inspect\nfrom contextlib import contextmanager, nullcontext\nimport torch\n\n_DTYPE_MAP = {\n    \"bf16\": torch.bfloat16,\n    \"bfloat16\": torch.bfloat16,\n    \"fp16\": torch.float16,\n    \"float16\": torch.float16,}\n\n\ndef _cuda_dtype_supported(dtype: torch.dtype) -> bool:\n    if not torch.cuda.is_available():\n        return False\n\n    return dtype in (torch.bfloat16, torch.float16)\n\n\ndef make_grad_scaler(device: str = \"cuda\", enabled: bool = True):\n    \"\"\"\n    Devuelve un GradScaler compatible con tu versión de PyTorch.\n    - Si AMP no está habilitado, devuelve None.\n    - Soporta torch.amp.GradScaler('cuda'|'cpu') o sin args.\n    \"\"\"\n    if not enabled:\n        return None\n\n    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n        try:\n            sig = inspect.signature(torch.amp.GradScaler)\n            if len(sig.parameters) >= 1:\n                return torch.amp.GradScaler(device if device in (\"cuda\", \"cpu\") else \"cuda\")\n            else:\n                return torch.amp.GradScaler()\n        except Exception:\n            pass\n\n    if hasattr(torch.cuda, \"amp\") and hasattr(torch.cuda.amp, \"GradScaler\"):\n        return torch.cuda.amp.GradScaler()\n\n    return None\n\n@contextmanager\ndef autocast_ctx(\n    device: str = \"cuda\",\n    enabled: bool = True,\n    dtype: str = \"bf16\",    \n    cache_enabled: bool = True):\n  \n    \"\"\"\n    Contexto robusto para autocast:\n      - CUDA: torch.amp.autocast(device_type=\"cuda\", dtype=...)\n      - CPU:  torch.amp.autocast(device_type=\"cpu\",  dtype=torch.bfloat16) si enabled\n      - fallback: nullcontext().\n\n    Notas:\n      * En BF16 NO uses GradScaler.\n      * En FP16 sí puedes usar GradScaler (torch.amp.GradScaler / torch.cuda.amp.GradScaler).\n    \"\"\"\n    if not enabled:\n        with nullcontext():\n            yield\n        return\n\n    if device == \"cuda\":\n        want = _DTYPE_MAP.get(dtype.lower(), torch.bfloat16)\n        use = want if _cuda_dtype_supported(want) else torch.float16\n        with torch.amp.autocast(device_type=\"cuda\", dtype=use, cache_enabled=cache_enabled):\n            yield\n        return\n\n    if device == \"cpu\":\n        try:\n            with torch.amp.autocast(device_type=\"cpu\", dtype=torch.bfloat16, cache_enabled=cache_enabled):\n                yield\n        except Exception:\n            # fallback seguro si el backend no soporta cpu autocast\n            with nullcontext():\n                yield\n        return\n\n    with nullcontext():\n        yield\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:30:12.370217Z","iopub.execute_input":"2025-11-18T05:30:12.370928Z","iopub.status.idle":"2025-11-18T05:30:12.380685Z","shell.execute_reply.started":"2025-11-18T05:30:12.370903Z","shell.execute_reply":"2025-11-18T05:30:12.380028Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import math\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.nn.functional as F\n\ndef train_gpt_lm(\n    model,\n    train_loader,\n    val_loader=None,\n    *,\n    epochs: int = 10,\n    base_lr: float = 3e-4,\n    weight_decay: float = 0.01,\n    warmup_steps: int = 2000,   # en steps, no epochs\n    label_smoothing: float = 0.0, \n    grad_clip: float | None = 1.0,\n    device: str = \"cuda\",\n    ckpt_path: str = \"gptmini_best.pt\",\n    log_every: int = 100,\n    preview_every: int | None = None,\n    id2tok_fn=None,             # callable: List[int] -> str\n    amp_enabled: bool = True,\n    amp_dtype: str = \"bf16\",    \n    val_checking: bool = False,     # por defecto NO se hace validación\n    save_ckpt_every: int | None = None,  # si no hay val, guardar cada N epochs\n):\n    \"\"\"\n    Entrena un modelo GPTMini (decoder-only LM) sobre (x, y) donde:\n      - x: [B, T]  ids de entrada\n      - y: [B, T]  ids objetivo (shifted)\n\n    - Usa AdamW + WarmupCosineScheduler\n    - CrossEntropy con label_smoothing opcional (sin PAD)\n    - Gradient clipping\n    - AMP (bf16/fp16) usando autocast_ctx y make_grad_scaler\n    - Si val_checking=True y val_loader no es None:\n        * corre validación y guarda mejor checkpoint por val_loss\n      Si val_checking=False:\n        * NO corre validación; puede guardar checkpoint cada 'save_ckpt_every' epochs\n    \"\"\"\n    device = torch.device(device)\n    torch.set_float32_matmul_precision(\"high\")\n    model.to(device)\n    model.train()\n\n    # Estimar total de steps (para el scheduler)\n    total_steps = epochs * len(train_loader)\n    \n    optimizer, scheduler = create_optimizer_and_scheduler(\n        model,base_lr=base_lr,weight_decay=weight_decay,\n        warmup_steps=warmup_steps,max_steps=total_steps,)\n\n    # Loss con y sin smoothing\n    ce_train = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n    ce_eval  = nn.CrossEntropyLoss(label_smoothing=0.0)\n\n    # AMP: GradScaler sólo si estamos en fp16; en bf16 normalmente no hace falta\n    use_scaler = amp_enabled and (amp_dtype.lower() in (\"fp16\", \"float16\"))\n    scaler = make_grad_scaler(device=\"cuda\" if device.type == \"cuda\" else \"cpu\",\n                              enabled=use_scaler)\n\n    best_val = float(\"inf\")\n    history = {\n        \"train_loss\": [], \"val_loss\": [],\n        \"train_ppl\":  [], \"val_ppl\":  [],\n        \"train_tok_acc\": [], \"val_tok_acc\": []}\n\n    global_step = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss_sum, epoch_tokens = 0.0, 0\n        epoch_acc_sum = 0.0\n        t0 = time.time()\n\n        for it, (x_ids, y_ids) in enumerate(train_loader, start=1):\n            global_step += 1\n            x_ids = x_ids.to(device, non_blocking=True)\n            y_ids = y_ids.to(device, non_blocking=True)\n            B, T = x_ids.shape\n            tokens = B * T\n\n            optimizer.zero_grad(set_to_none=True)\n\n            with autocast_ctx(device=device.type, enabled=amp_enabled, dtype=amp_dtype):\n                # Usamos la loss interna del modelo\n                logits, loss = model(x_ids, y_ids)   # [B, T, V], escalar\n                if loss is None:\n                    raise RuntimeError(\"GPTMini.forward debe devolver loss si targets != None\")\n\n            if scaler is not None:\n                # Asegurarnos de que loss es escalar\n                if loss.dim() > 0:\n                    loss = loss.mean()\n            \n                scaler.scale(loss).backward()\n                if grad_clip is not None:\n                    scaler.unscale_(optimizer)\n                    clip_grad_norm_(model.parameters(), grad_clip)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                # Asegurarnos de que loss es escalar\n                if loss.dim() > 0:\n                    loss = loss.mean()\n            \n                loss.backward()\n                if grad_clip is not None:\n                    clip_grad_norm_(model.parameters(), grad_clip)\n                optimizer.step()\n\n            if scheduler is not None:\n                scheduler.step()\n\n            with torch.no_grad():\n                acc = token_acc(logits, y_ids)\n\n            epoch_loss_sum += loss.item() * tokens\n            epoch_acc_sum  += acc * tokens\n            epoch_tokens   += tokens\n\n            if it % log_every == 0:\n                avg_loss = epoch_loss_sum / max(1, epoch_tokens)\n                avg_ppl  = math.exp(avg_loss)\n                avg_acc  = epoch_acc_sum / max(1, epoch_tokens)\n                tok_per_sec = epoch_tokens / (time.time() - t0 + 1e-9)\n                print(f\"[Epoch {epoch} | step {it:4d}/{len(train_loader)} | global_step={global_step}] \"\n                      f\"train_loss={avg_loss:.4f}  ppl={avg_ppl:.2f}  \"\n                      f\"tok_acc={avg_acc*100:.2f}%  tok/s={tok_per_sec:,.0f}\")\n\n\n            # Preview LM (teacher forcing, argmax)\n            if (preview_every is not None) and (id2tok_fn is not None) and (it % preview_every == 0):\n                with torch.no_grad():\n                    preds = logits.argmax(dim=-1)  # [B, T]\n                    b0 = 0\n\n                    in_ids  = x_ids[b0].tolist()\n                    tgt_ids = y_ids[b0].tolist()\n                    pred_ids= preds[b0].tolist()\n\n                    max_show = min(80, len(in_ids))\n                    in_ids   = in_ids[:max_show]\n                    tgt_ids  = tgt_ids[:max_show]\n                    pred_ids = pred_ids[:max_show]\n\n                    ctx = id2tok_fn(in_ids)\n                    ref = id2tok_fn(tgt_ids)\n                    hyp = id2tok_fn(pred_ids)\n\n                    print(\"— preview (LM, teacher-forced argmax) —\")\n                    print(\"CTX:\", repr(ctx))\n                    print(\"REF:\", repr(ref))\n                    print(\"HYP:\", repr(hyp))\n\n        \n        # Fin de epoch: promedios train\n        train_loss = epoch_loss_sum / max(1, epoch_tokens)\n        train_ppl  = math.exp(train_loss)\n        train_acc  = epoch_acc_sum / max(1, epoch_tokens)\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_ppl\"].append(train_ppl)\n        history[\"train_tok_acc\"].append(train_acc * 100.0)\n\n\n        # SIN VALIDACIÓN (val_checking == False o val_loader is None)\n        if (not val_checking) or (val_loader is None):\n            print(f\"Epoch {epoch} done | \"\n                  f\"train_loss={train_loss:.4f}  train_ppl={train_ppl:.2f}  \"\n                  f\"train_tok_acc={train_acc*100:.2f}%\")\n\n            # Guardar checkpoint cada 'save_ckpt_every' \n            if save_ckpt_every is not None and (epoch % save_ckpt_every == 0):\n                model_to_save = model.module if hasattr(model, \"module\") else model\n                torch.save({\n                    \"model_state\": model_to_save.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict(),\n                    \"epoch\": epoch}, ckpt_path)\n                \n                print(f\"Guardado checkpoint (cada {save_ckpt_every} epochs) -> {ckpt_path}\")\n\n            continue\n\n        # CON VALIDACIÓN (val_checking == True y val_loader no es None) \n        model.eval()\n        val_loss_sum, val_tokens = 0.0, 0\n        val_acc_sum = 0.0\n\n        with torch.no_grad():\n            for x_ids, y_ids in val_loader:\n                x_ids = x_ids.to(device, non_blocking=True)\n                y_ids = y_ids.to(device, non_blocking=True)\n                B, T = x_ids.shape\n                tokens = B * T\n\n                with autocast_ctx(device=device.type, enabled=amp_enabled, dtype=amp_dtype):\n                    logits, _ = model(x_ids, None)\n                    V = logits.size(-1)\n                    loss = ce_eval(\n                        logits.view(B * T, V),\n                        y_ids.view(B * T))\n\n                acc = token_acc(logits, y_ids)\n\n                val_loss_sum += loss.item() * tokens\n                val_acc_sum  += acc * tokens\n                val_tokens   += tokens\n\n        val_loss = val_loss_sum / max(1, val_tokens)\n        val_ppl  = math.exp(val_loss)\n        val_acc  = val_acc_sum / max(1, val_tokens)\n\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_ppl\"].append(val_ppl)\n        history[\"val_tok_acc\"].append(val_acc * 100.0)\n\n        print(f\"Epoch {epoch} done | \"\n              f\"train_loss={train_loss:.4f}  train_ppl={train_ppl:.2f}  train_tok_acc={train_acc*100:.2f}%  \"\n              f\"val_loss={val_loss:.4f}    val_ppl={val_ppl:.2f}    val_tok_acc={val_acc*100:.2f}%\")\n\n        # Guardar mejor checkpoint por val_loss\n        if val_loss < best_val:\n            best_val = val_loss\n            model_to_save = model.module if hasattr(model, \"module\") else model\n            torch.save({\n                    \"model_state\": model_to_save.state_dict(),\n                    \"optimizer_state\": optimizer.state_dict(),\n                    \"epoch\": epoch,\n                    \"val_loss\": val_loss,\n                }, ckpt_path)\n            \n            print(f\"Guardado checkpoint (best val_loss={val_loss:.4f}) -> {ckpt_path}\")\n\n    return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:51:51.362684Z","iopub.execute_input":"2025-11-18T05:51:51.363364Z","iopub.status.idle":"2025-11-18T05:51:51.384533Z","shell.execute_reply.started":"2025-11-18T05:51:51.363337Z","shell.execute_reply":"2025-11-18T05:51:51.383825Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.ipc_collect()\n\nimport torch, gc\ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:33:10.128631Z","iopub.execute_input":"2025-11-18T06:33:10.129471Z","iopub.status.idle":"2025-11-18T06:33:10.579979Z","shell.execute_reply.started":"2025-11-18T06:33:10.129439Z","shell.execute_reply":"2025-11-18T06:33:10.579198Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab_size()\nblock_size = 256\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = GPTMini(\n    vocab_size=vocab_size,\n    block_size=block_size,  \n    n_layer=8,             \n    n_head=8,            \n    d_model=512,dropout=0.1,).to(device)\n\nif torch.cuda.device_count() > 1:\n    print(f\"Usando {torch.cuda.device_count()} GPUs con DataParallel\")\n    model = torch.nn.DataParallel(model)\n    use_dataparallel = True\n\ndef id2tok_fn(ids):\n    return tokenizer.decode(ids)\n\n\nhistory = train_gpt_lm(\n    model,\n    train_loader,\n    val_loader=val_loader,\n    epochs=10,\n    base_lr=3e-4,\n    weight_decay=0.01,\n    warmup_steps=2000,\n    label_smoothing=0.1,\n    grad_clip=1.0,\n    device=device,\n    ckpt_path=\"gptmini_owt10k.pt\",\n    log_every=150,\n    preview_every=500,\n    id2tok_fn=id2tok_fn,\n    amp_enabled=True,\n    amp_dtype=\"fp16\",   \n    val_checking = False , save_ckpt_every = 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T05:52:10.511275Z","iopub.execute_input":"2025-11-18T05:52:10.512064Z","iopub.status.idle":"2025-11-18T06:21:26.993097Z","shell.execute_reply.started":"2025-11-18T05:52:10.512038Z","shell.execute_reply":"2025-11-18T06:21:26.991554Z"}},"outputs":[{"name":"stdout","text":"Usando 2 GPUs con DataParallel\n[Epoch 1 | step  150/680 | global_step=150] train_loss=8.9753  ppl=7905.66  tok_acc=3.49%  tok/s=62,364\n[Epoch 1 | step  300/680 | global_step=300] train_loss=8.2705  ppl=3906.99  tok_acc=5.50%  tok/s=63,175\n[Epoch 1 | step  450/680 | global_step=450] train_loss=7.8397  ppl=2539.43  tok_acc=6.93%  tok/s=63,386\n— preview (LM, teacher-forced argmax) —\nCTX: \" eldemar's royal treasury. their priorities during this time consisted of investigating the invasion forces, looking for possible signs of red robe, trying to track down the rest of the missing key pieces and figuring out some way to leave the time loop. of course, since actually retrieving even the known pieces of the key was impossible with their current skills, and they had no idea what kind\"\nREF: \"emar's royal treasury. their priorities during this time consisted of investigating the invasion forces, looking for possible signs of red robe, trying to track down the rest of the missing key pieces and figuring out some way to leave the time loop. of course, since actually retrieving even the known pieces of the key was impossible with their current skills, and they had no idea what kind of\"\nHYP: ',,, the,,\\n the, the was to,, the to the.. the, the to, the,,, the to the. to first. the first,.. the. to to the to the the first..\\n the. the the the,. the first to. the first. the. the first.. and the was the the to the of'\n[Epoch 1 | step  600/680 | global_step=600] train_loss=7.5426  ppl=1886.64  tok_acc=8.03%  tok/s=63,418\nEpoch 1 done | train_loss=7.4135  train_ppl=1658.14  train_tok_acc=8.55%\n[Epoch 2 | step  150/680 | global_step=830] train_loss=6.2898  ppl=539.04  tok_acc=13.37%  tok/s=62,978\n[Epoch 2 | step  300/680 | global_step=980] train_loss=6.2122  ppl=498.82  tok_acc=13.83%  tok/s=63,274\n[Epoch 2 | step  450/680 | global_step=1130] train_loss=6.1364  ppl=462.38  tok_acc=14.27%  tok/s=63,392\n— preview (LM, teacher-forced argmax) —\nCTX: ' destination. there was no new cable record for the walking dead spinoff in its second week, but there was a potentially telling victory. while its 8.2 million total viewers and 4.1 rating last night was down -18% in total viewers and -16% in adults 18-49 from its record-shattering august 23 debut that garnered 10.1 million viewers and a'\nREF: '. there was no new cable record for the walking dead spinoff in its second week, but there was a potentially telling victory. while its 8.2 million total viewers and 4.1 rating last night was down -18% in total viewers and -16% in adults 18-49 from its record-shattering august 23 debut that garnered 10.1 million viewers and a 4'\nHYP: \",\\n the's a time-,, the first..,. the own-, and he's a few the the.\\n the own-\\n,, of, the,\\n., year, a to the, of the of, the the,. the.,year, the first.year--.,,. waso in,\\n,,, the year\"\n[Epoch 2 | step  600/680 | global_step=1280] train_loss=6.0681  ppl=431.85  tok_acc=14.66%  tok/s=63,452\nEpoch 2 done | train_loss=6.0334  train_ppl=417.13  train_tok_acc=14.86%\n[Epoch 3 | step  150/680 | global_step=1510] train_loss=5.6820  ppl=293.54  tok_acc=16.88%  tok/s=62,867\n[Epoch 3 | step  300/680 | global_step=1660] train_loss=5.6385  ppl=281.04  tok_acc=17.14%  tok/s=63,291\n[Epoch 3 | step  450/680 | global_step=1810] train_loss=5.5964  ppl=269.44  tok_acc=17.38%  tok/s=63,472\n— preview (LM, teacher-forced argmax) —\nCTX: ' her. she headed to a south los angeles storage facility where they last stored their belongings.\\n\\nshe found her mother sitting on a garbage bag full of clothes.\\n\\n“khadijah’s here!” her sister jeanine yells. her mother’s face brightened.\\n\\nshe explained the details of her graduation, the bus route to get there and gave her'\nREF: '. she headed to a south los angeles storage facility where they last stored their belongings.\\n\\nshe found her mother sitting on a garbage bag full of clothes.\\n\\n“khadijah’s here!” her sister jeanine yells. her mother’s face brightened.\\n\\nshe explained the details of her graduation, the bus route to get there and gave her mother'\nHYP: ' her she said to her new court angeles,,, she were year in own to.\\n\\nthe said the \" to on the new,, of the,\\n\\n“iil,i,s first is\\n mother said-,ve,\\n mother,s mother her, her\\n\\n“ was that “ of her parentsuation of “ man,, the a, she her a'\n[Epoch 3 | step  600/680 | global_step=1960] train_loss=5.5536  ppl=258.18  tok_acc=17.64%  tok/s=63,533\nEpoch 3 done | train_loss=5.5330  train_ppl=252.89  train_tok_acc=17.77%\nGuardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n[Epoch 4 | step  150/680 | global_step=2190] train_loss=5.2710  ppl=194.62  tok_acc=19.35%  tok/s=63,515\n[Epoch 4 | step  300/680 | global_step=2340] train_loss=5.2379  ppl=188.28  tok_acc=19.57%  tok/s=63,366\n[Epoch 4 | step  450/680 | global_step=2490] train_loss=5.2096  ppl=183.01  tok_acc=19.77%  tok/s=63,484\n— preview (LM, teacher-forced argmax) —\nCTX: ' been devised to the same effect.\\n\\nhe added that providing small- to medium-range planes manufactured by the same company played a central role in the government’s plan to rejuvenate iran’s air transportation fleet.\\n\\nakhoundi further emphasized that atr will start to deliver the planes that iran has purchased within the next few weeks.\\n\\nhe also stressed that'\nREF: ' devised to the same effect.\\n\\nhe added that providing small- to medium-range planes manufactured by the same company played a central role in the government’s plan to rejuvenate iran’s air transportation fleet.\\n\\nakhoundi further emphasized that atr will start to deliver the planes that iran has purchased within the next few weeks.\\n\\nhe also stressed that iran'\nHYP: ' inised by the united day of\\n\\nthe said that the the-quality the-time- to by the first time. in few of in the world.s economy. make-ise the the.s office force..\\n\\n“- is, has that the the- be a the the first of have will been a the country few years.\\n\\nthe said said that the'\n[Epoch 4 | step  600/680 | global_step=2640] train_loss=5.1820  ppl=178.03  tok_acc=19.99%  tok/s=63,558\nEpoch 4 done | train_loss=5.1682  train_ppl=175.60  train_tok_acc=20.09%\n[Epoch 5 | step  150/680 | global_step=2870] train_loss=4.9551  ppl=141.90  tok_acc=21.60%  tok/s=63,434\n[Epoch 5 | step  300/680 | global_step=3020] train_loss=4.9375  ppl=139.41  tok_acc=21.75%  tok/s=63,400\n[Epoch 5 | step  450/680 | global_step=3170] train_loss=4.9189  ppl=136.86  tok_acc=21.91%  tok/s=63,514\n— preview (LM, teacher-forced argmax) —\nCTX: ' shared understanding about the parks and open space that our city needs as we grow and recommends sustainable funding strategies to meet those needs.\\n\\nbut, livability is about more than the physical environment around us – it is also about the vibrancy of our city and its neighborhoods. my budget includes new funding for my arts shared prosperity agenda – more than $7 million to support artists, cultural equity'\nREF: ' understanding about the parks and open space that our city needs as we grow and recommends sustainable funding strategies to meet those needs.\\n\\nbut, livability is about more than the physical environment around us – it is also about the vibrancy of our city and its neighborhoods. my budget includes new funding for my arts shared prosperity agenda – more than $7 million to support artists, cultural equity,'\nHYP: ' with of the future and the to. we community. to a can. ourends our development.. our our in.\\n\\nthe the weers is a the than a future development. the. and’ a a the mostrantance of the community. our own.\\n research is a and and the new and with and, and than a1.. $ the. and and,'\n[Epoch 5 | step  600/680 | global_step=3320] train_loss=4.8994  ppl=134.21  tok_acc=22.09%  tok/s=63,551\nEpoch 5 done | train_loss=4.8908  train_ppl=133.07  train_tok_acc=22.17%\n[Epoch 6 | step  150/680 | global_step=3550] train_loss=4.7168  ppl=111.81  tok_acc=23.61%  tok/s=63,605\n[Epoch 6 | step  300/680 | global_step=3700] train_loss=4.7019  ppl=110.16  tok_acc=23.83%  tok/s=63,705\n[Epoch 6 | step  450/680 | global_step=3850] train_loss=4.6926  ppl=109.13  tok_acc=23.94%  tok/s=63,547\n— preview (LM, teacher-forced argmax) —\nCTX: 'w0653b/rxdzz.txt.html http://u32.extabit.com/go/28du69vxbo4ix/?upld=1 http://d01.megashares.com/dl/22gofmh/rxdzz.txt http://minus.com/l3q9edct'\nREF: '0653b/rxdzz.txt.html http://u32.extabit.com/go/28du69vxbo4ix/?upld=1 http://d01.megashares.com/dl/22gofmh/rxdzz.txt http://minus.com/l3q9edctvs'\nHYP: 'w..21///a\\nxt.t.://wwwplo.org...org/c/c/xdddxd.cq=c0d://u3.coz...com/1/c/dd31xdx.htmlxt.://wwwor.com/dw...dd'\n[Epoch 6 | step  600/680 | global_step=4000] train_loss=4.6810  ppl=107.87  tok_acc=24.07%  tok/s=63,594\nEpoch 6 done | train_loss=4.6747  train_ppl=107.20  train_tok_acc=24.15%\nGuardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n[Epoch 7 | step  150/680 | global_step=4230] train_loss=4.5356  ppl=93.28  tok_acc=25.44%  tok/s=63,516\n[Epoch 7 | step  300/680 | global_step=4380] train_loss=4.5272  ppl=92.50  tok_acc=25.54%  tok/s=63,610\n[Epoch 7 | step  450/680 | global_step=4530] train_loss=4.5199  ppl=91.83  tok_acc=25.65%  tok/s=63,488\n— preview (LM, teacher-forced argmax) —\nCTX: 'ictions?\\n\\nnow we can find out. an iphone app, our climate has just been released with contributors including notable sceptics such as richard lindzen, roy spencer and even the uk\\'s own lord christopher monckton. the first thing you see when you open the app is a top 10 list, featuring the \"top 10 climate tips you should know\\'. their number 1 tip argues'\nREF: '?\\n\\nnow we can find out. an iphone app, our climate has just been released with contributors including notable sceptics such as richard lindzen, roy spencer and even the uk\\'s own lord christopher monckton. the first thing you see when you open the app is a top 10 list, featuring the \"top 10 climate tips you should know\\'. their number 1 tip argues that'\nHYP: ',\\n\\nthe,’’ a,\\n example,, we iphone is been been a. aors, the ones&ic, as thesonsey, andce, john the former. chief...k,.\\n first time is can is you\\'re the phone, a little-.. you a mostthe\"\"\"\" can be\"\\n own of, is that'\n[Epoch 7 | step  600/680 | global_step=4680] train_loss=4.5146  ppl=91.34  tok_acc=25.69%  tok/s=63,553\nEpoch 7 done | train_loss=4.5110  train_ppl=91.01  train_tok_acc=25.75%\n[Epoch 8 | step  150/680 | global_step=4910] train_loss=4.4031  ppl=81.71  tok_acc=26.72%  tok/s=63,387\n[Epoch 8 | step  300/680 | global_step=5060] train_loss=4.4017  ppl=81.59  tok_acc=26.75%  tok/s=63,542\n[Epoch 8 | step  450/680 | global_step=5210] train_loss=4.4009  ppl=81.52  tok_acc=26.80%  tok/s=63,451\n— preview (LM, teacher-forced argmax) —\nCTX: ' but they might contribute to changes of the global climate.”\\n\\nbuehler says that the technique used in the study has also been applied to study the mechanical properties of protein materials and polymers, whose structures are typically stabilized by hydrogen bonds. “for these structures, we found that the chemical conditions, for example, ph, ion concentration and ion type, are very important in'\nREF: ' they might contribute to changes of the global climate.”\\n\\nbuehler says that the technique used in the study has also been applied to study the mechanical properties of protein materials and polymers, whose structures are typically stabilized by hydrogen bonds. “for these structures, we found that the chemical conditions, for example, ph, ion concentration and ion type, are very important in affecting'\nHYP: ' it’ be to the to the new warming change\\n\\ntheirz,, the the climate is to the climate is been been used to the by effects effects of the and. thebi. but research are not usedized to the cells.\\nthe example experiments, the can that the cells cells are the example, areases areerg,, theional of are not important to the'\n[Epoch 8 | step  600/680 | global_step=5360] train_loss=4.3984  ppl=81.32  tok_acc=26.83%  tok/s=63,532\nEpoch 8 done | train_loss=4.3965  train_ppl=81.17  train_tok_acc=26.86%\n[Epoch 9 | step  150/680 | global_step=5590] train_loss=4.3315  ppl=76.06  tok_acc=27.44%  tok/s=63,634\n[Epoch 9 | step  300/680 | global_step=5740] train_loss=4.3283  ppl=75.81  tok_acc=27.52%  tok/s=63,611\n[Epoch 9 | step  450/680 | global_step=5890] train_loss=4.3258  ppl=75.62  tok_acc=27.56%  tok/s=63,604\n— preview (LM, teacher-forced argmax) —\nCTX: ' ... samsung is locked in a battle with apple.\\n\\n\"intellectual property rights are an important cornerstone of the single market. however, such rights should not be misused when they are essential to implement industry standards, which bring huge benefits to businesses and consumers alike,\" the eu competition commissioner, joaquin almunia, said in statement.\\n\\napple and samsung,'\nREF: ' samsung is locked in a battle with apple.\\n\\n\"intellectual property rights are an important cornerstone of the single market. however, such rights should not be misused when they are essential to implement industry standards, which bring huge benefits to businesses and consumers alike,\" the eu competition commissioner, joaquin almunia, said in statement.\\n\\napple and samsung, the'\nHYP: ' i is a in the similar against the,\\n\\n\"itoisingity\" are not important part,\",\" the world market,\" the, the as are be be usederable, it are not to the the,,\" but are the amounts to the and the,\",\" said company said commission said saidan,o,-,, said. a.\\n\\n\"\\'s apple are the'\n[Epoch 9 | step  600/680 | global_step=6040] train_loss=4.3251  ppl=75.58  tok_acc=27.56%  tok/s=63,486\nEpoch 9 done | train_loss=4.3249  train_ppl=75.55  train_tok_acc=27.57%\nGuardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n[Epoch 10 | step  150/680 | global_step=6270] train_loss=4.2960  ppl=73.40  tok_acc=27.87%  tok/s=63,344\n[Epoch 10 | step  300/680 | global_step=6420] train_loss=4.2961  ppl=73.42  tok_acc=27.85%  tok/s=63,368\n[Epoch 10 | step  450/680 | global_step=6570] train_loss=4.2958  ppl=73.39  tok_acc=27.86%  tok/s=63,444\n— preview (LM, teacher-forced argmax) —\nCTX: 'zz0pcqi4l2k\\\\\" rel=\"nofollow\"> marsh is currently being held in the dallas county jail on $3,500 bail—a bit more than the estimated $1.20 that six nuggets would normally cost. at least he\\'ll get three hots and a cot.\\n\\ncomment #5 [permalink]\\n\\n... floridiot'\nREF: '0pcqi4l2k\\\\\" rel=\"nofollow\"> marsh is currently being held in the dallas county jail on $3,500 bail—a bit more than the estimated $1.20 that six nuggets would normally cost. at least he\\'ll get three hots and a cot.\\n\\ncomment #5 [permalink]\\n\\n... floridiot said'\nHYP: 'le.0x:ffaxx\\\\ohttp\"\"#als a available used in the form area jail. the1.000,outsand $ of than $ other $1,5 million the monthsmn were be be $\\n least $ would be a ofots to a half.\\n\\n\" #12 [permalink]\\n\\n...\\nal=='\n[Epoch 10 | step  600/680 | global_step=6720] train_loss=4.2919  ppl=73.11  tok_acc=27.90%  tok/s=63,362\nEpoch 10 done | train_loss=4.2906  train_ppl=73.01  train_tok_acc=27.92%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"history = train_gpt_lm(\n    model,\n    train_loader,\n    val_loader=val_loader,\n    epochs=15,\n    base_lr=3e-4,\n    weight_decay=0.01,\n    warmup_steps=2000,\n    label_smoothing=0.1,\n    grad_clip=1.0,\n    device=device,\n    ckpt_path=\"gptmini_owt10k.pt\",\n    log_every=150,\n    preview_every=1000,\n    id2tok_fn=id2tok_fn,\n    amp_enabled=True,\n    amp_dtype=\"fp16\",   \n    val_checking = False , save_ckpt_every = 8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:26:54.122532Z","iopub.execute_input":"2025-11-18T06:26:54.122863Z","iopub.status.idle":"2025-11-18T06:33:03.125817Z","shell.execute_reply.started":"2025-11-18T06:26:54.122833Z","shell.execute_reply":"2025-11-18T06:33:03.124720Z"}},"outputs":[{"name":"stdout","text":"[Epoch 1 | step  150/680 | global_step=150] train_loss=4.2893  ppl=72.91  tok_acc=27.91%  tok/s=62,028\n[Epoch 1 | step  300/680 | global_step=300] train_loss=4.2883  ppl=72.84  tok_acc=27.93%  tok/s=63,176\n[Epoch 1 | step  450/680 | global_step=450] train_loss=4.2959  ppl=73.40  tok_acc=27.85%  tok/s=63,260\n[Epoch 1 | step  600/680 | global_step=600] train_loss=4.3012  ppl=73.79  tok_acc=27.78%  tok/s=63,290\nEpoch 1 done | train_loss=4.3020  train_ppl=73.85  train_tok_acc=27.78%\n[Epoch 2 | step  150/680 | global_step=830] train_loss=4.2913  ppl=73.06  tok_acc=27.82%  tok/s=63,453\n[Epoch 2 | step  300/680 | global_step=980] train_loss=4.2999  ppl=73.69  tok_acc=27.75%  tok/s=63,623\n[Epoch 2 | step  450/680 | global_step=1130] train_loss=4.3094  ppl=74.40  tok_acc=27.65%  tok/s=63,628\n[Epoch 2 | step  600/680 | global_step=1280] train_loss=4.3149  ppl=74.80  tok_acc=27.60%  tok/s=63,583\nEpoch 2 done | train_loss=4.3175  train_ppl=75.00  train_tok_acc=27.58%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3036610972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = train_gpt_lm(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/638714474.py\u001b[0m in \u001b[0;36mtrain_gpt_lm\u001b[0;34m(model, train_loader, val_loader, epochs, base_lr, weight_decay, warmup_steps, label_smoothing, grad_clip, device, ckpt_path, log_every, preview_every, id2tok_fn, amp_enabled, amp_dtype, val_checking, save_ckpt_every)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":46},{"cell_type":"code","source":"def generate(model, tokenizer, prompt, max_new_tokens=30, temperature=1.0, top_k=50, device=\"cuda\"):\n    model.eval()\n\n    ids = tokenizer.encode(prompt, add_special_tokens=False).ids\n    x = torch.tensor([ids], dtype=torch.long, device=device)\n\n    block_size = model.module.block_size if hasattr(model, \"module\") else model.block_size\n\n    for _ in range(max_new_tokens):\n        x_cond = x[:, -block_size:]\n\n        # forward\n        logits, _ = model(x_cond, None)\n        logits = logits[:, -1, :] / temperature\n\n        # top-k truncation\n        if top_k is not None:\n            values, _ = torch.topk(logits, top_k)\n            logits[logits < values[:, [-1]]] = -float(\"inf\")\n\n        probs = torch.softmax(logits, dim=-1)\n        next_id = torch.multinomial(probs, num_samples=1)\n\n        x = torch.cat([x, next_id], dim=1)\n\n    return tokenizer.decode(x[0].tolist())\n\n\nprompt = \"whats your name?\"\nprint(generate(model, tokenizer, prompt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T06:24:06.009586Z","iopub.execute_input":"2025-11-18T06:24:06.010198Z","iopub.status.idle":"2025-11-18T06:24:06.400582Z","shell.execute_reply.started":"2025-11-18T06:24:06.010172Z","shell.execute_reply":"2025-11-18T06:24:06.399877Z"}},"outputs":[{"name":"stdout","text":" whats your name? and if it's not an answer, you could see yourself in the future in order to find yourself.\n\nand that's not true.\n\n","output_type":"stream"}],"execution_count":45}]}