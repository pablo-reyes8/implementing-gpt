{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-18T06:37:33.632230Z",
     "iopub.status.busy": "2025-11-18T06:37:33.631614Z",
     "iopub.status.idle": "2025-11-18T06:49:24.017409Z",
     "shell.execute_reply": "2025-11-18T06:49:24.016599Z",
     "shell.execute_reply.started": "2025-11-18T06:37:33.632205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736f54e77b594960813e3c1ce859f0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88c4eb325294a25a0de7da81104ebeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/test-00000-of-00001.(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52aa11d3d0914439b143e68b834c1aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/train-00000-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ec5956a4974a7f85fd33c7a777e7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/train-00001-of-00002(…):   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2de30a670cd49d789f6b4445278e2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-103-raw-v1/validation-00000-of-(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bfd66cd3a64300872cc9fc0715aca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6261e15fad764500a407d2d6297d79b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf31a250907475bafbcea04b4c59009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1801350\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 3760\n",
      "})\n",
      "Entrenando tokenizer BPE...\n",
      "\n",
      "\n",
      "\n",
      "Tamaño vocabulario: 16000\n",
      "Tokenizer guardado en /kaggle/working/wikitext103_tokenizer.json\n",
      "Tokenizando y concatenando textos...\n",
      "Total de tokens en este split: 119,513,253\n",
      "Número de secuencias: 465,032\n",
      "Forma inputs:  torch.Size([465032, 256])\n",
      "Forma targets: torch.Size([465032, 256])\n",
      "Tokenizando y concatenando textos...\n",
      "Total de tokens en este split: 250,245\n",
      "Número de secuencias: 973\n",
      "Forma inputs:  torch.Size([973, 256])\n",
      "Forma targets: torch.Size([973, 256])\n",
      "Batch x shape: torch.Size([64, 256])\n",
      "Batch y shape: torch.Size([64, 256])\n",
      "Texto ejemplo (primer sample de x):\n",
      " nuns . \n",
      " the actor is the french association pour la béatification de l 'impératrice zita . \n",
      " the postulator for the cause is father alexander leonhardt . the judge of the tribunal is father bruno bonnet . the promoter of justice is the father françois scrive . \n",
      " = = titles , styles , honours and arms = = \n",
      " = = = titles and styles = = = \n",
      " 9 may 1892 – 21 october 1911 : her royal highness princess zita of bourbon @-@ parma \n",
      " 21 october 1911 – 21 november 1916 : her imperial and royal highness archduchess and princess zita of austria , princess of hungary and bohemia , princess of bourbon @-@ parma \n",
      " 21 november 1916 – 11 november 1918 : her imperial and royal apostolic majesty the empress of austria , apostolic queen of hungary \n",
      " 11 november 1918 – 14 march 1989 : \n",
      " her imperial and royal apostolic majesty empress zita of austria , apostolic queen of hungary ( used outside austria ) \n",
      " zita , duchess of bar ( inscribed in her passport ) \n",
      " zita habsburg @-@ lothringen ( used in austria ) \n",
      " = = =\n"
     ]
    }
   ],
   "source": [
    "from src.data.load_large_data import *\n",
    "\n",
    "\n",
    "DATASET_NAME   = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-103-raw-v1\"   \n",
    "\n",
    "VOCAB_SIZE = 32000        \n",
    "MIN_FREQ  = 2\n",
    "BLOCK_SIZE   = 256  # Ventana de contexto         \n",
    "VAL_FRACTION   = 0.1\n",
    "TOKENIZER_PATH = Path(\"wikitext103_tokenizer.json\")  \n",
    "\n",
    "CPU_COUNT   = os.cpu_count() or 2\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS  = 2 if CPU_COUNT <= 2 else min(4, CPU_COUNT - 1)             \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "train_loader, val_loader, tokenizer = create_dataloaders()\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch x shape:\", x.shape) \n",
    "print(\"Batch y shape:\", y.shape) \n",
    "\n",
    "example_ids = x[0].tolist()\n",
    "text = tokenizer.decode(example_ids)\n",
    "print(\"Texto ejemplo (primer sample de x):\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:49:33.167082Z",
     "iopub.status.busy": "2025-11-18T06:49:33.166590Z",
     "iopub.status.idle": "2025-11-18T06:49:33.449781Z",
     "shell.execute_reply": "2025-11-18T06:49:33.448866Z",
     "shell.execute_reply.started": "2025-11-18T06:49:33.167036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Batch 0 ===\n",
      "\n",
      "--- Ejemplo 0 ---\n",
      "Input IDs   (primeros 50): [2442, 416, 283, 197, 259, 480, 199, 185, 507, 7922, 350, 1457, 240, 185, 15305, 336, 243, 182, 1047, 259, 434, 4987, 737, 1457, 313, 185, 5561, 5037, 189, 680, 185, 737, 209, 5399, 216, 189, 7254, 551, 864, 189, 215, 3684, 3671, 341, 56, 6608, 13262, 11077, 189, 6181]\n",
      "Target IDs  (primeros 50): [416, 283, 197, 259, 480, 199, 185, 507, 7922, 350, 1457, 240, 185, 15305, 336, 243, 182, 1047, 259, 434, 4987, 737, 1457, 313, 185, 5561, 5037, 189, 680, 185, 737, 209, 5399, 216, 189, 7254, 551, 864, 189, 215, 3684, 3671, 341, 56, 6608, 13262, 11077, 189, 6181, 7618]\n",
      "Input texto (modelo VE):\n",
      "'fall after an f @-@ 5 . the other predominant match on the undercard was a six @-@ man tag team match from the raw brand , between the team of triple h , ric flair , and chris jericho facing shawn michaels , kevin'\n",
      "Target texto (modelo DEBE predecir):\n",
      "' after an f @-@ 5 . the other predominant match on the undercard was a six @-@ man tag team match from the raw brand , between the team of triple h , ric flair , and chris jericho facing shawn michaels , kevin nash'\n",
      "\n",
      "--- Ejemplo 1 ---\n",
      "Input IDs   (primeros 50): [13771, 50, 10237, 217, 220, 331, 9281, 199, 463, 590, 2664, 182, 4474, 1394, 221, 5128, 62, 259, 3118, 11379, 3986, 209, 408, 9546, 3848, 204, 8900, 4546, 199, 204, 276, 495, 189, 185, 4032, 1587, 2038, 722, 428, 10025, 2811, 332, 204, 408, 2183, 452, 282, 50, 62, 334]\n",
      "Target IDs  (primeros 50): [50, 10237, 217, 220, 331, 9281, 199, 463, 590, 2664, 182, 4474, 1394, 221, 5128, 62, 259, 3118, 11379, 3986, 209, 408, 9546, 3848, 204, 8900, 4546, 199, 204, 276, 495, 189, 185, 4032, 1587, 2038, 722, 428, 10025, 2811, 332, 204, 408, 2183, 452, 282, 50, 62, 334, 210]\n",
      "Input texto (modelo VE):\n",
      "' hexi corridor to lop nur . they repelled a joint xiongnu @-@ qiang invasion of this northwestern territory in 111 bc . in that year , the han court established four new frontier commanderies in this region : jiuqu'\n",
      "Target texto (modelo DEBE predecir):\n",
      "'i corridor to lop nur . they repelled a joint xiongnu @-@ qiang invasion of this northwestern territory in 111 bc . in that year , the han court established four new frontier commanderies in this region : jiuquan'\n",
      "\n",
      "================== RESUMEN AUTORREGRESIVO ==================\n",
      "Total posiciones comparadas (y[:, :-1] vs x[:, 1:]): 16320\n",
      "Coincidencias: 16320\n",
      "Proporción de coincidencia (ideal ~1.0): 1.000000\n",
      "\n",
      "Distribución de primeros tokens (input vs target):\n",
      "\n",
      "Top 10 tokens más frecuentes en x[:, 0]  (primer token que VE el modelo):\n",
      "  id=  185 | freq=     5 | texto=' the'\n",
      "  id=  256 | freq=     2 | texto=' as'\n",
      "  id=    3 | freq=     2 | texto=''\n",
      "  id=  199 | freq=     2 | texto=' .'\n",
      "  id=  229 | freq=     2 | texto=' ='\n",
      "  id= 2442 | freq=     1 | texto='fall'\n",
      "  id=13771 | freq=     1 | texto=' hex'\n",
      "  id=  217 | freq=     1 | texto=' to'\n",
      "  id= 4940 | freq=     1 | texto=' unique'\n",
      "  id=  351 | freq=     1 | texto='ud'\n",
      "\n",
      "Top 10 tokens más frecuentes en y[:, 0]  (primer token que DEBE predecir):\n",
      "  id=  229 | freq=     3 | texto=' ='\n",
      "  id=  452 | freq=     2 | texto=' :'\n",
      "  id=  215 | freq=     2 | texto=' and'\n",
      "  id=  259 | freq=     2 | texto=' @-@'\n",
      "  id=  199 | freq=     2 | texto=' .'\n",
      "  id=  185 | freq=     2 | texto=' the'\n",
      "  id=  209 | freq=     2 | texto=' of'\n",
      "  id=  416 | freq=     1 | texto=' after'\n",
      "  id=   50 | freq=     1 | texto='i'\n",
      "  id=  479 | freq=     1 | texto=' have'\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from src.data.data_utils import *\n",
    "  \n",
    "inspect_autoregressive_loader(train_loader, tokenizer,\n",
    "                              num_batches=1,  \n",
    "                              max_examples=2, \n",
    "                              max_tokens_print=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T05:27:39.741399Z",
     "iopub.status.busy": "2025-11-18T05:27:39.740569Z",
     "iopub.status.idle": "2025-11-18T05:27:39.927771Z",
     "shell.execute_reply": "2025-11-18T05:27:39.926784Z",
     "shell.execute_reply.started": "2025-11-18T05:27:39.741289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from src.model.embeddings import *\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "d_model = 256\n",
    "block_size = 256\n",
    "\n",
    "emb = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\n",
    "\n",
    "x_ids, y_ids = next(iter(train_loader))  \n",
    "x_ids = x_ids.to(device)\n",
    "\n",
    "x_emb = emb(x_ids)  # [B, T, d_model]\n",
    "print(x_emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T05:28:55.577568Z",
     "iopub.status.busy": "2025-11-18T05:28:55.577292Z",
     "iopub.status.idle": "2025-11-18T05:28:56.095549Z",
     "shell.execute_reply": "2025-11-18T05:28:56.094812Z",
     "shell.execute_reply.started": "2025-11-18T05:28:55.577549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from src.model.attention import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "d_model  = 256\n",
    "block_size = 256\n",
    "\n",
    "emb  = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\n",
    "attn = CausalSelfAttention(d_model, num_heads=8, block_size=block_size, dropout=0.1).to(device)\n",
    "\n",
    "x_ids, y_ids = next(iter(train_loader))  # [B, T]\n",
    "x_ids = x_ids.to(device)\n",
    "\n",
    "x = emb(x_ids)  # pesos + input \n",
    "x = attn(x)     # [B, T, d_model]\n",
    "print(x.shape) # [B, T, d_model] (self-attn causal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GPT blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T05:29:36.128881Z",
     "iopub.status.busy": "2025-11-18T05:29:36.128188Z",
     "iopub.status.idle": "2025-11-18T05:29:36.508530Z",
     "shell.execute_reply": "2025-11-18T05:29:36.507479Z",
     "shell.execute_reply.started": "2025-11-18T05:29:36.128850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de embeddings: torch.Size([64, 256, 256])\n",
      "Después de MLP: torch.Size([64, 256, 256])\n",
      "Después de GPT2Block: torch.Size([64, 256, 256])\n",
      "¿Hay NaNs en la salida? False\n"
     ]
    }
   ],
   "source": [
    "from src.model.gpt_blocks import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "d_model = 256\n",
    "block_size = 256\n",
    "num_heads = 8\n",
    "d_ff = 4 * d_model  # 1024\n",
    "\n",
    "emb = GPT2Embeddings(vocab_size, d_model, block_size, dropout=0.1).to(device)\n",
    "mlp = GPT2MLP(d_model, d_ff=d_ff, dropout=0.1).to(device)\n",
    "block = GPT2Block(d_model, num_heads=num_heads, block_size=block_size, \n",
    "                  d_ff=d_ff, dropout=0.1).to(device)\n",
    "\n",
    "\n",
    "x_ids, y_ids = next(iter(train_loader))  # [B, T]\n",
    "x_ids = x_ids.to(device)\n",
    "\n",
    "x = emb(x_ids)  # [B, T, d_model]\n",
    "print(f\"Después de embeddings: {x.shape}\")\n",
    "\n",
    "x_mlp = mlp(x)\n",
    "print(f\"Después de MLP: {x_mlp.shape}\")  # [B, T, d_model]\n",
    "\n",
    "x_block = block(x)\n",
    "print(f\"Después de GPT2Block: {x_block.shape}\")  # [B, T, d_model]\n",
    "\n",
    "print(f\"¿Hay NaNs en la salida? {torch.isnan(x_block).any().item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test FULL GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.model.gpt_model import *\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "block_size = 256  \n",
    "\n",
    "model = GPT2(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    d_model=256,\n",
    "    dropout=0.1).to(device)\n",
    "\n",
    "x_ids, y_ids = next(iter(train_loader))  # [B, T]\n",
    "x_ids = x_ids.to(device)\n",
    "y_ids = y_ids.to(device)\n",
    "\n",
    "logits, loss = model(x_ids, y_ids)\n",
    "print(\"logits shape:\", logits.shape)  # [B, T, vocab_size]\n",
    "print(\"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "import torch, gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T06:55:20.015334Z",
     "iopub.status.busy": "2025-11-18T06:55:20.015073Z",
     "iopub.status.idle": "2025-11-18T13:08:18.452811Z",
     "shell.execute_reply": "2025-11-18T13:08:18.451914Z",
     "shell.execute_reply.started": "2025-11-18T06:55:20.015319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando 2 GPUs con DataParallel\n",
      "[Epoch 1 | step  150/7267 | global_step=150] train_loss=8.9506  ppl=7712.62  tok_acc=4.44%  tok/s=51,953\n",
      "[Epoch 1 | step  300/7267 | global_step=300] train_loss=8.2313  ppl=3756.75  tok_acc=6.35%  tok/s=52,936\n",
      "[Epoch 1 | step  450/7267 | global_step=450] train_loss=7.7675  ppl=2362.63  tok_acc=8.10%  tok/s=52,863\n",
      "[Epoch 1 | step  600/7267 | global_step=600] train_loss=7.4614  ppl=1739.52  tok_acc=9.40%  tok/s=53,006\n",
      "[Epoch 1 | step  750/7267 | global_step=750] train_loss=7.2223  ppl=1369.65  tok_acc=10.55%  tok/s=53,011\n",
      "[Epoch 1 | step  900/7267 | global_step=900] train_loss=7.0297  ppl=1129.71  tok_acc=11.52%  tok/s=53,081\n",
      "[Epoch 1 | step 1050/7267 | global_step=1050] train_loss=6.8694  ppl=962.41  tok_acc=12.33%  tok/s=53,106\n",
      "[Epoch 1 | step 1200/7267 | global_step=1200] train_loss=6.7319  ppl=838.73  tok_acc=13.03%  tok/s=53,142\n",
      "[Epoch 1 | step 1350/7267 | global_step=1350] train_loss=6.6111  ppl=743.31  tok_acc=13.65%  tok/s=53,136\n",
      "[Epoch 1 | step 1500/7267 | global_step=1500] train_loss=6.5029  ppl=667.09  tok_acc=14.23%  tok/s=53,153\n",
      "[Epoch 1 | step 1650/7267 | global_step=1650] train_loss=6.4041  ppl=604.33  tok_acc=14.75%  tok/s=53,170\n",
      "[Epoch 1 | step 1800/7267 | global_step=1800] train_loss=6.3129  ppl=551.66  tok_acc=15.24%  tok/s=53,161\n",
      "[Epoch 1 | step 1950/7267 | global_step=1950] train_loss=6.2291  ppl=507.29  tok_acc=15.70%  tok/s=53,178\n",
      "[Epoch 1 | step 2100/7267 | global_step=2100] train_loss=6.1504  ppl=468.92  tok_acc=16.14%  tok/s=53,168\n",
      "[Epoch 1 | step 2250/7267 | global_step=2250] train_loss=6.0761  ppl=435.34  tok_acc=16.57%  tok/s=53,176\n",
      "[Epoch 1 | step 2400/7267 | global_step=2400] train_loss=6.0066  ppl=406.11  tok_acc=16.98%  tok/s=53,180\n",
      "[Epoch 1 | step 2550/7267 | global_step=2550] train_loss=5.9406  ppl=380.16  tok_acc=17.36%  tok/s=53,197\n",
      "[Epoch 1 | step 2700/7267 | global_step=2700] train_loss=5.8784  ppl=357.23  tok_acc=17.74%  tok/s=53,188\n",
      "[Epoch 1 | step 2850/7267 | global_step=2850] train_loss=5.8187  ppl=336.54  tok_acc=18.11%  tok/s=53,194\n",
      "[Epoch 1 | step 3000/7267 | global_step=3000] train_loss=5.7620  ppl=318.00  tok_acc=18.48%  tok/s=53,193\n",
      "[Epoch 1 | step 3150/7267 | global_step=3150] train_loss=5.7073  ppl=301.05  tok_acc=18.85%  tok/s=53,202\n",
      "[Epoch 1 | step 3300/7267 | global_step=3300] train_loss=5.6546  ppl=285.60  tok_acc=19.22%  tok/s=53,198\n",
      "[Epoch 1 | step 3450/7267 | global_step=3450] train_loss=5.6041  ppl=271.55  tok_acc=19.58%  tok/s=53,203\n",
      "[Epoch 1 | step 3600/7267 | global_step=3600] train_loss=5.5559  ppl=258.77  tok_acc=19.94%  tok/s=53,208\n",
      "[Epoch 1 | step 3750/7267 | global_step=3750] train_loss=5.5103  ppl=247.22  tok_acc=20.27%  tok/s=53,212\n",
      "[Epoch 1 | step 3900/7267 | global_step=3900] train_loss=5.4666  ppl=236.64  tok_acc=20.60%  tok/s=53,204\n",
      "[Epoch 1 | step 4050/7267 | global_step=4050] train_loss=5.4249  ppl=226.98  tok_acc=20.91%  tok/s=53,208\n",
      "[Epoch 1 | step 4200/7267 | global_step=4200] train_loss=5.3850  ppl=218.11  tok_acc=21.21%  tok/s=53,211\n",
      "[Epoch 1 | step 4350/7267 | global_step=4350] train_loss=5.3471  ppl=209.99  tok_acc=21.50%  tok/s=53,206\n",
      "[Epoch 1 | step 4500/7267 | global_step=4500] train_loss=5.3106  ppl=202.46  tok_acc=21.78%  tok/s=53,208\n",
      "[Epoch 1 | step 4650/7267 | global_step=4650] train_loss=5.2758  ppl=195.56  tok_acc=22.04%  tok/s=53,200\n",
      "[Epoch 1 | step 4800/7267 | global_step=4800] train_loss=5.2422  ppl=189.09  tok_acc=22.30%  tok/s=53,200\n",
      "[Epoch 1 | step 4950/7267 | global_step=4950] train_loss=5.2101  ppl=183.10  tok_acc=22.55%  tok/s=53,200\n",
      "[Epoch 1 | step 5100/7267 | global_step=5100] train_loss=5.1791  ppl=177.53  tok_acc=22.79%  tok/s=53,200\n",
      "[Epoch 1 | step 5250/7267 | global_step=5250] train_loss=5.1496  ppl=172.36  tok_acc=23.02%  tok/s=53,198\n",
      "[Epoch 1 | step 5400/7267 | global_step=5400] train_loss=5.1211  ppl=167.52  tok_acc=23.24%  tok/s=53,197\n",
      "[Epoch 1 | step 5550/7267 | global_step=5550] train_loss=5.0937  ppl=163.00  tok_acc=23.46%  tok/s=53,200\n",
      "[Epoch 1 | step 5700/7267 | global_step=5700] train_loss=5.0673  ppl=158.75  tok_acc=23.67%  tok/s=53,191\n",
      "[Epoch 1 | step 5850/7267 | global_step=5850] train_loss=5.0420  ppl=154.78  tok_acc=23.87%  tok/s=53,191\n",
      "[Epoch 1 | step 6000/7267 | global_step=6000] train_loss=5.0173  ppl=151.01  tok_acc=24.06%  tok/s=53,184\n",
      "[Epoch 1 | step 6150/7267 | global_step=6150] train_loss=4.9939  ppl=147.51  tok_acc=24.25%  tok/s=53,184\n",
      "[Epoch 1 | step 6300/7267 | global_step=6300] train_loss=4.9712  ppl=144.19  tok_acc=24.43%  tok/s=53,181\n",
      "[Epoch 1 | step 6450/7267 | global_step=6450] train_loss=4.9490  ppl=141.03  tok_acc=24.60%  tok/s=53,181\n",
      "[Epoch 1 | step 6600/7267 | global_step=6600] train_loss=4.9275  ppl=138.04  tok_acc=24.77%  tok/s=53,171\n",
      "[Epoch 1 | step 6750/7267 | global_step=6750] train_loss=4.9069  ppl=135.22  tok_acc=24.94%  tok/s=53,175\n",
      "[Epoch 1 | step 6900/7267 | global_step=6900] train_loss=4.8868  ppl=132.53  tok_acc=25.10%  tok/s=53,178\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: \" lieu of cash taxes ) to mobilize the populace in panning for gold in the kingdom 's rivers . this expense , coupled with rainilaiarivony 's removal of $ 50 @,@ 000 in silver and gold coins from the tomb of ranavalona i to offset the cost of purchasing arms in the run @-@ up to the first franco @-@ hova war , effectively empt\"\n",
      "REF: \"u of cash taxes ) to mobilize the populace in panning for gold in the kingdom 's rivers . this expense , coupled with rainilaiarivony 's removal of $ 50 @,@ 000 in silver and gold coins from the tomb of ranavalona i to offset the cost of purchasing arms in the run @-@ up to the first franco @-@ hova war , effectively emptied\"\n",
      "HYP: ' to , the , , , theize the governmentace of thehandu the , the united ofs territories . \\n was was which with the andization ,ismism ,s lack of the 1 million 000 in cash , silver , , the united of theuona , , theend the expansion of the the . the city of up to the city half @-@ spanishul war . and allowingied'\n",
      "[Epoch 1 | step 7050/7267 | global_step=7050] train_loss=4.8674  ppl=129.98  tok_acc=25.26%  tok/s=53,174\n",
      "[Epoch 1 | step 7200/7267 | global_step=7200] train_loss=4.8485  ppl=127.55  tok_acc=25.41%  tok/s=53,170\n",
      "Epoch 1 done | train_loss=4.8403  train_ppl=126.51  train_tok_acc=25.47%\n",
      "[Epoch 2 | step  150/7267 | global_step=7417] train_loss=3.9157  ppl=50.18  tok_acc=32.95%  tok/s=53,038\n",
      "[Epoch 2 | step  300/7267 | global_step=7567] train_loss=3.9144  ppl=50.12  tok_acc=32.94%  tok/s=53,172\n",
      "[Epoch 2 | step  450/7267 | global_step=7717] train_loss=3.9119  ppl=49.99  tok_acc=32.98%  tok/s=53,192\n",
      "[Epoch 2 | step  600/7267 | global_step=7867] train_loss=3.9082  ppl=49.81  tok_acc=33.02%  tok/s=53,156\n",
      "[Epoch 2 | step  750/7267 | global_step=8017] train_loss=3.9053  ppl=49.67  tok_acc=33.04%  tok/s=53,178\n",
      "[Epoch 2 | step  900/7267 | global_step=8167] train_loss=3.9038  ppl=49.59  tok_acc=33.06%  tok/s=53,210\n",
      "[Epoch 2 | step 1050/7267 | global_step=8317] train_loss=3.8997  ppl=49.39  tok_acc=33.10%  tok/s=53,195\n",
      "[Epoch 2 | step 1200/7267 | global_step=8467] train_loss=3.8964  ppl=49.22  tok_acc=33.13%  tok/s=53,187\n",
      "[Epoch 2 | step 1350/7267 | global_step=8617] train_loss=3.8923  ppl=49.02  tok_acc=33.16%  tok/s=53,167\n",
      "[Epoch 2 | step 1500/7267 | global_step=8767] train_loss=3.8892  ppl=48.87  tok_acc=33.20%  tok/s=53,167\n",
      "[Epoch 2 | step 1650/7267 | global_step=8917] train_loss=3.8856  ppl=48.69  tok_acc=33.24%  tok/s=53,178\n",
      "[Epoch 2 | step 1800/7267 | global_step=9067] train_loss=3.8822  ppl=48.53  tok_acc=33.27%  tok/s=53,187\n",
      "[Epoch 2 | step 1950/7267 | global_step=9217] train_loss=3.8791  ppl=48.38  tok_acc=33.30%  tok/s=53,181\n",
      "[Epoch 2 | step 2100/7267 | global_step=9367] train_loss=3.8756  ppl=48.21  tok_acc=33.34%  tok/s=53,199\n",
      "[Epoch 2 | step 2250/7267 | global_step=9517] train_loss=3.8725  ppl=48.06  tok_acc=33.37%  tok/s=53,182\n",
      "[Epoch 2 | step 2400/7267 | global_step=9667] train_loss=3.8698  ppl=47.93  tok_acc=33.40%  tok/s=53,192\n",
      "[Epoch 2 | step 2550/7267 | global_step=9817] train_loss=3.8667  ppl=47.79  tok_acc=33.43%  tok/s=53,186\n",
      "[Epoch 2 | step 2700/7267 | global_step=9967] train_loss=3.8638  ppl=47.64  tok_acc=33.46%  tok/s=53,195\n",
      "[Epoch 2 | step 2850/7267 | global_step=10117] train_loss=3.8607  ppl=47.50  tok_acc=33.49%  tok/s=53,196\n",
      "[Epoch 2 | step 3000/7267 | global_step=10267] train_loss=3.8577  ppl=47.35  tok_acc=33.52%  tok/s=53,200\n",
      "[Epoch 2 | step 3150/7267 | global_step=10417] train_loss=3.8545  ppl=47.21  tok_acc=33.55%  tok/s=53,204\n",
      "[Epoch 2 | step 3300/7267 | global_step=10567] train_loss=3.8515  ppl=47.06  tok_acc=33.58%  tok/s=53,193\n",
      "[Epoch 2 | step 3450/7267 | global_step=10717] train_loss=3.8483  ppl=46.91  tok_acc=33.62%  tok/s=53,198\n",
      "[Epoch 2 | step 3600/7267 | global_step=10867] train_loss=3.8454  ppl=46.78  tok_acc=33.64%  tok/s=53,197\n",
      "[Epoch 2 | step 3750/7267 | global_step=11017] train_loss=3.8427  ppl=46.65  tok_acc=33.67%  tok/s=53,201\n",
      "[Epoch 2 | step 3900/7267 | global_step=11167] train_loss=3.8397  ppl=46.51  tok_acc=33.70%  tok/s=53,195\n",
      "[Epoch 2 | step 4050/7267 | global_step=11317] train_loss=3.8367  ppl=46.37  tok_acc=33.73%  tok/s=53,201\n",
      "[Epoch 2 | step 4200/7267 | global_step=11467] train_loss=3.8341  ppl=46.25  tok_acc=33.76%  tok/s=53,203\n",
      "[Epoch 2 | step 4350/7267 | global_step=11617] train_loss=3.8316  ppl=46.14  tok_acc=33.78%  tok/s=53,200\n",
      "[Epoch 2 | step 4500/7267 | global_step=11767] train_loss=3.8293  ppl=46.03  tok_acc=33.81%  tok/s=53,192\n",
      "[Epoch 2 | step 4650/7267 | global_step=11917] train_loss=3.8264  ppl=45.90  tok_acc=33.84%  tok/s=53,191\n",
      "[Epoch 2 | step 4800/7267 | global_step=12067] train_loss=3.8242  ppl=45.80  tok_acc=33.86%  tok/s=53,190\n",
      "[Epoch 2 | step 4950/7267 | global_step=12217] train_loss=3.8216  ppl=45.68  tok_acc=33.89%  tok/s=53,183\n",
      "[Epoch 2 | step 5100/7267 | global_step=12367] train_loss=3.8191  ppl=45.56  tok_acc=33.91%  tok/s=53,186\n",
      "[Epoch 2 | step 5250/7267 | global_step=12517] train_loss=3.8162  ppl=45.43  tok_acc=33.94%  tok/s=53,182\n",
      "[Epoch 2 | step 5400/7267 | global_step=12667] train_loss=3.8134  ppl=45.30  tok_acc=33.96%  tok/s=53,182\n",
      "[Epoch 2 | step 5550/7267 | global_step=12817] train_loss=3.8108  ppl=45.19  tok_acc=33.99%  tok/s=53,182\n",
      "[Epoch 2 | step 5700/7267 | global_step=12967] train_loss=3.8084  ppl=45.08  tok_acc=34.01%  tok/s=53,185\n",
      "[Epoch 2 | step 5850/7267 | global_step=13117] train_loss=3.8059  ppl=44.97  tok_acc=34.04%  tok/s=53,184\n",
      "[Epoch 2 | step 6000/7267 | global_step=13267] train_loss=3.8034  ppl=44.85  tok_acc=34.06%  tok/s=53,187\n",
      "[Epoch 2 | step 6150/7267 | global_step=13417] train_loss=3.8009  ppl=44.74  tok_acc=34.09%  tok/s=53,189\n",
      "[Epoch 2 | step 6300/7267 | global_step=13567] train_loss=3.7986  ppl=44.64  tok_acc=34.11%  tok/s=53,187\n",
      "[Epoch 2 | step 6450/7267 | global_step=13717] train_loss=3.7961  ppl=44.53  tok_acc=34.14%  tok/s=53,188\n",
      "[Epoch 2 | step 6600/7267 | global_step=13867] train_loss=3.7938  ppl=44.43  tok_acc=34.16%  tok/s=53,187\n",
      "[Epoch 2 | step 6750/7267 | global_step=14017] train_loss=3.7916  ppl=44.33  tok_acc=34.18%  tok/s=53,192\n",
      "[Epoch 2 | step 6900/7267 | global_step=14167] train_loss=3.7892  ppl=44.22  tok_acc=34.21%  tok/s=53,190\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: 'ates and sounds with inherent secondary articulation have also been mostly rejected , with the idea that such features should be indicated with tie bars or diacritics : 〈 ƍ 〉 for [ zw ] is one . in addition , the rare voiceless implosives , 〈 ƥ ƭ ƈ ƙ ʠ 〉 , have'\n",
      "REF: ' and sounds with inherent secondary articulation have also been mostly rejected , with the idea that such features should be indicated with tie bars or diacritics : 〈 ƍ 〉 for [ zw ] is one . in addition , the rare voiceless implosives , 〈 ƥ ƭ ƈ ƙ ʠ 〉 , have been'\n",
      "HYP: ' , the like thely soundsulation . been been used used . but the use of the a are be used by a @-@ . aameritics . the� �͡ 〉 � � � ] ] , a of \\n the to the term andiceless isantsome are which� ư �ư �ư �ư �Ɗ 〉 for � been'\n",
      "[Epoch 2 | step 7050/7267 | global_step=14317] train_loss=3.7871  ppl=44.13  tok_acc=34.23%  tok/s=53,191\n",
      "[Epoch 2 | step 7200/7267 | global_step=14467] train_loss=3.7848  ppl=44.03  tok_acc=34.25%  tok/s=53,187\n",
      "Epoch 2 done | train_loss=3.7839  train_ppl=43.99  train_tok_acc=34.26%\n",
      "[Epoch 3 | step  150/7267 | global_step=14684] train_loss=3.6283  ppl=37.65  tok_acc=35.78%  tok/s=53,075\n",
      "[Epoch 3 | step  300/7267 | global_step=14834] train_loss=3.6295  ppl=37.69  tok_acc=35.75%  tok/s=52,940\n",
      "[Epoch 3 | step  450/7267 | global_step=14984] train_loss=3.6269  ppl=37.59  tok_acc=35.77%  tok/s=53,032\n",
      "[Epoch 3 | step  600/7267 | global_step=15134] train_loss=3.6273  ppl=37.61  tok_acc=35.76%  tok/s=53,030\n",
      "[Epoch 3 | step  750/7267 | global_step=15284] train_loss=3.6265  ppl=37.58  tok_acc=35.79%  tok/s=53,073\n",
      "[Epoch 3 | step  900/7267 | global_step=15434] train_loss=3.6256  ppl=37.55  tok_acc=35.80%  tok/s=53,102\n",
      "[Epoch 3 | step 1050/7267 | global_step=15584] train_loss=3.6256  ppl=37.55  tok_acc=35.80%  tok/s=53,124\n",
      "[Epoch 3 | step 1200/7267 | global_step=15734] train_loss=3.6253  ppl=37.54  tok_acc=35.81%  tok/s=53,130\n",
      "[Epoch 3 | step 1350/7267 | global_step=15884] train_loss=3.6251  ppl=37.53  tok_acc=35.81%  tok/s=53,133\n",
      "[Epoch 3 | step 1500/7267 | global_step=16034] train_loss=3.6253  ppl=37.54  tok_acc=35.81%  tok/s=53,142\n",
      "[Epoch 3 | step 1650/7267 | global_step=16184] train_loss=3.6237  ppl=37.48  tok_acc=35.83%  tok/s=53,137\n",
      "[Epoch 3 | step 1800/7267 | global_step=16334] train_loss=3.6231  ppl=37.45  tok_acc=35.84%  tok/s=53,134\n",
      "[Epoch 3 | step 1950/7267 | global_step=16484] train_loss=3.6231  ppl=37.45  tok_acc=35.84%  tok/s=53,145\n",
      "[Epoch 3 | step 2100/7267 | global_step=16634] train_loss=3.6225  ppl=37.43  tok_acc=35.85%  tok/s=53,148\n",
      "[Epoch 3 | step 2250/7267 | global_step=16784] train_loss=3.6220  ppl=37.41  tok_acc=35.85%  tok/s=53,151\n",
      "[Epoch 3 | step 2400/7267 | global_step=16934] train_loss=3.6216  ppl=37.40  tok_acc=35.86%  tok/s=53,151\n",
      "[Epoch 3 | step 2550/7267 | global_step=17084] train_loss=3.6213  ppl=37.38  tok_acc=35.86%  tok/s=53,141\n",
      "[Epoch 3 | step 2700/7267 | global_step=17234] train_loss=3.6200  ppl=37.34  tok_acc=35.88%  tok/s=53,145\n",
      "[Epoch 3 | step 2850/7267 | global_step=17384] train_loss=3.6189  ppl=37.30  tok_acc=35.89%  tok/s=53,142\n",
      "[Epoch 3 | step 3000/7267 | global_step=17534] train_loss=3.6184  ppl=37.28  tok_acc=35.90%  tok/s=53,144\n",
      "[Epoch 3 | step 3150/7267 | global_step=17684] train_loss=3.6175  ppl=37.25  tok_acc=35.91%  tok/s=53,140\n",
      "[Epoch 3 | step 3300/7267 | global_step=17834] train_loss=3.6168  ppl=37.22  tok_acc=35.92%  tok/s=53,133\n",
      "[Epoch 3 | step 3450/7267 | global_step=17984] train_loss=3.6155  ppl=37.17  tok_acc=35.93%  tok/s=53,138\n",
      "[Epoch 3 | step 3600/7267 | global_step=18134] train_loss=3.6145  ppl=37.13  tok_acc=35.94%  tok/s=53,136\n",
      "[Epoch 3 | step 3750/7267 | global_step=18284] train_loss=3.6138  ppl=37.11  tok_acc=35.95%  tok/s=53,134\n",
      "[Epoch 3 | step 3900/7267 | global_step=18434] train_loss=3.6129  ppl=37.07  tok_acc=35.96%  tok/s=53,131\n",
      "[Epoch 3 | step 4050/7267 | global_step=18584] train_loss=3.6121  ppl=37.04  tok_acc=35.97%  tok/s=53,129\n",
      "[Epoch 3 | step 4200/7267 | global_step=18734] train_loss=3.6112  ppl=37.01  tok_acc=35.98%  tok/s=53,123\n",
      "[Epoch 3 | step 4350/7267 | global_step=18884] train_loss=3.6107  ppl=36.99  tok_acc=35.99%  tok/s=53,127\n",
      "[Epoch 3 | step 4500/7267 | global_step=19034] train_loss=3.6099  ppl=36.96  tok_acc=35.99%  tok/s=53,123\n",
      "[Epoch 3 | step 4650/7267 | global_step=19184] train_loss=3.6091  ppl=36.93  tok_acc=36.00%  tok/s=53,125\n",
      "[Epoch 3 | step 4800/7267 | global_step=19334] train_loss=3.6087  ppl=36.92  tok_acc=36.01%  tok/s=53,127\n",
      "[Epoch 3 | step 4950/7267 | global_step=19484] train_loss=3.6078  ppl=36.88  tok_acc=36.02%  tok/s=53,129\n",
      "[Epoch 3 | step 5100/7267 | global_step=19634] train_loss=3.6066  ppl=36.84  tok_acc=36.03%  tok/s=53,126\n",
      "[Epoch 3 | step 5250/7267 | global_step=19784] train_loss=3.6057  ppl=36.81  tok_acc=36.04%  tok/s=53,129\n",
      "[Epoch 3 | step 5400/7267 | global_step=19934] train_loss=3.6047  ppl=36.77  tok_acc=36.05%  tok/s=53,133\n",
      "[Epoch 3 | step 5550/7267 | global_step=20084] train_loss=3.6039  ppl=36.74  tok_acc=36.06%  tok/s=53,133\n",
      "[Epoch 3 | step 5700/7267 | global_step=20234] train_loss=3.6029  ppl=36.71  tok_acc=36.07%  tok/s=53,132\n",
      "[Epoch 3 | step 5850/7267 | global_step=20384] train_loss=3.6023  ppl=36.68  tok_acc=36.08%  tok/s=53,129\n",
      "[Epoch 3 | step 6000/7267 | global_step=20534] train_loss=3.6015  ppl=36.65  tok_acc=36.09%  tok/s=53,130\n",
      "[Epoch 3 | step 6150/7267 | global_step=20684] train_loss=3.6007  ppl=36.62  tok_acc=36.10%  tok/s=53,130\n",
      "[Epoch 3 | step 6300/7267 | global_step=20834] train_loss=3.6000  ppl=36.60  tok_acc=36.11%  tok/s=53,133\n",
      "[Epoch 3 | step 6450/7267 | global_step=20984] train_loss=3.5992  ppl=36.57  tok_acc=36.12%  tok/s=53,134\n",
      "[Epoch 3 | step 6600/7267 | global_step=21134] train_loss=3.5983  ppl=36.54  tok_acc=36.13%  tok/s=53,137\n",
      "[Epoch 3 | step 6750/7267 | global_step=21284] train_loss=3.5974  ppl=36.50  tok_acc=36.14%  tok/s=53,139\n",
      "[Epoch 3 | step 6900/7267 | global_step=21434] train_loss=3.5964  ppl=36.47  tok_acc=36.15%  tok/s=53,144\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: ' in 1923 when chase died . ( in 1924 , after completing his studies at occidental college in los angeles , jaeger began a 30 @-@ year teaching career at riverside junior college in riverside , california . ) jaeger wrote the initial eulogy for eytel upon his death and in 1948 , recalling his time with him , jaeger said : \\n as'\n",
      "REF: ' 1923 when chase died . ( in 1924 , after completing his studies at occidental college in los angeles , jaeger began a 30 @-@ year teaching career at riverside junior college in riverside , california . ) jaeger wrote the initial eulogy for eytel upon his death and in 1948 , recalling his time with him , jaeger said : \\n as an'\n",
      "HYP: ' the , the was of \\n the the , the a the studies at theultal college , new angeles , california.ger moved to career @-@ year course career at theide high high in newide , california . he \\naeger was a first script @-@ogy , theelel , his death in his 1925 he whening the experience at the , \"aeger said , \" i a'\n",
      "[Epoch 3 | step 7050/7267 | global_step=21584] train_loss=3.5955  ppl=36.43  tok_acc=36.16%  tok/s=53,144\n",
      "[Epoch 3 | step 7200/7267 | global_step=21734] train_loss=3.5947  ppl=36.40  tok_acc=36.16%  tok/s=53,148\n",
      "Epoch 3 done | train_loss=3.5943  train_ppl=36.39  train_tok_acc=36.17%\n",
      "Guardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n",
      "[Epoch 4 | step  150/7267 | global_step=21951] train_loss=3.5011  ppl=33.15  tok_acc=37.06%  tok/s=53,068\n",
      "[Epoch 4 | step  300/7267 | global_step=22101] train_loss=3.4976  ppl=33.04  tok_acc=37.13%  tok/s=53,213\n",
      "[Epoch 4 | step  450/7267 | global_step=22251] train_loss=3.4995  ppl=33.10  tok_acc=37.13%  tok/s=53,151\n",
      "[Epoch 4 | step  600/7267 | global_step=22401] train_loss=3.4999  ppl=33.11  tok_acc=37.11%  tok/s=53,203\n",
      "[Epoch 4 | step  750/7267 | global_step=22551] train_loss=3.5014  ppl=33.16  tok_acc=37.10%  tok/s=53,186\n",
      "[Epoch 4 | step  900/7267 | global_step=22701] train_loss=3.5025  ppl=33.20  tok_acc=37.09%  tok/s=53,214\n",
      "[Epoch 4 | step 1050/7267 | global_step=22851] train_loss=3.5034  ppl=33.23  tok_acc=37.07%  tok/s=53,186\n",
      "[Epoch 4 | step 1200/7267 | global_step=23001] train_loss=3.5038  ppl=33.24  tok_acc=37.07%  tok/s=53,212\n",
      "[Epoch 4 | step 1350/7267 | global_step=23151] train_loss=3.5039  ppl=33.24  tok_acc=37.07%  tok/s=53,223\n",
      "[Epoch 4 | step 1500/7267 | global_step=23301] train_loss=3.5046  ppl=33.27  tok_acc=37.07%  tok/s=53,242\n",
      "[Epoch 4 | step 1650/7267 | global_step=23451] train_loss=3.5042  ppl=33.25  tok_acc=37.08%  tok/s=53,257\n",
      "[Epoch 4 | step 1800/7267 | global_step=23601] train_loss=3.5035  ppl=33.23  tok_acc=37.09%  tok/s=53,255\n",
      "[Epoch 4 | step 1950/7267 | global_step=23751] train_loss=3.5041  ppl=33.25  tok_acc=37.08%  tok/s=53,249\n",
      "[Epoch 4 | step 2100/7267 | global_step=23901] train_loss=3.5035  ppl=33.23  tok_acc=37.09%  tok/s=53,243\n",
      "[Epoch 4 | step 2250/7267 | global_step=24051] train_loss=3.5031  ppl=33.22  tok_acc=37.10%  tok/s=53,254\n",
      "[Epoch 4 | step 2400/7267 | global_step=24201] train_loss=3.5032  ppl=33.22  tok_acc=37.10%  tok/s=53,255\n",
      "[Epoch 4 | step 2550/7267 | global_step=24351] train_loss=3.5031  ppl=33.22  tok_acc=37.10%  tok/s=53,262\n",
      "[Epoch 4 | step 2700/7267 | global_step=24501] train_loss=3.5026  ppl=33.20  tok_acc=37.11%  tok/s=53,267\n",
      "[Epoch 4 | step 2850/7267 | global_step=24651] train_loss=3.5020  ppl=33.18  tok_acc=37.12%  tok/s=53,277\n",
      "[Epoch 4 | step 3000/7267 | global_step=24801] train_loss=3.5016  ppl=33.17  tok_acc=37.12%  tok/s=53,260\n",
      "[Epoch 4 | step 3150/7267 | global_step=24951] train_loss=3.5012  ppl=33.16  tok_acc=37.13%  tok/s=53,262\n",
      "[Epoch 4 | step 3300/7267 | global_step=25101] train_loss=3.5012  ppl=33.15  tok_acc=37.13%  tok/s=53,262\n",
      "[Epoch 4 | step 3450/7267 | global_step=25251] train_loss=3.5008  ppl=33.14  tok_acc=37.13%  tok/s=53,246\n",
      "[Epoch 4 | step 3600/7267 | global_step=25401] train_loss=3.5004  ppl=33.13  tok_acc=37.14%  tok/s=53,245\n",
      "[Epoch 4 | step 3750/7267 | global_step=25551] train_loss=3.5002  ppl=33.12  tok_acc=37.14%  tok/s=53,235\n",
      "[Epoch 4 | step 3900/7267 | global_step=25701] train_loss=3.5003  ppl=33.12  tok_acc=37.14%  tok/s=53,241\n",
      "[Epoch 4 | step 4050/7267 | global_step=25851] train_loss=3.5001  ppl=33.12  tok_acc=37.14%  tok/s=53,244\n",
      "[Epoch 4 | step 4200/7267 | global_step=26001] train_loss=3.4997  ppl=33.10  tok_acc=37.15%  tok/s=53,248\n",
      "[Epoch 4 | step 4350/7267 | global_step=26151] train_loss=3.4991  ppl=33.08  tok_acc=37.16%  tok/s=53,245\n",
      "[Epoch 4 | step 4500/7267 | global_step=26301] train_loss=3.4989  ppl=33.08  tok_acc=37.16%  tok/s=53,249\n",
      "[Epoch 4 | step 4650/7267 | global_step=26451] train_loss=3.4986  ppl=33.07  tok_acc=37.16%  tok/s=53,252\n",
      "[Epoch 4 | step 4800/7267 | global_step=26601] train_loss=3.4981  ppl=33.05  tok_acc=37.17%  tok/s=53,249\n",
      "[Epoch 4 | step 4950/7267 | global_step=26751] train_loss=3.4976  ppl=33.03  tok_acc=37.18%  tok/s=53,254\n",
      "[Epoch 4 | step 5100/7267 | global_step=26901] train_loss=3.4970  ppl=33.01  tok_acc=37.19%  tok/s=53,251\n",
      "[Epoch 4 | step 5250/7267 | global_step=27051] train_loss=3.4967  ppl=33.01  tok_acc=37.19%  tok/s=53,255\n",
      "[Epoch 4 | step 5400/7267 | global_step=27201] train_loss=3.4965  ppl=33.00  tok_acc=37.19%  tok/s=53,256\n",
      "[Epoch 4 | step 5550/7267 | global_step=27351] train_loss=3.4961  ppl=32.99  tok_acc=37.20%  tok/s=53,259\n",
      "[Epoch 4 | step 5700/7267 | global_step=27501] train_loss=3.4959  ppl=32.98  tok_acc=37.20%  tok/s=53,254\n",
      "[Epoch 4 | step 5850/7267 | global_step=27651] train_loss=3.4953  ppl=32.96  tok_acc=37.21%  tok/s=53,256\n",
      "[Epoch 4 | step 6000/7267 | global_step=27801] train_loss=3.4949  ppl=32.95  tok_acc=37.22%  tok/s=53,253\n",
      "[Epoch 4 | step 6150/7267 | global_step=27951] train_loss=3.4946  ppl=32.94  tok_acc=37.22%  tok/s=53,254\n",
      "[Epoch 4 | step 6300/7267 | global_step=28101] train_loss=3.4944  ppl=32.93  tok_acc=37.22%  tok/s=53,250\n",
      "[Epoch 4 | step 6450/7267 | global_step=28251] train_loss=3.4939  ppl=32.91  tok_acc=37.23%  tok/s=53,253\n",
      "[Epoch 4 | step 6600/7267 | global_step=28401] train_loss=3.4935  ppl=32.90  tok_acc=37.23%  tok/s=53,253\n",
      "[Epoch 4 | step 6750/7267 | global_step=28551] train_loss=3.4932  ppl=32.89  tok_acc=37.24%  tok/s=53,254\n",
      "[Epoch 4 | step 6900/7267 | global_step=28701] train_loss=3.4927  ppl=32.87  tok_acc=37.25%  tok/s=53,250\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: ' . the title track , a \" densely produced blast of layered vocals [ and ] strummed acoustic guitars \" , features a \" circling bass riff \" similar to that of \" picture book \" by the kinks . \" waiting \" , which has been categorized as a \" retro @-@ pop lament \" , is based on the riff from petula clark \\'s 1964 song \" downtown'\n",
      "REF: ' the title track , a \" densely produced blast of layered vocals [ and ] strummed acoustic guitars \" , features a \" circling bass riff \" similar to that of \" picture book \" by the kinks . \" waiting \" , which has been categorized as a \" retro @-@ pop lament \" , is based on the riff from petula clark \\'s 1964 song \" downtown \"'\n",
      "HYP: ' the first of was \" song songer @-@ track \" theered \" \" and ] autmer vocals guitar \" , was a guitar heavyqueing @-@ guitar \" . to the of the the of \" . the bandks . the the for was a features a describedized as a \" rock @-@ rock song \" , is a on the song of the sounds , \\'s song song \" the \"'\n",
      "[Epoch 4 | step 7050/7267 | global_step=28851] train_loss=3.4922  ppl=32.86  tok_acc=37.25%  tok/s=53,252\n",
      "[Epoch 4 | step 7200/7267 | global_step=29001] train_loss=3.4917  ppl=32.84  tok_acc=37.26%  tok/s=53,251\n",
      "Epoch 4 done | train_loss=3.4914  train_ppl=32.83  train_tok_acc=37.26%\n",
      "[Epoch 5 | step  150/7267 | global_step=29218] train_loss=3.4181  ppl=30.51  tok_acc=38.00%  tok/s=52,733\n",
      "[Epoch 5 | step  300/7267 | global_step=29368] train_loss=3.4178  ppl=30.50  tok_acc=38.01%  tok/s=53,040\n",
      "[Epoch 5 | step  450/7267 | global_step=29518] train_loss=3.4188  ppl=30.53  tok_acc=37.98%  tok/s=53,063\n",
      "[Epoch 5 | step  600/7267 | global_step=29668] train_loss=3.4174  ppl=30.49  tok_acc=38.01%  tok/s=53,114\n",
      "[Epoch 5 | step  750/7267 | global_step=29818] train_loss=3.4169  ppl=30.48  tok_acc=38.02%  tok/s=53,164\n",
      "[Epoch 5 | step  900/7267 | global_step=29968] train_loss=3.4171  ppl=30.48  tok_acc=38.02%  tok/s=53,190\n",
      "[Epoch 5 | step 1050/7267 | global_step=30118] train_loss=3.4184  ppl=30.52  tok_acc=38.01%  tok/s=53,188\n",
      "[Epoch 5 | step 1200/7267 | global_step=30268] train_loss=3.4198  ppl=30.56  tok_acc=37.99%  tok/s=53,200\n",
      "[Epoch 5 | step 1350/7267 | global_step=30418] train_loss=3.4202  ppl=30.57  tok_acc=37.99%  tok/s=53,199\n",
      "[Epoch 5 | step 1500/7267 | global_step=30568] train_loss=3.4209  ppl=30.60  tok_acc=37.98%  tok/s=53,215\n",
      "[Epoch 5 | step 1650/7267 | global_step=30718] train_loss=3.4213  ppl=30.61  tok_acc=37.98%  tok/s=53,208\n",
      "[Epoch 5 | step 1800/7267 | global_step=30868] train_loss=3.4219  ppl=30.63  tok_acc=37.97%  tok/s=53,216\n",
      "[Epoch 5 | step 1950/7267 | global_step=31018] train_loss=3.4223  ppl=30.64  tok_acc=37.97%  tok/s=53,225\n",
      "[Epoch 5 | step 2100/7267 | global_step=31168] train_loss=3.4226  ppl=30.65  tok_acc=37.97%  tok/s=53,229\n",
      "[Epoch 5 | step 2250/7267 | global_step=31318] train_loss=3.4230  ppl=30.66  tok_acc=37.97%  tok/s=53,229\n",
      "[Epoch 5 | step 2400/7267 | global_step=31468] train_loss=3.4236  ppl=30.68  tok_acc=37.96%  tok/s=53,241\n",
      "[Epoch 5 | step 2550/7267 | global_step=31618] train_loss=3.4234  ppl=30.67  tok_acc=37.97%  tok/s=53,244\n",
      "[Epoch 5 | step 2700/7267 | global_step=31768] train_loss=3.4234  ppl=30.67  tok_acc=37.97%  tok/s=53,250\n",
      "[Epoch 5 | step 2850/7267 | global_step=31918] train_loss=3.4229  ppl=30.66  tok_acc=37.98%  tok/s=53,252\n",
      "[Epoch 5 | step 3000/7267 | global_step=32068] train_loss=3.4231  ppl=30.67  tok_acc=37.98%  tok/s=53,255\n",
      "[Epoch 5 | step 3150/7267 | global_step=32218] train_loss=3.4233  ppl=30.67  tok_acc=37.98%  tok/s=53,261\n",
      "[Epoch 5 | step 3300/7267 | global_step=32368] train_loss=3.4231  ppl=30.67  tok_acc=37.98%  tok/s=53,261\n",
      "[Epoch 5 | step 3450/7267 | global_step=32518] train_loss=3.4233  ppl=30.67  tok_acc=37.98%  tok/s=53,266\n",
      "[Epoch 5 | step 3600/7267 | global_step=32668] train_loss=3.4230  ppl=30.66  tok_acc=37.99%  tok/s=53,262\n",
      "[Epoch 5 | step 3750/7267 | global_step=32818] train_loss=3.4225  ppl=30.64  tok_acc=38.00%  tok/s=53,266\n",
      "[Epoch 5 | step 3900/7267 | global_step=32968] train_loss=3.4224  ppl=30.64  tok_acc=38.00%  tok/s=53,266\n",
      "[Epoch 5 | step 4050/7267 | global_step=33118] train_loss=3.4222  ppl=30.64  tok_acc=38.00%  tok/s=53,262\n",
      "[Epoch 5 | step 4200/7267 | global_step=33268] train_loss=3.4222  ppl=30.64  tok_acc=38.00%  tok/s=53,267\n",
      "[Epoch 5 | step 4350/7267 | global_step=33418] train_loss=3.4221  ppl=30.63  tok_acc=38.00%  tok/s=53,257\n",
      "[Epoch 5 | step 4500/7267 | global_step=33568] train_loss=3.4221  ppl=30.63  tok_acc=38.00%  tok/s=53,254\n",
      "[Epoch 5 | step 4650/7267 | global_step=33718] train_loss=3.4220  ppl=30.63  tok_acc=38.00%  tok/s=53,248\n",
      "[Epoch 5 | step 4800/7267 | global_step=33868] train_loss=3.4218  ppl=30.62  tok_acc=38.01%  tok/s=53,249\n",
      "[Epoch 5 | step 4950/7267 | global_step=34018] train_loss=3.4215  ppl=30.62  tok_acc=38.01%  tok/s=53,238\n",
      "[Epoch 5 | step 5100/7267 | global_step=34168] train_loss=3.4213  ppl=30.61  tok_acc=38.02%  tok/s=53,237\n",
      "[Epoch 5 | step 5250/7267 | global_step=34318] train_loss=3.4214  ppl=30.61  tok_acc=38.02%  tok/s=53,237\n",
      "[Epoch 5 | step 5400/7267 | global_step=34468] train_loss=3.4213  ppl=30.61  tok_acc=38.02%  tok/s=53,233\n",
      "[Epoch 5 | step 5550/7267 | global_step=34618] train_loss=3.4214  ppl=30.61  tok_acc=38.02%  tok/s=53,227\n",
      "[Epoch 5 | step 5700/7267 | global_step=34768] train_loss=3.4213  ppl=30.61  tok_acc=38.02%  tok/s=53,226\n",
      "[Epoch 5 | step 5850/7267 | global_step=34918] train_loss=3.4211  ppl=30.60  tok_acc=38.02%  tok/s=53,228\n",
      "[Epoch 5 | step 6000/7267 | global_step=35068] train_loss=3.4209  ppl=30.60  tok_acc=38.03%  tok/s=53,228\n",
      "[Epoch 5 | step 6150/7267 | global_step=35218] train_loss=3.4207  ppl=30.59  tok_acc=38.03%  tok/s=53,227\n",
      "[Epoch 5 | step 6300/7267 | global_step=35368] train_loss=3.4206  ppl=30.59  tok_acc=38.03%  tok/s=53,223\n",
      "[Epoch 5 | step 6450/7267 | global_step=35518] train_loss=3.4203  ppl=30.58  tok_acc=38.04%  tok/s=53,226\n",
      "[Epoch 5 | step 6600/7267 | global_step=35668] train_loss=3.4200  ppl=30.57  tok_acc=38.04%  tok/s=53,221\n",
      "[Epoch 5 | step 6750/7267 | global_step=35818] train_loss=3.4195  ppl=30.55  tok_acc=38.05%  tok/s=53,222\n",
      "[Epoch 5 | step 6900/7267 | global_step=35968] train_loss=3.4192  ppl=30.54  tok_acc=38.05%  tok/s=53,219\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: \" war against the ottoman empire , which french diplomacy despite great efforts failed to achieve . furthermore , sobieski was opposed by the papacy , by polish gentry who saw the ottomans as the greater threat , and by polish magnates bribed by berlin and vienna . inner @-@ polish catholic opposition to an intervention on the protestant hungarian rebels ' side added to the resentments . thus ,\"\n",
      "REF: \" against the ottoman empire , which french diplomacy despite great efforts failed to achieve . furthermore , sobieski was opposed by the papacy , by polish gentry who saw the ottomans as the greater threat , and by polish magnates bribed by berlin and vienna . inner @-@ polish catholic opposition to an intervention on the protestant hungarian rebels ' side added to the resentments . thus , while\"\n",
      "HYP: \" , the united empire . the was andats was the pressure to to achieve . \\n , the @-@ib was ' a to the frenchacy , who the diplomry , had the french as a only power of and by the diplomates whoribing them the . the . \\n @-@ city politicsism to the ottoman in the part side throne , demands was to the situationentment of \\n , the\"\n",
      "[Epoch 5 | step 7050/7267 | global_step=36118] train_loss=3.4188  ppl=30.53  tok_acc=38.06%  tok/s=53,221\n",
      "[Epoch 5 | step 7200/7267 | global_step=36268] train_loss=3.4186  ppl=30.53  tok_acc=38.06%  tok/s=53,222\n",
      "Epoch 5 done | train_loss=3.4185  train_ppl=30.52  train_tok_acc=38.06%\n",
      "[Epoch 6 | step  150/7267 | global_step=36485] train_loss=3.3485  ppl=28.46  tok_acc=38.75%  tok/s=53,076\n",
      "[Epoch 6 | step  300/7267 | global_step=36635] train_loss=3.3454  ppl=28.37  tok_acc=38.80%  tok/s=52,992\n",
      "[Epoch 6 | step  450/7267 | global_step=36785] train_loss=3.3501  ppl=28.50  tok_acc=38.75%  tok/s=53,086\n",
      "[Epoch 6 | step  600/7267 | global_step=36935] train_loss=3.3522  ppl=28.57  tok_acc=38.73%  tok/s=53,116\n",
      "[Epoch 6 | step  750/7267 | global_step=37085] train_loss=3.3540  ppl=28.62  tok_acc=38.72%  tok/s=53,093\n",
      "[Epoch 6 | step  900/7267 | global_step=37235] train_loss=3.3554  ppl=28.66  tok_acc=38.70%  tok/s=53,073\n",
      "[Epoch 6 | step 1050/7267 | global_step=37385] train_loss=3.3558  ppl=28.67  tok_acc=38.70%  tok/s=53,078\n",
      "[Epoch 6 | step 1200/7267 | global_step=37535] train_loss=3.3570  ppl=28.70  tok_acc=38.69%  tok/s=53,090\n",
      "[Epoch 6 | step 1350/7267 | global_step=37685] train_loss=3.3591  ppl=28.76  tok_acc=38.66%  tok/s=53,087\n",
      "[Epoch 6 | step 1500/7267 | global_step=37835] train_loss=3.3597  ppl=28.78  tok_acc=38.66%  tok/s=53,093\n",
      "[Epoch 6 | step 1650/7267 | global_step=37985] train_loss=3.3603  ppl=28.80  tok_acc=38.66%  tok/s=53,073\n",
      "[Epoch 6 | step 1800/7267 | global_step=38135] train_loss=3.3607  ppl=28.81  tok_acc=38.67%  tok/s=53,089\n",
      "[Epoch 6 | step 1950/7267 | global_step=38285] train_loss=3.3606  ppl=28.81  tok_acc=38.67%  tok/s=53,091\n",
      "[Epoch 6 | step 2100/7267 | global_step=38435] train_loss=3.3614  ppl=28.83  tok_acc=38.66%  tok/s=53,088\n",
      "[Epoch 6 | step 2250/7267 | global_step=38585] train_loss=3.3614  ppl=28.83  tok_acc=38.66%  tok/s=53,082\n",
      "[Epoch 6 | step 2400/7267 | global_step=38735] train_loss=3.3619  ppl=28.84  tok_acc=38.66%  tok/s=53,095\n",
      "[Epoch 6 | step 2550/7267 | global_step=38885] train_loss=3.3623  ppl=28.86  tok_acc=38.65%  tok/s=53,106\n",
      "[Epoch 6 | step 2700/7267 | global_step=39035] train_loss=3.3622  ppl=28.85  tok_acc=38.66%  tok/s=53,109\n",
      "[Epoch 6 | step 2850/7267 | global_step=39185] train_loss=3.3624  ppl=28.86  tok_acc=38.66%  tok/s=53,105\n",
      "[Epoch 6 | step 3000/7267 | global_step=39335] train_loss=3.3620  ppl=28.85  tok_acc=38.66%  tok/s=53,114\n",
      "[Epoch 6 | step 3150/7267 | global_step=39485] train_loss=3.3622  ppl=28.85  tok_acc=38.66%  tok/s=53,116\n",
      "[Epoch 6 | step 3300/7267 | global_step=39635] train_loss=3.3621  ppl=28.85  tok_acc=38.67%  tok/s=53,115\n",
      "[Epoch 6 | step 3450/7267 | global_step=39785] train_loss=3.3625  ppl=28.86  tok_acc=38.66%  tok/s=53,123\n",
      "[Epoch 6 | step 3600/7267 | global_step=39935] train_loss=3.3630  ppl=28.87  tok_acc=38.66%  tok/s=53,115\n",
      "[Epoch 6 | step 3750/7267 | global_step=40085] train_loss=3.3632  ppl=28.88  tok_acc=38.66%  tok/s=53,120\n",
      "[Epoch 6 | step 3900/7267 | global_step=40235] train_loss=3.3629  ppl=28.87  tok_acc=38.66%  tok/s=53,125\n",
      "[Epoch 6 | step 4050/7267 | global_step=40385] train_loss=3.3631  ppl=28.88  tok_acc=38.66%  tok/s=53,133\n",
      "[Epoch 6 | step 4200/7267 | global_step=40535] train_loss=3.3630  ppl=28.87  tok_acc=38.67%  tok/s=53,131\n",
      "[Epoch 6 | step 4350/7267 | global_step=40685] train_loss=3.3627  ppl=28.87  tok_acc=38.67%  tok/s=53,138\n",
      "[Epoch 6 | step 4500/7267 | global_step=40835] train_loss=3.3625  ppl=28.86  tok_acc=38.68%  tok/s=53,143\n",
      "[Epoch 6 | step 4650/7267 | global_step=40985] train_loss=3.3625  ppl=28.86  tok_acc=38.67%  tok/s=53,144\n",
      "[Epoch 6 | step 4800/7267 | global_step=41135] train_loss=3.3628  ppl=28.87  tok_acc=38.67%  tok/s=53,152\n",
      "[Epoch 6 | step 4950/7267 | global_step=41285] train_loss=3.3628  ppl=28.87  tok_acc=38.67%  tok/s=53,152\n",
      "[Epoch 6 | step 5100/7267 | global_step=41435] train_loss=3.3627  ppl=28.87  tok_acc=38.68%  tok/s=53,158\n",
      "[Epoch 6 | step 5250/7267 | global_step=41585] train_loss=3.3626  ppl=28.86  tok_acc=38.68%  tok/s=53,163\n",
      "[Epoch 6 | step 5400/7267 | global_step=41735] train_loss=3.3624  ppl=28.86  tok_acc=38.68%  tok/s=53,166\n",
      "[Epoch 6 | step 5550/7267 | global_step=41885] train_loss=3.3624  ppl=28.86  tok_acc=38.68%  tok/s=53,163\n",
      "[Epoch 6 | step 5700/7267 | global_step=42035] train_loss=3.3622  ppl=28.85  tok_acc=38.69%  tok/s=53,168\n",
      "[Epoch 6 | step 5850/7267 | global_step=42185] train_loss=3.3622  ppl=28.85  tok_acc=38.69%  tok/s=53,165\n",
      "[Epoch 6 | step 6000/7267 | global_step=42335] train_loss=3.3619  ppl=28.84  tok_acc=38.70%  tok/s=53,168\n",
      "[Epoch 6 | step 6150/7267 | global_step=42485] train_loss=3.3617  ppl=28.84  tok_acc=38.70%  tok/s=53,165\n",
      "[Epoch 6 | step 6300/7267 | global_step=42635] train_loss=3.3616  ppl=28.83  tok_acc=38.70%  tok/s=53,168\n",
      "[Epoch 6 | step 6450/7267 | global_step=42785] train_loss=3.3613  ppl=28.83  tok_acc=38.70%  tok/s=53,173\n",
      "[Epoch 6 | step 6600/7267 | global_step=42935] train_loss=3.3610  ppl=28.82  tok_acc=38.71%  tok/s=53,175\n",
      "[Epoch 6 | step 6750/7267 | global_step=43085] train_loss=3.3607  ppl=28.81  tok_acc=38.71%  tok/s=53,176\n",
      "[Epoch 6 | step 6900/7267 | global_step=43235] train_loss=3.3605  ppl=28.80  tok_acc=38.72%  tok/s=53,179\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: ' perspective on the value of life ... it ’ s hard not to admire her . \" she stated that she had \" a bit of a freak @-@ out moment \" when she first saw the cat @-@ suit . when asked about fighting in the costume , johansson responded \" a big part of me is like \\' can i move in this ? can i run in it ? can i like throw'\n",
      "REF: ' on the value of life ... it ’ s hard not to admire her . \" she stated that she had \" a bit of a freak @-@ out moment \" when she first saw the cat @-@ suit . when asked about fighting in the costume , johansson responded \" a big part of me is like \\' can i move in this ? can i run in it ? can i like throw myself'\n",
      "HYP: ' of the subject of the , the is s a to to sayire the . \" \\n also that the was \" a lot of aweak show like of \" , she was met her filmast and , \\n asked about the , the film , sheansson said , i lot , of the , a a i i have \\' the world \\' i move in this ? \\' i go it your'\n",
      "[Epoch 6 | step 7050/7267 | global_step=43385] train_loss=3.3604  ppl=28.80  tok_acc=38.72%  tok/s=53,181\n",
      "[Epoch 6 | step 7200/7267 | global_step=43535] train_loss=3.3604  ppl=28.80  tok_acc=38.72%  tok/s=53,179\n",
      "Epoch 6 done | train_loss=3.3602  train_ppl=28.79  train_tok_acc=38.72%\n",
      "Guardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n",
      "[Epoch 7 | step  150/7267 | global_step=43752] train_loss=3.3072  ppl=27.31  tok_acc=39.29%  tok/s=52,953\n",
      "[Epoch 7 | step  300/7267 | global_step=43902] train_loss=3.3095  ppl=27.37  tok_acc=39.27%  tok/s=53,068\n",
      "[Epoch 7 | step  450/7267 | global_step=44052] train_loss=3.3076  ppl=27.32  tok_acc=39.30%  tok/s=53,051\n",
      "[Epoch 7 | step  600/7267 | global_step=44202] train_loss=3.3109  ppl=27.41  tok_acc=39.25%  tok/s=53,114\n",
      "[Epoch 7 | step  750/7267 | global_step=44352] train_loss=3.3122  ppl=27.45  tok_acc=39.24%  tok/s=53,108\n",
      "[Epoch 7 | step  900/7267 | global_step=44502] train_loss=3.3102  ppl=27.39  tok_acc=39.26%  tok/s=53,111\n",
      "[Epoch 7 | step 1050/7267 | global_step=44652] train_loss=3.3114  ppl=27.42  tok_acc=39.25%  tok/s=53,119\n",
      "[Epoch 7 | step 1200/7267 | global_step=44802] train_loss=3.3112  ppl=27.42  tok_acc=39.25%  tok/s=53,135\n",
      "[Epoch 7 | step 1350/7267 | global_step=44952] train_loss=3.3115  ppl=27.43  tok_acc=39.25%  tok/s=53,156\n",
      "[Epoch 7 | step 1500/7267 | global_step=45102] train_loss=3.3120  ppl=27.44  tok_acc=39.24%  tok/s=53,174\n",
      "[Epoch 7 | step 1650/7267 | global_step=45252] train_loss=3.3109  ppl=27.41  tok_acc=39.25%  tok/s=53,192\n",
      "[Epoch 7 | step 1800/7267 | global_step=45402] train_loss=3.3111  ppl=27.42  tok_acc=39.25%  tok/s=53,176\n",
      "[Epoch 7 | step 1950/7267 | global_step=45552] train_loss=3.3112  ppl=27.42  tok_acc=39.25%  tok/s=53,187\n",
      "[Epoch 7 | step 2100/7267 | global_step=45702] train_loss=3.3119  ppl=27.44  tok_acc=39.24%  tok/s=53,181\n",
      "[Epoch 7 | step 2250/7267 | global_step=45852] train_loss=3.3118  ppl=27.43  tok_acc=39.25%  tok/s=53,185\n",
      "[Epoch 7 | step 2400/7267 | global_step=46002] train_loss=3.3118  ppl=27.43  tok_acc=39.25%  tok/s=53,178\n",
      "[Epoch 7 | step 2550/7267 | global_step=46152] train_loss=3.3124  ppl=27.45  tok_acc=39.24%  tok/s=53,186\n",
      "[Epoch 7 | step 2700/7267 | global_step=46302] train_loss=3.3122  ppl=27.45  tok_acc=39.24%  tok/s=53,189\n",
      "[Epoch 7 | step 2850/7267 | global_step=46452] train_loss=3.3125  ppl=27.45  tok_acc=39.24%  tok/s=53,194\n",
      "[Epoch 7 | step 3000/7267 | global_step=46602] train_loss=3.3128  ppl=27.46  tok_acc=39.24%  tok/s=53,195\n",
      "[Epoch 7 | step 3150/7267 | global_step=46752] train_loss=3.3129  ppl=27.46  tok_acc=39.24%  tok/s=53,199\n",
      "[Epoch 7 | step 3300/7267 | global_step=46902] train_loss=3.3126  ppl=27.46  tok_acc=39.24%  tok/s=53,202\n",
      "[Epoch 7 | step 3450/7267 | global_step=47052] train_loss=3.3133  ppl=27.47  tok_acc=39.24%  tok/s=53,198\n",
      "[Epoch 7 | step 3600/7267 | global_step=47202] train_loss=3.3132  ppl=27.47  tok_acc=39.24%  tok/s=53,205\n",
      "[Epoch 7 | step 3750/7267 | global_step=47352] train_loss=3.3134  ppl=27.48  tok_acc=39.24%  tok/s=53,201\n",
      "[Epoch 7 | step 3900/7267 | global_step=47502] train_loss=3.3133  ppl=27.48  tok_acc=39.24%  tok/s=53,211\n",
      "[Epoch 7 | step 4050/7267 | global_step=47652] train_loss=3.3132  ppl=27.47  tok_acc=39.25%  tok/s=53,221\n",
      "[Epoch 7 | step 4200/7267 | global_step=47802] train_loss=3.3135  ppl=27.48  tok_acc=39.24%  tok/s=53,219\n",
      "[Epoch 7 | step 4350/7267 | global_step=47952] train_loss=3.3137  ppl=27.49  tok_acc=39.24%  tok/s=53,211\n",
      "[Epoch 7 | step 4500/7267 | global_step=48102] train_loss=3.3136  ppl=27.48  tok_acc=39.25%  tok/s=53,211\n",
      "[Epoch 7 | step 4650/7267 | global_step=48252] train_loss=3.3138  ppl=27.49  tok_acc=39.24%  tok/s=53,212\n",
      "[Epoch 7 | step 4800/7267 | global_step=48402] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,206\n",
      "[Epoch 7 | step 4950/7267 | global_step=48552] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,207\n",
      "[Epoch 7 | step 5100/7267 | global_step=48702] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,202\n",
      "[Epoch 7 | step 5250/7267 | global_step=48852] train_loss=3.3135  ppl=27.48  tok_acc=39.25%  tok/s=53,206\n",
      "[Epoch 7 | step 5400/7267 | global_step=49002] train_loss=3.3133  ppl=27.48  tok_acc=39.26%  tok/s=53,207\n",
      "[Epoch 7 | step 5550/7267 | global_step=49152] train_loss=3.3133  ppl=27.48  tok_acc=39.26%  tok/s=53,214\n",
      "[Epoch 7 | step 5700/7267 | global_step=49302] train_loss=3.3131  ppl=27.47  tok_acc=39.26%  tok/s=53,210\n",
      "[Epoch 7 | step 5850/7267 | global_step=49452] train_loss=3.3130  ppl=27.47  tok_acc=39.26%  tok/s=53,213\n",
      "[Epoch 7 | step 6000/7267 | global_step=49602] train_loss=3.3127  ppl=27.46  tok_acc=39.26%  tok/s=53,210\n",
      "[Epoch 7 | step 6150/7267 | global_step=49752] train_loss=3.3127  ppl=27.46  tok_acc=39.26%  tok/s=53,214\n",
      "[Epoch 7 | step 6300/7267 | global_step=49902] train_loss=3.3127  ppl=27.46  tok_acc=39.27%  tok/s=53,212\n",
      "[Epoch 7 | step 6450/7267 | global_step=50052] train_loss=3.3127  ppl=27.46  tok_acc=39.27%  tok/s=53,217\n",
      "[Epoch 7 | step 6600/7267 | global_step=50202] train_loss=3.3126  ppl=27.46  tok_acc=39.27%  tok/s=53,219\n",
      "[Epoch 7 | step 6750/7267 | global_step=50352] train_loss=3.3126  ppl=27.46  tok_acc=39.27%  tok/s=53,223\n",
      "[Epoch 7 | step 6900/7267 | global_step=50502] train_loss=3.3125  ppl=27.45  tok_acc=39.27%  tok/s=53,220\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: ' minnesota governor and olympic medalist \\n les auge , hockey player \\n alana blahoski , olympic gold medalist in hockey \\n matthew d. bostrom , former saint paul police assistant chief and current ramsey county sheriff . \\n herb brooks , hockey coach of the \" miracle on ice \" gold medal winning u.s. olympic hockey team \\n warren e.'\n",
      "REF: ' governor and olympic medalist \\n les auge , hockey player \\n alana blahoski , olympic gold medalist in hockey \\n matthew d. bostrom , former saint paul police assistant chief and current ramsey county sheriff . \\n herb brooks , hockey coach of the \" miracle on ice \" gold medal winning u.s. olympic hockey team \\n warren e. burger'\n",
      "HYP: ' . of the goldist . =ter.ux a \\n \\n =ain ,anche , , , olympic medal medalist \\n the \\n = g. smithwick , olympic olympic @-@ \\' chief \\n \\n former olympicsey university sheriff \\n \\n = rit , former player \\n the university newacle \" ice \" \\n medalist team.s. soccer gold team \\n = b. smith'\n",
      "[Epoch 7 | step 7050/7267 | global_step=50652] train_loss=3.3122  ppl=27.45  tok_acc=39.28%  tok/s=53,223\n",
      "[Epoch 7 | step 7200/7267 | global_step=50802] train_loss=3.3120  ppl=27.44  tok_acc=39.28%  tok/s=53,223\n",
      "Epoch 7 done | train_loss=3.3120  train_ppl=27.44  train_tok_acc=39.28%\n",
      "[Epoch 8 | step  150/7267 | global_step=51019] train_loss=3.2681  ppl=26.26  tok_acc=39.77%  tok/s=52,856\n",
      "[Epoch 8 | step  300/7267 | global_step=51169] train_loss=3.2704  ppl=26.32  tok_acc=39.75%  tok/s=53,113\n",
      "[Epoch 8 | step  450/7267 | global_step=51319] train_loss=3.2675  ppl=26.24  tok_acc=39.75%  tok/s=53,120\n",
      "[Epoch 8 | step  600/7267 | global_step=51469] train_loss=3.2676  ppl=26.25  tok_acc=39.76%  tok/s=53,144\n",
      "[Epoch 8 | step  750/7267 | global_step=51619] train_loss=3.2689  ppl=26.28  tok_acc=39.75%  tok/s=53,195\n",
      "[Epoch 8 | step  900/7267 | global_step=51769] train_loss=3.2702  ppl=26.32  tok_acc=39.73%  tok/s=53,205\n",
      "[Epoch 8 | step 1050/7267 | global_step=51919] train_loss=3.2707  ppl=26.33  tok_acc=39.72%  tok/s=53,192\n",
      "[Epoch 8 | step 1200/7267 | global_step=52069] train_loss=3.2713  ppl=26.34  tok_acc=39.72%  tok/s=53,197\n",
      "[Epoch 8 | step 1350/7267 | global_step=52219] train_loss=3.2710  ppl=26.34  tok_acc=39.73%  tok/s=53,196\n",
      "[Epoch 8 | step 1500/7267 | global_step=52369] train_loss=3.2716  ppl=26.35  tok_acc=39.72%  tok/s=53,206\n",
      "[Epoch 8 | step 1650/7267 | global_step=52519] train_loss=3.2719  ppl=26.36  tok_acc=39.72%  tok/s=53,198\n",
      "[Epoch 8 | step 1800/7267 | global_step=52669] train_loss=3.2718  ppl=26.36  tok_acc=39.73%  tok/s=53,201\n",
      "[Epoch 8 | step 1950/7267 | global_step=52819] train_loss=3.2723  ppl=26.37  tok_acc=39.72%  tok/s=53,216\n",
      "[Epoch 8 | step 2100/7267 | global_step=52969] train_loss=3.2727  ppl=26.38  tok_acc=39.72%  tok/s=53,218\n",
      "[Epoch 8 | step 2250/7267 | global_step=53119] train_loss=3.2730  ppl=26.39  tok_acc=39.72%  tok/s=53,215\n",
      "[Epoch 8 | step 2400/7267 | global_step=53269] train_loss=3.2731  ppl=26.39  tok_acc=39.72%  tok/s=53,219\n",
      "[Epoch 8 | step 2550/7267 | global_step=53419] train_loss=3.2735  ppl=26.40  tok_acc=39.71%  tok/s=53,215\n",
      "[Epoch 8 | step 2700/7267 | global_step=53569] train_loss=3.2736  ppl=26.41  tok_acc=39.71%  tok/s=53,201\n",
      "[Epoch 8 | step 2850/7267 | global_step=53719] train_loss=3.2737  ppl=26.41  tok_acc=39.71%  tok/s=53,195\n",
      "[Epoch 8 | step 3000/7267 | global_step=53869] train_loss=3.2737  ppl=26.41  tok_acc=39.71%  tok/s=53,186\n",
      "[Epoch 8 | step 3150/7267 | global_step=54019] train_loss=3.2740  ppl=26.42  tok_acc=39.70%  tok/s=53,184\n",
      "[Epoch 8 | step 3300/7267 | global_step=54169] train_loss=3.2740  ppl=26.42  tok_acc=39.71%  tok/s=53,186\n",
      "[Epoch 8 | step 3450/7267 | global_step=54319] train_loss=3.2743  ppl=26.43  tok_acc=39.70%  tok/s=53,186\n",
      "[Epoch 8 | step 3600/7267 | global_step=54469] train_loss=3.2741  ppl=26.42  tok_acc=39.70%  tok/s=53,185\n",
      "[Epoch 8 | step 3750/7267 | global_step=54619] train_loss=3.2741  ppl=26.42  tok_acc=39.71%  tok/s=53,181\n",
      "[Epoch 8 | step 3900/7267 | global_step=54769] train_loss=3.2742  ppl=26.42  tok_acc=39.71%  tok/s=53,188\n",
      "[Epoch 8 | step 4050/7267 | global_step=54919] train_loss=3.2738  ppl=26.41  tok_acc=39.71%  tok/s=53,183\n",
      "[Epoch 8 | step 4200/7267 | global_step=55069] train_loss=3.2737  ppl=26.41  tok_acc=39.71%  tok/s=53,192\n",
      "[Epoch 8 | step 4350/7267 | global_step=55219] train_loss=3.2738  ppl=26.41  tok_acc=39.71%  tok/s=53,192\n",
      "[Epoch 8 | step 4500/7267 | global_step=55369] train_loss=3.2738  ppl=26.41  tok_acc=39.71%  tok/s=53,204\n",
      "[Epoch 8 | step 4650/7267 | global_step=55519] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,207\n",
      "[Epoch 8 | step 4800/7267 | global_step=55669] train_loss=3.2738  ppl=26.41  tok_acc=39.72%  tok/s=53,217\n",
      "[Epoch 8 | step 4950/7267 | global_step=55819] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,221\n",
      "[Epoch 8 | step 5100/7267 | global_step=55969] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,228\n",
      "[Epoch 8 | step 5250/7267 | global_step=56119] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,237\n",
      "[Epoch 8 | step 5400/7267 | global_step=56269] train_loss=3.2735  ppl=26.40  tok_acc=39.72%  tok/s=53,238\n",
      "[Epoch 8 | step 5550/7267 | global_step=56419] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,238\n",
      "[Epoch 8 | step 5700/7267 | global_step=56569] train_loss=3.2739  ppl=26.42  tok_acc=39.71%  tok/s=53,246\n",
      "[Epoch 8 | step 5850/7267 | global_step=56719] train_loss=3.2741  ppl=26.42  tok_acc=39.71%  tok/s=53,255\n",
      "[Epoch 8 | step 6000/7267 | global_step=56869] train_loss=3.2740  ppl=26.42  tok_acc=39.71%  tok/s=53,259\n",
      "[Epoch 8 | step 6150/7267 | global_step=57019] train_loss=3.2739  ppl=26.41  tok_acc=39.72%  tok/s=53,263\n",
      "[Epoch 8 | step 6300/7267 | global_step=57169] train_loss=3.2739  ppl=26.41  tok_acc=39.72%  tok/s=53,264\n",
      "[Epoch 8 | step 6450/7267 | global_step=57319] train_loss=3.2741  ppl=26.42  tok_acc=39.72%  tok/s=53,265\n",
      "[Epoch 8 | step 6600/7267 | global_step=57469] train_loss=3.2740  ppl=26.42  tok_acc=39.72%  tok/s=53,259\n",
      "[Epoch 8 | step 6750/7267 | global_step=57619] train_loss=3.2739  ppl=26.41  tok_acc=39.72%  tok/s=53,262\n",
      "[Epoch 8 | step 6900/7267 | global_step=57769] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,258\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: ' as the corporation counsel in a great metropolis ; the merchant at the cross @-@ roads store is as much a business man as the merchant of new york ; the farmer who goes forth in the morning and toils all day , who begins in spring and toils all summer , and who by the application of brain and muscle to the natural resources of the country creates wealth , is as much a business man'\n",
      "REF: ' the corporation counsel in a great metropolis ; the merchant at the cross @-@ roads store is as much a business man as the merchant of new york ; the farmer who goes forth in the morning and toils all day , who begins in spring and toils all summer , and who by the application of brain and muscle to the natural resources of the country creates wealth , is as much a business man as'\n",
      "HYP: \" the first of , the case dealis . the company ' the timeroad river office in the follows as part . as a corporation at the york . and company ' is to to the city ; the the in the ; and is to the and ends eat , day , and the is the evening of thew brain , the city world of the city . a and wealth the much a business man as\"\n",
      "[Epoch 8 | step 7050/7267 | global_step=57919] train_loss=3.2735  ppl=26.40  tok_acc=39.73%  tok/s=53,261\n",
      "[Epoch 8 | step 7200/7267 | global_step=58069] train_loss=3.2734  ppl=26.40  tok_acc=39.73%  tok/s=53,262\n",
      "Epoch 8 done | train_loss=3.2734  train_ppl=26.40  train_tok_acc=39.73%\n",
      "[Epoch 9 | step  150/7267 | global_step=58286] train_loss=3.2475  ppl=25.73  tok_acc=40.00%  tok/s=53,137\n",
      "[Epoch 9 | step  300/7267 | global_step=58436] train_loss=3.2470  ppl=25.71  tok_acc=40.03%  tok/s=53,088\n",
      "[Epoch 9 | step  450/7267 | global_step=58586] train_loss=3.2460  ppl=25.69  tok_acc=40.03%  tok/s=53,142\n",
      "[Epoch 9 | step  600/7267 | global_step=58736] train_loss=3.2448  ppl=25.66  tok_acc=40.04%  tok/s=53,178\n",
      "[Epoch 9 | step  750/7267 | global_step=58886] train_loss=3.2447  ppl=25.65  tok_acc=40.03%  tok/s=53,191\n",
      "[Epoch 9 | step  900/7267 | global_step=59036] train_loss=3.2439  ppl=25.63  tok_acc=40.05%  tok/s=53,194\n",
      "[Epoch 9 | step 1050/7267 | global_step=59186] train_loss=3.2450  ppl=25.66  tok_acc=40.04%  tok/s=53,218\n",
      "[Epoch 9 | step 1200/7267 | global_step=59336] train_loss=3.2446  ppl=25.65  tok_acc=40.04%  tok/s=53,245\n",
      "[Epoch 9 | step 1350/7267 | global_step=59486] train_loss=3.2451  ppl=25.66  tok_acc=40.04%  tok/s=53,248\n",
      "[Epoch 9 | step 1500/7267 | global_step=59636] train_loss=3.2443  ppl=25.64  tok_acc=40.05%  tok/s=53,241\n",
      "[Epoch 9 | step 1650/7267 | global_step=59786] train_loss=3.2445  ppl=25.65  tok_acc=40.05%  tok/s=53,221\n",
      "[Epoch 9 | step 1800/7267 | global_step=59936] train_loss=3.2445  ppl=25.65  tok_acc=40.05%  tok/s=53,222\n",
      "[Epoch 9 | step 1950/7267 | global_step=60086] train_loss=3.2446  ppl=25.65  tok_acc=40.05%  tok/s=53,204\n",
      "[Epoch 9 | step 2100/7267 | global_step=60236] train_loss=3.2447  ppl=25.65  tok_acc=40.05%  tok/s=53,207\n",
      "[Epoch 9 | step 2250/7267 | global_step=60386] train_loss=3.2448  ppl=25.66  tok_acc=40.04%  tok/s=53,192\n",
      "[Epoch 9 | step 2400/7267 | global_step=60536] train_loss=3.2454  ppl=25.67  tok_acc=40.04%  tok/s=53,197\n",
      "[Epoch 9 | step 2550/7267 | global_step=60686] train_loss=3.2447  ppl=25.66  tok_acc=40.05%  tok/s=53,203\n",
      "[Epoch 9 | step 2700/7267 | global_step=60836] train_loss=3.2449  ppl=25.66  tok_acc=40.05%  tok/s=53,208\n",
      "[Epoch 9 | step 2850/7267 | global_step=60986] train_loss=3.2448  ppl=25.66  tok_acc=40.05%  tok/s=53,198\n",
      "[Epoch 9 | step 3000/7267 | global_step=61136] train_loss=3.2448  ppl=25.66  tok_acc=40.05%  tok/s=53,204\n",
      "[Epoch 9 | step 3150/7267 | global_step=61286] train_loss=3.2454  ppl=25.67  tok_acc=40.04%  tok/s=53,214\n",
      "[Epoch 9 | step 3300/7267 | global_step=61436] train_loss=3.2455  ppl=25.67  tok_acc=40.04%  tok/s=53,202\n",
      "[Epoch 9 | step 3450/7267 | global_step=61586] train_loss=3.2453  ppl=25.67  tok_acc=40.05%  tok/s=53,209\n",
      "[Epoch 9 | step 3600/7267 | global_step=61736] train_loss=3.2450  ppl=25.66  tok_acc=40.05%  tok/s=53,203\n",
      "[Epoch 9 | step 3750/7267 | global_step=61886] train_loss=3.2450  ppl=25.66  tok_acc=40.05%  tok/s=53,206\n",
      "[Epoch 9 | step 3900/7267 | global_step=62036] train_loss=3.2451  ppl=25.66  tok_acc=40.05%  tok/s=53,210\n",
      "[Epoch 9 | step 4050/7267 | global_step=62186] train_loss=3.2453  ppl=25.67  tok_acc=40.05%  tok/s=53,213\n",
      "[Epoch 9 | step 4200/7267 | global_step=62336] train_loss=3.2455  ppl=25.67  tok_acc=40.05%  tok/s=53,206\n",
      "[Epoch 9 | step 4350/7267 | global_step=62486] train_loss=3.2453  ppl=25.67  tok_acc=40.05%  tok/s=53,207\n",
      "[Epoch 9 | step 4500/7267 | global_step=62636] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,209\n",
      "[Epoch 9 | step 4650/7267 | global_step=62786] train_loss=3.2456  ppl=25.68  tok_acc=40.05%  tok/s=53,206\n",
      "[Epoch 9 | step 4800/7267 | global_step=62936] train_loss=3.2454  ppl=25.67  tok_acc=40.05%  tok/s=53,208\n",
      "[Epoch 9 | step 4950/7267 | global_step=63086] train_loss=3.2454  ppl=25.67  tok_acc=40.06%  tok/s=53,208\n",
      "[Epoch 9 | step 5100/7267 | global_step=63236] train_loss=3.2453  ppl=25.67  tok_acc=40.06%  tok/s=53,211\n",
      "[Epoch 9 | step 5250/7267 | global_step=63386] train_loss=3.2454  ppl=25.67  tok_acc=40.05%  tok/s=53,216\n",
      "[Epoch 9 | step 5400/7267 | global_step=63536] train_loss=3.2455  ppl=25.68  tok_acc=40.05%  tok/s=53,216\n",
      "[Epoch 9 | step 5550/7267 | global_step=63686] train_loss=3.2457  ppl=25.68  tok_acc=40.05%  tok/s=53,213\n",
      "[Epoch 9 | step 5700/7267 | global_step=63836] train_loss=3.2457  ppl=25.68  tok_acc=40.05%  tok/s=53,215\n",
      "[Epoch 9 | step 5850/7267 | global_step=63986] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,213\n",
      "[Epoch 9 | step 6000/7267 | global_step=64136] train_loss=3.2459  ppl=25.68  tok_acc=40.05%  tok/s=53,218\n",
      "[Epoch 9 | step 6150/7267 | global_step=64286] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,215\n",
      "[Epoch 9 | step 6300/7267 | global_step=64436] train_loss=3.2458  ppl=25.68  tok_acc=40.05%  tok/s=53,218\n",
      "[Epoch 9 | step 6450/7267 | global_step=64586] train_loss=3.2456  ppl=25.68  tok_acc=40.05%  tok/s=53,221\n",
      "[Epoch 9 | step 6600/7267 | global_step=64736] train_loss=3.2454  ppl=25.67  tok_acc=40.06%  tok/s=53,226\n",
      "[Epoch 9 | step 6750/7267 | global_step=64886] train_loss=3.2456  ppl=25.68  tok_acc=40.06%  tok/s=53,225\n",
      "[Epoch 9 | step 6900/7267 | global_step=65036] train_loss=3.2455  ppl=25.68  tok_acc=40.06%  tok/s=53,230\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: \" in washington , dc , thus beginning a 15 @-@ year affiliation as laucke 's active supporter in the u.s. pell 's former campaign manager , raymond nelson , handled logistics for many of laucke 's u.s. performances . in 1973 , laucke starred in a documentary produced by radio @-@ québec called la guitare , and he performed\"\n",
      "REF: \" washington , dc , thus beginning a 15 @-@ year affiliation as laucke 's active supporter in the u.s. pell 's former campaign manager , raymond nelson , handled logistics for many of laucke 's u.s. performances . in 1973 , laucke starred in a documentary produced by radio @-@ québec called la guitare , and he performed at\"\n",
      "HYP: ' the , d , and making the new @-@ year periodiliation with thevkends new political . the state.s. senateension \\'s campaign position . , john b , was theging and the of theaucke \\'s campaign.s. campaign . \\n the , laucke was in the short film by the personality televisionizb , \" va , which in was in'\n",
      "[Epoch 9 | step 7050/7267 | global_step=65186] train_loss=3.2456  ppl=25.68  tok_acc=40.06%  tok/s=53,231\n",
      "[Epoch 9 | step 7200/7267 | global_step=65336] train_loss=3.2455  ppl=25.67  tok_acc=40.06%  tok/s=53,231\n",
      "Epoch 9 done | train_loss=3.2456  train_ppl=25.68  train_tok_acc=40.06%\n",
      "Guardado checkpoint (cada 3 epochs) -> gptmini_owt10k.pt\n",
      "[Epoch 10 | step  150/7267 | global_step=65553] train_loss=3.2264  ppl=25.19  tok_acc=40.24%  tok/s=53,163\n",
      "[Epoch 10 | step  300/7267 | global_step=65703] train_loss=3.2280  ppl=25.23  tok_acc=40.26%  tok/s=53,160\n",
      "[Epoch 10 | step  450/7267 | global_step=65853] train_loss=3.2272  ppl=25.21  tok_acc=40.26%  tok/s=53,161\n",
      "[Epoch 10 | step  600/7267 | global_step=66003] train_loss=3.2281  ppl=25.23  tok_acc=40.24%  tok/s=53,210\n",
      "[Epoch 10 | step  750/7267 | global_step=66153] train_loss=3.2281  ppl=25.23  tok_acc=40.25%  tok/s=53,185\n",
      "[Epoch 10 | step  900/7267 | global_step=66303] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,200\n",
      "[Epoch 10 | step 1050/7267 | global_step=66453] train_loss=3.2292  ppl=25.26  tok_acc=40.24%  tok/s=53,168\n",
      "[Epoch 10 | step 1200/7267 | global_step=66603] train_loss=3.2288  ppl=25.25  tok_acc=40.24%  tok/s=53,206\n",
      "[Epoch 10 | step 1350/7267 | global_step=66753] train_loss=3.2283  ppl=25.24  tok_acc=40.25%  tok/s=53,216\n",
      "[Epoch 10 | step 1500/7267 | global_step=66903] train_loss=3.2290  ppl=25.25  tok_acc=40.24%  tok/s=53,230\n",
      "[Epoch 10 | step 1650/7267 | global_step=67053] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,216\n",
      "[Epoch 10 | step 1800/7267 | global_step=67203] train_loss=3.2292  ppl=25.26  tok_acc=40.24%  tok/s=53,240\n",
      "[Epoch 10 | step 1950/7267 | global_step=67353] train_loss=3.2285  ppl=25.24  tok_acc=40.25%  tok/s=53,236\n",
      "[Epoch 10 | step 2100/7267 | global_step=67503] train_loss=3.2290  ppl=25.25  tok_acc=40.25%  tok/s=53,236\n",
      "[Epoch 10 | step 2250/7267 | global_step=67653] train_loss=3.2287  ppl=25.25  tok_acc=40.25%  tok/s=53,240\n",
      "[Epoch 10 | step 2400/7267 | global_step=67803] train_loss=3.2291  ppl=25.26  tok_acc=40.24%  tok/s=53,237\n",
      "[Epoch 10 | step 2550/7267 | global_step=67953] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,237\n",
      "[Epoch 10 | step 2700/7267 | global_step=68103] train_loss=3.2291  ppl=25.26  tok_acc=40.24%  tok/s=53,246\n",
      "[Epoch 10 | step 2850/7267 | global_step=68253] train_loss=3.2291  ppl=25.26  tok_acc=40.25%  tok/s=53,240\n",
      "[Epoch 10 | step 3000/7267 | global_step=68403] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,240\n",
      "[Epoch 10 | step 3150/7267 | global_step=68553] train_loss=3.2288  ppl=25.25  tok_acc=40.25%  tok/s=53,245\n",
      "[Epoch 10 | step 3300/7267 | global_step=68703] train_loss=3.2289  ppl=25.25  tok_acc=40.25%  tok/s=53,247\n",
      "[Epoch 10 | step 3450/7267 | global_step=68853] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,243\n",
      "[Epoch 10 | step 3600/7267 | global_step=69003] train_loss=3.2298  ppl=25.27  tok_acc=40.24%  tok/s=53,246\n",
      "[Epoch 10 | step 3750/7267 | global_step=69153] train_loss=3.2297  ppl=25.27  tok_acc=40.24%  tok/s=53,246\n",
      "[Epoch 10 | step 3900/7267 | global_step=69303] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,244\n",
      "[Epoch 10 | step 4050/7267 | global_step=69453] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,249\n",
      "[Epoch 10 | step 4200/7267 | global_step=69603] train_loss=3.2294  ppl=25.26  tok_acc=40.24%  tok/s=53,248\n",
      "[Epoch 10 | step 4350/7267 | global_step=69753] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,249\n",
      "[Epoch 10 | step 4500/7267 | global_step=69903] train_loss=3.2292  ppl=25.26  tok_acc=40.25%  tok/s=53,251\n",
      "[Epoch 10 | step 4650/7267 | global_step=70053] train_loss=3.2293  ppl=25.26  tok_acc=40.24%  tok/s=53,257\n",
      "[Epoch 10 | step 4800/7267 | global_step=70203] train_loss=3.2292  ppl=25.26  tok_acc=40.24%  tok/s=53,251\n",
      "[Epoch 10 | step 4950/7267 | global_step=70353] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,245\n",
      "[Epoch 10 | step 5100/7267 | global_step=70503] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,244\n",
      "[Epoch 10 | step 5250/7267 | global_step=70653] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,240\n",
      "[Epoch 10 | step 5400/7267 | global_step=70803] train_loss=3.2295  ppl=25.27  tok_acc=40.24%  tok/s=53,246\n",
      "[Epoch 10 | step 5550/7267 | global_step=70953] train_loss=3.2294  ppl=25.26  tok_acc=40.24%  tok/s=53,247\n",
      "[Epoch 10 | step 5700/7267 | global_step=71103] train_loss=3.2297  ppl=25.27  tok_acc=40.24%  tok/s=53,243\n",
      "[Epoch 10 | step 5850/7267 | global_step=71253] train_loss=3.2296  ppl=25.27  tok_acc=40.24%  tok/s=53,245\n",
      "[Epoch 10 | step 6000/7267 | global_step=71403] train_loss=3.2298  ppl=25.28  tok_acc=40.23%  tok/s=53,242\n",
      "[Epoch 10 | step 6150/7267 | global_step=71553] train_loss=3.2298  ppl=25.27  tok_acc=40.23%  tok/s=53,242\n",
      "[Epoch 10 | step 6300/7267 | global_step=71703] train_loss=3.2298  ppl=25.27  tok_acc=40.24%  tok/s=53,240\n",
      "[Epoch 10 | step 6450/7267 | global_step=71853] train_loss=3.2300  ppl=25.28  tok_acc=40.23%  tok/s=53,244\n",
      "[Epoch 10 | step 6600/7267 | global_step=72003] train_loss=3.2300  ppl=25.28  tok_acc=40.23%  tok/s=53,241\n",
      "[Epoch 10 | step 6750/7267 | global_step=72153] train_loss=3.2301  ppl=25.28  tok_acc=40.23%  tok/s=53,245\n",
      "[Epoch 10 | step 6900/7267 | global_step=72303] train_loss=3.2299  ppl=25.28  tok_acc=40.23%  tok/s=53,240\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: ' field , florida . its mission focus is unconventional warfare : counter @-@ terrorism , combat search and rescue , personnel recovery , psychological operations , aviation assistance to developing nations , \" deep battlefield \" resupply , interdiction and close air support . the wing \\'s core missions include aerospace surface interface , agile combat support , combat aviation advisory operations , information operations , personnel recovery /'\n",
      "REF: ' , florida . its mission focus is unconventional warfare : counter @-@ terrorism , combat search and rescue , personnel recovery , psychological operations , aviation assistance to developing nations , \" deep battlefield \" resupply , interdiction and close air support . the wing \\'s core missions include aerospace surface interface , agile combat support , combat aviation advisory operations , information operations , personnel recovery / recovery'\n",
      "HYP: ' , and , \\n population was was onnentional , , the @-@ terrorism , and , and rescue , and search , and warfare , and , , the countries , and the @-@ \" ,upply , and @-@isc , other air support , \\n mission \\'s mission mission include theobace , @-@ , airm air , , and support , , , and support , and recovery , recovery'\n",
      "[Epoch 10 | step 7050/7267 | global_step=72453] train_loss=3.2300  ppl=25.28  tok_acc=40.23%  tok/s=53,243\n",
      "[Epoch 10 | step 7200/7267 | global_step=72603] train_loss=3.2299  ppl=25.28  tok_acc=40.23%  tok/s=53,243\n",
      "Epoch 10 done | train_loss=3.2300  train_ppl=25.28  train_tok_acc=40.23%\n"
     ]
    }
   ],
   "source": [
    "from src.training.main_loop import *\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "block_size = 256\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPT2(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,  \n",
    "    n_layer=10,             \n",
    "    n_head=8,            \n",
    "    d_model=512,dropout=0.1,).to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Usando {torch.cuda.device_count()} GPUs con DataParallel\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    use_dataparallel = True\n",
    "\n",
    "def id2tok_fn(ids):\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "\n",
    "history = train_gpt_lm(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=10,\n",
    "    base_lr=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=2000,\n",
    "    label_smoothing=0.1,\n",
    "    grad_clip=1.0,\n",
    "    device=device,\n",
    "    ckpt_path=\"gptmini_owt10k.pt\",\n",
    "    log_every=150,\n",
    "    preview_every=7000,\n",
    "    id2tok_fn=id2tok_fn,\n",
    "    amp_enabled=True,\n",
    "    amp_dtype=\"fp16\",   \n",
    "    val_checking = False , save_ckpt_every = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T13:08:18.454683Z",
     "iopub.status.busy": "2025-11-18T13:08:18.454386Z",
     "iopub.status.idle": "2025-11-18T14:53:42.223476Z",
     "shell.execute_reply": "2025-11-18T14:53:42.222320Z",
     "shell.execute_reply.started": "2025-11-18T13:08:18.454664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | step 1000/7267 | global_step=1000] train_loss=3.2498  ppl=25.79  tok_acc=39.95%  tok/s=53,346\n",
      "[Epoch 1 | step 2000/7267 | global_step=2000] train_loss=3.2949  ppl=26.97  tok_acc=39.41%  tok/s=53,379\n",
      "[Epoch 1 | step 3000/7267 | global_step=3000] train_loss=3.3319  ppl=27.99  tok_acc=38.97%  tok/s=53,389\n",
      "[Epoch 1 | step 4000/7267 | global_step=4000] train_loss=3.3542  ppl=28.62  tok_acc=38.73%  tok/s=53,375\n",
      "[Epoch 1 | step 5000/7267 | global_step=5000] train_loss=3.3672  ppl=29.00  tok_acc=38.59%  tok/s=53,371\n",
      "[Epoch 1 | step 6000/7267 | global_step=6000] train_loss=3.3758  ppl=29.25  tok_acc=38.50%  tok/s=53,366\n",
      "[Epoch 1 | step 7000/7267 | global_step=7000] train_loss=3.3815  ppl=29.42  tok_acc=38.44%  tok/s=53,365\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: \" and detection of vibrations underwater . compared to the harbor seal , the california sea lion 's vibrissae are smoother and less specialized and thus perform less when following hydrodynamic trails , although they still perform well . \\n = = ecology = = \\n = = = range and habitat = = = \\n the california sea lion ranges along the western coast and islands\"\n",
      "REF: \" detection of vibrations underwater . compared to the harbor seal , the california sea lion 's vibrissae are smoother and less specialized and thus perform less when following hydrodynamic trails , although they still perform well . \\n = = ecology = = \\n = = = range and habitat = = = \\n the california sea lion ranges along the western coast and islands of\"\n",
      "HYP: ' the of ther . . the to the other , , the bay state has wass highrationationalive are notoother than have prone than have more a than compared thechynamic or . and the are have more in \\n = = = = = \\n = = = life = habitat = = = \\n the california sea lion is from the coast coast of is ,'\n",
      "Epoch 1 done | train_loss=3.3826  train_ppl=29.45  train_tok_acc=38.43%\n",
      "[Epoch 2 | step 1000/7267 | global_step=8267] train_loss=3.3629  ppl=28.87  tok_acc=38.60%  tok/s=53,275\n",
      "[Epoch 2 | step 2000/7267 | global_step=9267] train_loss=3.3666  ppl=28.98  tok_acc=38.59%  tok/s=53,294\n",
      "[Epoch 2 | step 3000/7267 | global_step=10267] train_loss=3.3675  ppl=29.01  tok_acc=38.59%  tok/s=53,309\n",
      "[Epoch 2 | step 4000/7267 | global_step=11267] train_loss=3.3664  ppl=28.97  tok_acc=38.61%  tok/s=53,304\n",
      "[Epoch 2 | step 5000/7267 | global_step=12267] train_loss=3.3646  ppl=28.92  tok_acc=38.64%  tok/s=53,296\n",
      "[Epoch 2 | step 6000/7267 | global_step=13267] train_loss=3.3624  ppl=28.86  tok_acc=38.67%  tok/s=53,297\n",
      "[Epoch 2 | step 7000/7267 | global_step=14267] train_loss=3.3594  ppl=28.77  tok_acc=38.72%  tok/s=53,307\n",
      "— preview (LM, teacher-forced argmax) —\n",
      "CTX: \" as of 2012 . the procedural part of oracle 's pl / sql supports boolean however variables ; these can also be assigned null and the value is considered the same as unknown . \\n = = controversy = = \\n = = = common mistakes = = = \\n misunderstanding of how null works is the cause of a great number of errors in sq\"\n",
      "REF: \" of 2012 . the procedural part of oracle 's pl / sql supports boolean however variables ; these can also be assigned null and the value is considered the same as unknown . \\n = = controversy = = \\n = = = common mistakes = = = \\n misunderstanding of how null works is the cause of a great number of errors in sql\"\n",
      "HYP: \" the the . \\n songural of of theion wass storyural el is theiling ' ' ,ables are the include be be used to @-@s c pl of not to same as the . \\n = = = = = \\n = = = = names = = = \\n theinterstanding is the theull is are used subject of the dispute number of errors , thel\"\n",
      "Epoch 2 done | train_loss=3.3585  train_ppl=28.74  train_tok_acc=38.73%\n",
      "Guardado checkpoint (cada 2 epochs) -> gptmini_owt10k.pt\n",
      "[Epoch 3 | step 1000/7267 | global_step=15534] train_loss=3.2833  ppl=26.66  tok_acc=39.55%  tok/s=53,302\n",
      "[Epoch 3 | step 2000/7267 | global_step=16534] train_loss=3.2822  ppl=26.64  tok_acc=39.58%  tok/s=53,295\n",
      "[Epoch 3 | step 3000/7267 | global_step=17534] train_loss=3.2809  ppl=26.60  tok_acc=39.60%  tok/s=53,313\n",
      "[Epoch 3 | step 4000/7267 | global_step=18534] train_loss=3.2788  ppl=26.54  tok_acc=39.64%  tok/s=53,331\n",
      "[Epoch 3 | step 5000/7267 | global_step=19534] train_loss=3.2765  ppl=26.48  tok_acc=39.67%  tok/s=53,332\n",
      "[Epoch 3 | step 6000/7267 | global_step=20534] train_loss=3.2737  ppl=26.41  tok_acc=39.72%  tok/s=53,321\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/3902434519.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = train_gpt_lm(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/638714474.py\u001b[0m in \u001b[0;36mtrain_gpt_lm\u001b[0;34m(model, train_loader, val_loader, epochs, base_lr, weight_decay, warmup_steps, label_smoothing, grad_clip, device, ckpt_path, log_every, preview_every, id2tok_fn, amp_enabled, amp_dtype, val_checking, save_ckpt_every)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_gpt_lm(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=4,\n",
    "    base_lr=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=2000,\n",
    "    label_smoothing=0.1,\n",
    "    grad_clip=1.0,\n",
    "    device=device,\n",
    "    ckpt_path=\"gptmini_owt10k.pt\",\n",
    "    log_every=1000,\n",
    "    preview_every=7000,\n",
    "    id2tok_fn=id2tok_fn,\n",
    "    amp_enabled=True,\n",
    "    amp_dtype=\"fp16\",   \n",
    "    val_checking = False , save_ckpt_every = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T14:54:19.990687Z",
     "iopub.status.busy": "2025-11-18T14:54:19.989940Z",
     "iopub.status.idle": "2025-11-18T14:54:20.497287Z",
     "shell.execute_reply": "2025-11-18T14:54:20.496474Z",
     "shell.execute_reply.started": "2025-11-18T14:54:19.990658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " are you happy?able with your story 's storyline , if anything 's going on the run as ' what they did ' – and you 're getting too much\n"
     ]
    }
   ],
   "source": [
    "from src.inference.generate_text import *\n",
    "\n",
    "\n",
    "prompt = \"are you happy?\"\n",
    "print(generate(model, tokenizer, prompt))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
